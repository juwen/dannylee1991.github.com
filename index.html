<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿." />



  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta property="og:type" content="website">
<meta property="og:title" content="DannyLee">
<meta property="og:url" content="http://dannylee1991.github.io/index.html">
<meta property="og:site_name" content="DannyLee">
<meta property="og:description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DannyLee">
<meta name="twitter:description" content="一只在迈向机器学习道路上狂奔的程序猿.">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>

<!--baidu统计-->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2f967e5ec4f276411160d27aeace7722";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  <title> DannyLee </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">DannyLee</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu menu-left">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      

      
      
        <li class="menu-item menu-item-search">
          <a href="#" class="st-search-show-outputs">
            <i class="menu-item-icon icon-next-search"></i> <br />
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'ss9-_Hsd4DyhyGw4m99P','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/06/01/斯坦福机器学习课程 第十周 (1)大数据集梯度下降/" itemprop="url">
                斯坦福机器学习课程 第十周 (1)大数据集梯度下降
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-06-01T22:27:00+08:00" content="2017-06-01">
            2017-06-01
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/06/01/斯坦福机器学习课程 第十周 (1)大数据集梯度下降/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/06/01/斯坦福机器学习课程 第十周 (1)大数据集梯度下降/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="处理大数据的学习算法">处理大数据的学习算法</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/CipHf/learning-with-large-datasets" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在接下来的几节中，我们会讲<strong>大规模的机器学习</strong>（就是用来处理大数据的算法）。如果我们看近5到10年的机器学习的历史，我们会发现现在的学习算法比5年前的好很多，其中的原因之一就是我们现在拥有很多可以训练算法的数据。</p>
</blockquote>
<p>在机器学习领域有一种说法是：</p>
<ul>
<li><p>“It`s not who has the best algorithm that wins. It`s who has the most data.”</p>
</li>
<li><p>“谁的数据最多，谁才能笑到最后。”</p>
</li>
</ul>
<p>因此，我们更期望在大的数据集上做训练。</p>
<p>但训练大的数据集也有它自己的问题，特别是计算量的问题。</p>
<p>假设我们的训练集的大小$m=100,000,000$。</p>
<blockquote>
<p>这对于现代的数据集其实是很现实的。比如对于美国的人口普查数据集来说美国有3亿人口，我们通常都能得到上亿条的数据。</p>
<p>如果我们看一下很受欢迎的网站的浏览量，我们也很容易得到上亿条的记录。</p>
</blockquote>
<p>假设我们要训练一个线性回归模型或者是逻辑回归模型，这是梯度下降的规则：</p>
<p><img src="/img/17_06_01/001.gif" alt=""></p>
<p>当你在计算梯度下降的时候，这里的$m$是一个上亿的值时，你需要通过计算上亿个数的导数项的和来计算仅仅一步的梯度下降。</p>
<p>在接下来的内容中，我们会介绍如何优化这个算法，或者换一个更高效率的算法。</p>
<p>在这一系列讲大型的机器学习的课程后，我们会知道如何拟合包括上亿数据的<strong>线性回归</strong>、<strong>逻辑回归</strong>、<strong>神经网络</strong>等等的例子。</p>
<p>当然，在我们训练一个上亿条数据的模型之前，我们还应该问自己为什么不用几千条数据呢？也许我们可以随机从上亿条的数据集里选个一千条的子集，然后用我们的算法计算。</p>
<p>在我们投入精力和开发软件来训练大数据的模型之前，我们往往会在一个比较小的数据集上来验证算法是否合适。</p>
<p>我们通常的方法是画出学习曲线，如果你画了学习曲线而且你的训练目标看上去像这样：</p>
<p><img src="/img/17_06_01/002.png" alt=""></p>
<p>$J_{train}(\theta)$是训练集上的代价函数，$J_{cv}(\theta)$是验证集上的代价函数。从图像中可以看出来，这个算法看起来像是处于<strong>高方差</strong>的状态。因此，我们需要增加训练集。</p>
<p>相比之下，如果你画出的学习曲线是这样的：</p>
<p><img src="/img/17_06_01/003.png" alt=""></p>
<p>这看起来像经典的<strong>高偏差</strong>学习算法。在这个例子中，我们可以看出来，数据量增长到一定程度之后，算法并不会好很多。因此我们没必要花费精力来扩大算法的规模。当然，如果你遇到了这种情况，一个很自然的方法是多加一些特征；或者在你的神经网络里加一些隐藏的单元等等。通过这些操作，你的算法的表现会越来越趋于第一种情况的图形：</p>
<p><img src="/img/17_06_01/002.png" alt=""></p>
<p>而这个过程中，我们应该花时间在添加基础设施来改进算法，而不是用多于一千条数据来建模，因为改进算法会更加有效果。</p>
<p>所以在大规模的机器学习中，我们喜欢找到合理的计算量的方法或高效率的计算量的方法来处理大的数据集。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/05/28/斯坦福机器学习课程 第九周 (4)预测电影评分/" itemprop="url">
                斯坦福机器学习课程 第九周 (4)预测电影评分
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-05-28T08:44:00+08:00" content="2017-05-28">
            2017-05-28
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/05/28/斯坦福机器学习课程 第九周 (4)预测电影评分/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/05/28/斯坦福机器学习课程 第九周 (4)预测电影评分/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h1 id="预测电影评分">预测电影评分</h1><h2 id="问题制定">问题制定</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Rhg6r/problem-formulation" target="_blank" rel="external">视频地址</a></p>
<p>在接下来的内容中，我想介绍给你们有关<strong>推荐系统(recommender systems)</strong>的内容。</p>
<p>我想讨论推荐系统有两个原因：</p>
<ul>
<li><p><strong>原因一：对工业界有至关重要的作用</strong></p>
<p>  在过去的几年中，我有时会去参观硅谷的各种科技类公司。我经常在那些公司里与开发机器学习应用的人交流。然后我问他们什么才是机器学习最重要的应用？或者什么样的机器学习的应用是你最想让它的表现得到改进的？我最常听到的回答其中之一就是 现在硅谷有好多个团队正试图建立更好的推荐系统。</p>
<p>  亚马逊、Netflix、eBay又或者苹果公司的iTunes Genius等，很多网站或者很多系统，试图向用户推荐新产品。比如说亚马逊向你推荐新书，Netflix 向你推荐新电影…。而这些推荐系统可能会看看你以前购买过什么书，或者以前你给哪些电影进行过评分。</p>
<p>  这些系统贡献了现今亚马逊收入的相当大一部分。而对像Netflix这样的公司，他们向用户推荐的电影占了用户观看的电影的相当大一部分。于是一个推荐系统其表现的一些改进，就能带来显著且即刻产生的影响。这种影响关系到许多公司的最终业绩。</p>
<p>  推荐系统在机器学习学术界是个很有意思的问题。如果我们去参加一个学术类的机器学习会议，<br>推荐系统的问题几乎得不到什么关注至少它是学术界当前动向里较小的一部分。但如果你看看正在发生的事对很多科技类公司而言，建立这些推荐系统似乎是优先要办的事。这就是我想在这门课中谈论推荐系统的原因之一。</p>
</li>
<li><p><strong>原因二：自动学习到优良特征</strong></p>
<p>  对于机器学习来说，特征量是重要的。你选择的特征对你学习算法的表现有很大影响。</p>
<p>  在机器学习领域有这么一个宏大的想法，就是对于一些问题而言（可能不是所有问题），存在一些算法能<strong>试图自动地替你学习到一组优良的特征量</strong>。这样与其手动设计或者手动编写特征，你或许能够采用一种算法来自动学习到使用什么特征量。而推荐系统就是这种情形的一个例子。</p>
</li>
</ul>
<h3 id="推荐系统_问题表述">推荐系统 问题表述</h3><p>以预测电影评分这个时兴的问题为例，假想你是一个销售或出租电影的网站，你让用户使用1至5颗星 给不同的电影评分：</p>
<p><img src="/img/17_05_28/001.png" alt=""></p>
<p>假设下面的表格是几个用户针对五部电影给出的评分。其中”?”代表用户没有给出评分：</p>
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
<p>引入以下几个变量：</p>
<p>$$<br>n_u=用户数量 \\<br>n_m=电影数量 \\<br>r(i,j)=如果用户j对电影i投过票，则记为1 \\<br>y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）<br>$$</p>
<p>推荐系统问题就是：</p>
<p>在给定上面的这些数据（即$r(i,j)$和$y^{(i,j)}$）时，然后视图去预测上面表格中那些”?”的值。这样我们就可以向用户推荐他们可能还没看过的新电影了。</p>
<h2 id="基于内容的推荐">基于内容的推荐</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/uG59z/content-based-recommendations" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>上一节，我们引出了<strong>推荐系统</strong>的问题，这一节，我将介绍一种<strong>基于内容的推荐</strong>方法。</p>
</blockquote>
<p>这是上一节中的数据：</p>
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">$x_1$(爱情片)</th>
<th style="text-align:center">$x_2$(动作片)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0.99</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
</tr>
</tbody>
</table>
<p>重申一下之前的定义：</p>
<p>$$<br>n_u=用户数量 \\<br>n_m=电影数量 \\<br>r(i,j)=如果用户j对电影i投过票，则记为1 \\<br>y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）<br>$$</p>
<p>在这里的数据中：</p>
<p>$$<br>n_u=4 \\<br>n_m=5<br>$$</p>
<p>并且我们又追加了两列特征$x_1$和$x_2$，分别表示当前电影属于<strong>爱情片</strong>和<strong>动作片</strong>的程度。我们用$n=2$来代表特征数（这里不包括$x_0$）。</p>
<p>有了每部电影的类型特征数据，我们就可以用一个特征矩阵表示某一部电影了。假设上面五部电影，从上到下我们依次使用数字$1、2、3、4、5$来代替。那么对于第一部电影《爱到最后》的特征向量表示为：</p>
<p>$$<br>\begin{equation}<br>x^{(1)}=\left[<br>\begin{matrix}<br>1\\<br>0.9\\<br>0<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<blockquote>
<p>这三个数分别代表三个特征的值:$x_0$、$x_1$和$x_2$。其中$x_0$是截距特征变量，值为1。</p>
</blockquote>
<h3 id="使用一个已经训练好的模型进行预测">使用一个已经训练好的模型进行预测</h3><p>为了进行预测，我们可以把对每个观众打分的预测当成一个独立的线性回归问题。</p>
<p>具体来说，比如每一个用户$j$，都学习出一个参数$\theta^{(j)}$（一般情况下，$\theta^{(j)}$的维度都是$n+1$，$n$是特征数，不包括截距项$x_0$）。</p>
<p>然后我们要根据参数向量$\theta^{(j)}$与特征$x^{(i)}$的内积来预测用户对电影$i$的评分：</p>
<p>$$<br>(\theta^{(j)})^Tx^{(i)}<br>$$</p>
<h4 id="举例">举例</h4><p>我们以用户1 Alice为例，Alice对应的特征向量应该是$\theta^{(1)}$，我们要预测Alice对于电影《小爱犬》的评分，这部电影的特征向量为：</p>
<p>$$<br>\begin{equation}<br>x^{(3)}=\left[<br>\begin{matrix}<br>1\\<br>0.99\\<br>0<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>假如你已经得到了Alice的特征参数向量$\theta^{(1)}$的值（后面我们会具体讲如何得到这个值）：</p>
<p>$$<br>\begin{equation}<br>\theta^{(1)}=\left[<br>\begin{matrix}<br>0\\<br>5\\<br>0<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>那么对于Alice关于《小爱犬》的平分预测如下：</p>
<p>$$<br>(\theta^{(1)})^Tx^{(3)}=5×0.99=4.95<br>$$</p>
<p>这里我们实际在做的事情就是：<strong>对每个用户应用不同的线性回归模型</strong>。</p>
<h3 id="更正式的描述">更正式的描述</h3><p>关于这个计算过程，更正式的描述如下：</p>
<p>已知：</p>
<p>$$<br>r(i,j)=如果用户j对电影i投过票，则记为1 \\<br>y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）\\<br>\theta^{(j)} = 用户j的参数向量 \\<br>x^{(i)} = 电影i的特征向量<br>$$</p>
<p>对于用户$j$关于电影$i$的预测评分为：</p>
<p>$$<br>(\theta^{(j)})^Tx^{(i)}<br>$$</p>
<h3 id="训练预测模型：计算参数向量$\theta$">训练预测模型：计算参数向量$\theta$</h3><p>通过以下运算，可以学习到参数$\theta^{(j)}$：</p>
<p><img src="/img/17_05_28/002.gif" alt=""></p>
<blockquote>
<p><strong>公式解读：</strong></p>
<p>其中$\sum_{i:r(i,j)=1}$代表对所有满足$r(i,j)=1$的元素进行求和。</p>
<p>$(\theta^{(j)})^T(x^{(i)})-y^{(i,j)}$代表预测用户$j$对电影$i$的平分减去用户对这部电影的实际评分。</p>
<p>$\frac{\lambda}{2m^{(j)}}\sum_{k=1}^n((\theta_k^{(j)}))^2$是正则化项。</p>
</blockquote>
<p>对上面的式子通过求最小化，我们最终可以得到一个表现良好的$\theta^{(j)}$。</p>
<p>为了让公式变得更简单一些，我们可以去除掉常数项$\frac{1}{m^{(j)}}$，对最终结果无影响：</p>
<p><img src="/img/17_05_28/003.gif" alt=""></p>
<h4 id="完整的描述">完整的描述</h4><p>接下来对训练过程进行完整的描述一遍：</p>
<h5 id="优化目标">优化目标</h5><p>为了学习第$j$个用户的参数$\theta^{(j)}$，我们执行以下运算：</p>
<p><img src="/img/17_05_28/003.gif" alt=""></p>
<p>为了学习全部用户的参数$\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$，我们执行以下运算：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>通过这一运算，我们就能得出所有用户的参数向量$\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$，从而对所有用户作出预测了。</p>
<h5 id="优化算法">优化算法</h5><p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>是我们最优化目标，记为$J(\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)})$。</p>
<p>为了最小化这个目标函数$J(\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)})$，我们需要使用梯度下降算法来计算：</p>
<ul>
<li><strong>k=0时</strong></li>
</ul>
<p><img src="/img/17_05_28/005.gif" alt=""></p>
<ul>
<li><strong>k≠0时</strong></li>
</ul>
<p><img src="/img/17_05_28/006.gif" alt=""></p>
<blockquote>
<p><strong>注意：</strong>其中$\alpha$是学习率。</p>
</blockquote>
<p>上面的过程实际上是在对优化目标函数$J(\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)})$求偏微分的过程：</p>
<p>$$<br>\frac{\partial}{\partial\theta_k^{j}}<br>J(\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)})<br>$$</p>
<p>以上就是通过梯度下降来最小化代价函数$J$的全过程，当然，如果你愿意的话，你可以尝试使用更高级的优化算比如<strong>聚类下降</strong>或者<em>L-BFGS(Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm)*</em>或者别的方法来最小化代价函数J。</p>
<blockquote>
<p>以上内容，我们介绍了使用一种线性回归的变体来预测不同用户对不同电影评分的算法，这种算法被称作<strong>“基于内容的推荐”</strong>，因为我们已知了不同电影的特征（比如电影是爱情片的程度，是动作片的程度等），从而来进行预测。但事实上，对于很多电影，我们并没有这些特征，或者很难得到这些特征。所以，在下一节中，我们将介绍一种<strong>“不基于内容的推荐系统”</strong>。</p>
</blockquote>
<h1 id="协同过滤">协同过滤</h1><h2 id="协同过滤-1">协同过滤</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/2WoBV/collaborative-filtering" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这一节，我们将介绍一种叫做<strong>协同过滤(collaborative filtering)</strong>的推荐算法。</p>
<p>关于这个算法值得一提的一个特点，那就是它能实现<strong>对特征的学习</strong>。就是说这个算法可以实现自动学习要使用的特征值。</p>
</blockquote>
<p>在之前的电影评分的例子中，我们有下面的这组数据：</p>
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">$x_1$(爱情片)</th>
<th style="text-align:center">$x_2$(动作片)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0.99</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
</tr>
</tbody>
</table>
<p>我们除了有每个用户对不同电影的评分之外，同时也有每个电影属于不同类别特征的程度值$x_1$和$x_2$。</p>
<p>但是，在实际情况中，这样做的难度很大，因为我们很难对所有的电影给出相关特征的评分。而且通常情况，我们还希望能得到除了这两个特征之外的其他特征。</p>
<p>为了能做到这一点，我们换一种形式，假设我们并不知道每部电影具体的特征值是多少：</p>
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">$x_1$(爱情片)</th>
<th style="text-align:center">$x_2$(动作片)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
<p>假设我们采访了每一位用户，而且每一位用户都告诉我们他们是否喜欢爱情电影；以及他们是否喜欢动作电影，这样Alice、Bob、Carol以及Dave就有了他们对应的参数：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\begin{equation}\theta^{(1)}=\left[\begin{matrix}0\\5\\0\end{matrix}\right]\end{equation}$</td>
<td style="text-align:center">$\begin{equation}\theta^{(2)}=\left[\begin{matrix}0\\5\\0\end{matrix}\right]\end{equation}$</td>
<td style="text-align:center">$\begin{equation}\theta^{(3)}=\left[\begin{matrix}0\\0\\5\end{matrix}\right]\end{equation}$</td>
<td style="text-align:center">$\begin{equation}\theta^{(4)}=\left[\begin{matrix}0\\0\\5\end{matrix}\right]\end{equation}$</td>
</tr>
</tbody>
</table>
<p>每个特征向量的第二个元素表示对爱情片的喜欢程度，第三个元素表示对动作片的喜欢程度。可以看出来Alice和Bob更喜欢爱情片，而Carol和Dave更喜欢动作片。</p>
<p>有了这些数据，我们就可以着眼于用户，看看任意用户$j$对应的不同题材电影的喜欢程度$\theta^{(j)}$。有了这些参数值，理论上我们就能推测出每部电影的特征变量$x_1$和$x_2$的值。</p>
<p>举个例子：</p>
<p>我们看第一个电影，假设我们不知道这部电影的主要内容，我们只知道Alice和Bob喜欢这部电影，而Carol和Dave不喜欢它。从他们四个人的特征向量就可以看出，Alice和Bob给出了5分，而Carol和Dave给出了0分。因此我们可以推断，这部电影可能是一部爱情片，而不太可也能是动作片。所以可能$x_1=1.0$而$x_2=0.0$。</p>
<p>其实这说明了我们在寻找能使得以下式子成立的$x^{(1)}$：</p>
<p>$$<br>(\theta^{(1)})^Tx^{(1)}≈5 \\<br>(\theta^{(2)})^Tx^{(1)}≈5 \\<br>(\theta^{(3)})^Tx^{(1)}≈0 \\<br>(\theta^{(4)})^Tx^{(1)}≈0<br>$$</p>
<p>因此，我们可以得出$x^{(1)}$的值为：</p>
<p>$$<br>\begin{equation}<br>x^{(1)}<br>=\left[<br>\begin{matrix}<br>1\\<br>1.0\\<br>0.0<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<blockquote>
<p>第一个元素$x^{(1)}_0$是截距。</p>
</blockquote>
<p>我们可以按照这种方式把其他电影的特征预测出来。</p>
<h3 id="协同过滤算法的正式描述">协同过滤算法的正式描述</h3><p><strong>优化算法：</strong></p>
<p>在已知给定参数$\theta^{(1)},…,\theta^{(n_u)}$的情况下，对下面函数最小化，得到特征值$x^{(i)}$：</p>
<p><img src="/img/17_05_28/007.gif" alt=""></p>
<p>这就是我们如何从一部特定的电影中学习到特征的方法。</p>
<p>那么对于所有的电影，计算所有的特征，我们通过最小化下面的函数可以得到：</p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>这里我们对所有$n_m$个电影的特征代价函数求和，然后最小化这个整体的代价函数，这样就能得到针对所有电影的合理的特征值了。</p>
<h3 id="鸡生蛋？蛋生鸡？">鸡生蛋？蛋生鸡？</h3><p>在上一节中，我们提到了如果给定了一系列的特征值$x^{(1)},…,x^{(n_m)}$，以及用户对电影的评分，我们就可以得到不同用户对应的参数向量$\theta^{(1)},…,\theta^{(n_u)}$。</p>
<p>在本节内容中，我们又讲到了在给定参数$\theta^{(1)},…,\theta^{(n_u)}$的情况下，我们可以预测每个电影的特征向量$x^{(1)},…,x^{(n_m)}$。</p>
<p>这有点类似<strong>先有鸡还是先有蛋</strong>的问题。我们知道了$x$就能预测出$\theta$，反之，如果我们知道了$\theta$，我们就能预测出$x$。</p>
<p>这样一来，我们能做到的就是首先随机初始化参数向量$\theta$，然后根据这些初始化得到的$\theta$来得到不同电影的特征值$x$:</p>
<p>$$<br>Guess \ \ \ \ \theta → x<br>$$</p>
<p>有了这些特征值$x$，我们就可以得到对参数$\theta$更好的估计：</p>
<p>$$<br>x → \theta<br>$$</p>
<p>同样，我们可以根据这个更好的$\theta$来得到更好的特征集$x$：</p>
<p>$$<br>\theta → x<br>$$</p>
<p>如此循环往复。</p>
<p>如果你一直重复上述的计算过程，你的算法将会收敛到一组合理的特征值$x$，以及一组合理的对不同用户参数的估计值$\theta$。</p>
<p>这就是基本的<strong>协同过滤算法</strong>，但这实际上不是我们最终使用的算法。在下一节中，我们将改进这个算法，让其在计算时更为高效。但这节课希望能让你基本了解这个算法的基本原理。</p>
<blockquote>
<p>总结一下，在本节，我们了解了最基本的协同过滤算法。</p>
<p>协同过滤算法指的是当你针对一大批用户数据执行这个算法时，这些用户实际上在高效地进行了协同合作来得到每个人对电影的评分值。只要用户对某几部电影进行评分，每个用户就都在帮助算法更好的学习出特征。这样以来，通过自己对几部电影评分之后，我就能帮助系统更好的学习到特征。这些特征可以被系统运用，为其他人做出更准确的电影预测。</p>
<p>协同的另一层意思是说每位用户都在为了大家的利益，而学习出更好的特征。这就是协同过滤。</p>
</blockquote>
<h2 id="协同过滤算法">协同过滤算法</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/f26nH/collaborative-filtering-algorithm" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在前几节中，我们谈到几个概念。</p>
<p>首先，如果给你几个特征表示电影，我们可以使用这些资料去获得用户的参数数据。</p>
<p>第二，如果给你用户的参数数据，你可以使用这些资料去获得电影的特征。</p>
<p>本节中，我们将会使用这些概念，并且将它们合并成<strong>协同过滤算法 (Collaborative Filtering Algorithm)</strong>。</p>
</blockquote>
<p>在之前的内容中，我们介绍了通过最小化以下目标函数，来在已知电影特征向量$x^{(1)},…,x^{(n_m)}$的情况下，预测每个用户对应的参数向量$\theta^{(1)},…,\theta^{(n_u)}$：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>也介绍了通过最小化以下目标函数，来在已知每个用户参数向量$\theta^{(1)},…,\theta^{(n_u)}$的情况下，来预测每个电影的特征向量$x^{(1)},…,x^{(n_m)}$：</p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>我们首先通过随机初始化一组参数$\theta$，然后来推导特征向量$x$；有了特征向量$x$，我们再去反推更好的参数向量$\theta$，然后如此循环往复，直到收敛。</p>
<p>而实际上，我们有一个更有效率的算法，让我们不必再这样不停地来回计算，而是能同时把$x$和$\theta$同时计算出来。</p>
<p>我们做的就是把上面两个式子合而为一：</p>
<p>同时最小化$x^{(1)},…,x^{(n_m)}$和$\theta^{(1)},…,\theta^{(n_u)}$：</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>优化目标函数：</p>
<p><img src="/img/17_05_28/010.gif" alt=""></p>
<h3 id="公式解读">公式解读</h3><p>接下来，我们对上面这个式子的推导过程进行详细的解读。</p>
<h4 id="求和部分">求和部分</h4><p>首先需要说明的是，在之前的这两个式子中：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>仔细观察这两部分，其实是一样的：</p>
<p><img src="/img/17_05_28/011.gif" alt=""></p>
<p><img src="/img/17_05_28/012.gif" alt=""></p>
<p>第一个是：所有的用户的评过分的电影中，预测评分和真实评分的差值平方总和。</p>
<p>第二个是：所有的电影中对它评过分的用户，预测的评分和真实评分的差值平方总和。</p>
<p>这两者的计算结果其实是一样的，只不过条件先后不同导致计算顺序不同而已。</p>
<p>因此，在最终的代价函数中：</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>这一部分：</p>
<p><img src="/img/17_05_28/013.gif" alt=""></p>
<p>其实这个表达式就是上面那两种表达式的整体的表示形式。计算结果与上面那两个式子也是相同的。</p>
<h4 id="正则化部分">正则化部分</h4><p>最终的代价函数</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>的后半段：</p>
<p><img src="/img/17_05_28/014.gif" alt=""></p>
<p>其实就是把这两个式子中的正则化部分求和得出的：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>如果你把</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>中$x^{(1)},…,x^{(n_m)}$部分看做是常数，然后关于$\theta$做优化，那么这个式子其实就相当于：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>反之，如果你把$\theta^{(1)},…,\theta^{(n_u)}$部分看做常数，然后关于$x$做优化，那么这个式子其实就相当于是：</p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<h4 id="总结">总结</h4><p>最终，我们通过合并之前的两个式子，得到了同时关于$x$和$\theta$的代价函数表达式：</p>
<p><img src="/img/17_05_28/009.gif" alt=""><br><img src="/img/17_05_28/010.gif" alt=""></p>
<p>这和之前的算法之间唯一的不同就是<strong>不需要反复计算</strong>（就是前面提到的先关于$\theta$最小化，然后再关于$x$最小化，然后再次关于$\theta$最小化，反复直到收敛）。在新版本里头 不需要不断地在$x$和$θ$这两个参数之间不停折腾，我们所要做的是将这两组参数同时化简。</p>
<h4 id="注意点">注意点</h4><p>最后值得提醒的一件事，当我们以这样的方法学习特征量时，我们必须要保证去掉$x_0$项，这样来保证我们学习到的特征量$x$是$n$维的，而不是$n+1$维的。</p>
<p>同样地，因为参数$θ$与特征向量$x$是在同一个维度上，所以$θ$也是$n$维的。因为如果没有$x_0$，那么$θ_0$也不再需要。</p>
<h3 id="协同过滤算法的正式描述-1">协同过滤算法的正式描述</h3><p>把上面所有的过程总结下来，就是所谓的<strong>协同过滤算法</strong>。下面是对这一算法的具体步骤描述：</p>
<ul>
<li>1.首先用较小的初始值来随机初始化参数$x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)}$。<blockquote>
<p>这一步有点像训练神经网络，在神经网络中的训练中，我们也是用小的随机数来初始化权值。</p>
</blockquote>
</li>
<li>2.通过使用梯度下降算法（或者其他更高级的优化算法）来最小化代价函数$J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})$。<ul>
<li>例如循环每一个$j=1,…,n_u,i=1,…,n_m$ 来执行对应的梯度下降：</li>
</ul>
</li>
</ul>
<p><img src="/img/17_05_28/015.png" alt=""></p>
<p>其中这一部分：</p>
<p><img src="/img/17_05_28/016.png" alt=""></p>
<p>就是对代价函数的偏微分项：</p>
<p>$$<br>\frac{\partial}{\partial x_k^{(i)}}<br>J(x^{(1)},…,x^{(n_m)})<br>$$</p>
<p>这一部分：</p>
<p><img src="/img/17_05_28/017.png" alt=""></p>
<p>对应也是下面这个代价函数的偏微分项：</p>
<p>$$<br>\frac{\partial}{\partial \theta_k^{(j)}}<br>J(\theta^{(1)},…,\theta^{(n_u)})<br>$$</p>
<blockquote>
<p>值得提醒的是，在这个公式中，我们不再用到$x_0=1$这一项，因此$x$是$n$维的（$x\in R^n$）而不是$n+1$维；$\theta$也是$n$维（$\theta\in R^n$）。</p>
<p>所以我们在做正则化的时候，也是对这$n$维的参数进行正则化，并不包括$x_0$和$\theta_0$。</p>
</blockquote>
<ul>
<li>3.最终，通过用户对应的参数向量$\theta$和训练得出的特征向量$x$，来预测用户给出的评分$\theta^Tx$。</li>
</ul>
<h1 id="低秩矩阵分解">低秩矩阵分解</h1><h2 id="矢量化：低秩矩阵分解">矢量化：低秩矩阵分解</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/CEXN0/vectorization-low-rank-matrix-factorization" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上几节中，我们谈到了<strong>协同过滤算法</strong>，本节视频中我将会讲到有关该算法的向量化实现，以及说说有关该算法你可以做的其他事情。</p>
<p>比如说，你可以做到<strong>相似产品推荐</strong>的功能。</p>
</blockquote>
<p>还是以之前的电影评分为例：</p>
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
<p>我们将这些数据存储到矩阵$Y$中：</p>
<p>$$<br>\begin{equation}<br>Y=\left[<br>\begin{matrix}<br>5&amp;5&amp;0&amp;0\\<br>5&amp;?&amp;?&amp;0\\<br>?&amp;4&amp;0&amp;?\\<br>0&amp;0&amp;5&amp;4\\<br>0&amp;0&amp;5&amp;0<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>$Y^{(i,j)}$代表第i行第j列的数据，即第i部电影的第j个用户的评分。</p>
<p>同时，我们也可以得出下面这个预测评分矩阵：</p>
<p>$$<br>\begin{equation}<br>\left[<br>\begin{matrix}<br>(\theta^{(1)})^T(x^{(1)})&amp;(\theta^{(2)})^T(x^{(1)})&amp;…&amp;(\theta^{(n_u)})^T(x^{(1)})\\<br>(\theta^{(1)})^T(x^{(2)})&amp;(\theta^{(2)})^T(x^{(2)})&amp;…&amp;(\theta^{(n_u)})^T(x^{(2)})\\<br>┋&amp;┋&amp;┋&amp;┋\\<br>(\theta^{(1)})^T(x^{(n_m)})&amp;(\theta^{(2)})^T(x^{(n_m)})&amp;…&amp;(\theta^{(n_u)})^T(x^{(n_m)})<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>其中$(\theta^{(j)})^T(x^{(i)})$代表第i个电影中第j个用户，预计给出的评分。</p>
<p>这个预测矩阵可以看做是<strong>电影的特征矩阵</strong>和<strong>用户的参数矩阵的转置</strong>的乘积。</p>
<p>电影特征矩阵如下：</p>
<p>$$<br>\begin{equation}<br>X=\left[<br>\begin{matrix}<br>(x^{(1)})^T\\<br>(x^{(2)})^T\\<br>┋\\<br>(x^{(n_m)})^T<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>其中每一个元素都是一部电影的特征向量。</p>
<p>用户的参数矩阵如下：</p>
<p>$$<br>\begin{equation}<br>\Theta=\left[<br>\begin{matrix}<br>(\theta^{(1)})^T\\<br>(\theta^{(2)})^T\\<br>┋\\<br>(\theta^{(n_u)})^T<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>其中每个元素都是一个用户的参数向量。</p>
<p>有了所有电影的特征矩阵$X$和所有用户的参数矩阵$\Theta$后，我们就可以得到上面的预测矩阵了：</p>
<p>$$<br>\begin{equation}<br>X\Theta^T=\left[<br>\begin{matrix}<br>(\theta^{(1)})^T(x^{(1)})&amp;(\theta^{(2)})^T(x^{(1)})&amp;…&amp;(\theta^{(n_u)})^T(x^{(1)})\\<br>(\theta^{(1)})^T(x^{(2)})&amp;(\theta^{(2)})^T(x^{(2)})&amp;…&amp;(\theta^{(n_u)})^T(x^{(2)})\\<br>┋&amp;┋&amp;┋&amp;┋\\<br>(\theta^{(1)})^T(x^{(n_m)})&amp;(\theta^{(2)})^T(x^{(n_m)})&amp;…&amp;(\theta^{(n_u)})^T(x^{(n_m)})<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<blockquote>
<p>我们的协同过滤算法还有另外一个名字：<strong>低秩矩阵分解（Low Rank Matrix Factorization）</strong>。</p>
</blockquote>
<h3 id="其他功能">其他功能</h3><p>最后，我们还可以通过使用协同过滤算法产生的结果做一些额外的事情。比如说<strong>找到相关的电影</strong>。</p>
<p>如何做到针对用户喜欢的某一部电影，然后推荐给他另一部类似的电影呢？</p>
<p>对于每一部电影，我们都可以学习到他的特征向量$x^{(i)} \in R^n$。</p>
<p>假设我们想找到和电影$i$最类似的一部电影，有一种方式就是求出所有电影的特征向量，然后和电影$i$的特征向量求欧氏距离，距离最小的那个就是最类似的：</p>
<p>$$<br>Small ||x^{(i)}-x^{(j)}||  → 电影i和电影j最类似<br>$$</p>
<p>类似的，如果你想找到与电影$i$最类似的5部电影，你只需求与特征向量$x^{(i)}$欧式距离最小的五部电影即可。</p>
<h2 id="实现细节：均值归一化">实现细节：均值归一化</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Adk8G/implementational-detail-mean-normalization" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>到目前为止，你已经了解到了推荐系统算法或者 协同过滤算法的所有要点。</p>
<p>在本节中，我想分享最后一点实现过程中的细节，这一点就是<strong>均值归一化</strong>。有时它可以让算法运行得更好。</p>
</blockquote>
<h3 id="动机">动机</h3><p>为了了解均值归一化这个想法的动机，我们考虑这样一个例子。</p>
<p>有一个用户Eve没有给任何电影评分：</p>
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">Eve(5)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
</tbody>
</table>
<p>$$<br>\begin{equation}<br>Y=\left[<br>\begin{matrix}<br>5&amp;5&amp;0&amp;0&amp;?\\<br>5&amp;?&amp;?&amp;0&amp;?\\<br>?&amp;4&amp;0&amp;?&amp;?\\<br>0&amp;0&amp;5&amp;4&amp;?\\<br>0&amp;0&amp;5&amp;0&amp;?<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>我们来看看协同过滤算法会对这个用户做什么。</p>
<p>假设我们的电影只有两个特征$n=2$，用户的参数向量也是2维的：$\theta^{(5)} \in R^2$。</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>由于用户没有给任何电影评分，因此代价函数的前半部分没有任何电影满足$r(i,j)=1$的条件，所以可以忽略。所以，真正影响代价函数值变化的只有这一项：</p>
<p>$$<br>\frac{\lambda}{2}<br>\sum_{j=1}^{n_u}<br>\sum_{k=1}^{n}<br>(\theta_k^{(j)})^2<br>$$</p>
<p>对应到我们的具体的两个特征上之后，就是如下的形式：</p>
<p>$$<br>\frac{\lambda}{2}<br>[<br>(\theta_1^{(5)})^2+(\theta_2^{(5)})^2<br>]<br>$$</p>
<p>因此，为了最小化这一项，我们得到的结果就是：</p>
<p>$$<br>\begin{equation}<br>\theta^{(5)}=\left[<br>\begin{matrix}<br>0\\<br>0<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>所以，我们可以得出用户对电影评分的预测结果：</p>
<p>$$<br>(\theta^{(5)})^Tx^{(1)}=0<br>$$</p>
<p>都是0颗星。</p>
<p>这个结果看起来并没有什么用，因为其实有的电影是对于某些人来说是有较高的评分的，所以某些电影是值得被推荐的。但是我们得到的预测结果都是0颗星，这个结果在暗示我们不要推荐电影给他，这其实也不够好。</p>
<p>所以，我们需要引入<strong>均值归一化</strong>来解决我们这个问题。</p>
<h3 id="均值归一化处理数据">均值归一化处理数据</h3><p>对于已知的电影评分数据：</p>
<p>$$<br>\begin{equation}<br>Y=\left[<br>\begin{matrix}<br>5&amp;5&amp;0&amp;0&amp;?\\<br>5&amp;?&amp;?&amp;0&amp;?\\<br>?&amp;4&amp;0&amp;?&amp;?\\<br>0&amp;0&amp;5&amp;4&amp;?\\<br>0&amp;0&amp;5&amp;0&amp;?<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>我们要做的就是计算每个电影所得评分的均值，用向量$μ$表示：</p>
<p>$$<br>\begin{equation}<br>μ=\left[<br>\begin{matrix}<br>2.5\\<br>2.5\\<br>2\\<br>2.25\\<br>1.25<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>然后将所有评分减去平均评分，得到新的$Y$：</p>
<p>$$<br>\begin{equation}<br>Y=\left[<br>\begin{matrix}<br>2.5&amp;2.5&amp;-2.5&amp;-2.5&amp;?\\<br>2.5&amp;?&amp;?&amp;-2.5&amp;?\\<br>?&amp;2&amp;-2&amp;?&amp;?\\<br>-2.25&amp;-2.25&amp;2.75&amp;1.75&amp;?\\<br>-1.25&amp;-1.25&amp;3.75&amp;-1.25&amp;?<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>这样做的目的，<strong>就是把每部电影的评分的平均数都调整为0</strong>。</p>
<p>然后我们对经过归一化处理之后的矩阵使用协同过滤算法，即使用户没有对电影做出过评分，那么我们得到的参数向量是：</p>
<p>$$<br>\begin{equation}<br>\theta^{(5)}=\left[<br>\begin{matrix}<br>0\\<br>0<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>我们最终预测用户给出的分值是：</p>
<p>$$<br>(\theta^{(5)})^Tx^{(1)} + μ_i<br>$$</p>
<p>在这个例子中，我们预测Eva对五部电影初始化的评分分别为：2.5，2.5，2，2.25，1.25。</p>
<p>因为：</p>
<p>$$<br>\begin{equation}<br>μ=\left[<br>\begin{matrix}<br>2.5\\<br>2.5\\<br>2\\<br>2.25\\<br>1.25<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/05/26/斯坦福机器学习课程 第九周 (3)多元高斯分布（选学）/" itemprop="url">
                斯坦福机器学习课程 第九周 (3)多元高斯分布（选学）
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-05-26T21:00:00+08:00" content="2017-05-26">
            2017-05-26
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/05/26/斯坦福机器学习课程 第九周 (3)多元高斯分布（选学）/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/05/26/斯坦福机器学习课程 第九周 (3)多元高斯分布（选学）/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="多元高斯分布">多元高斯分布</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Cf8DF/multivariate-gaussian-distribution" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>接下来我将介绍<strong>多元高斯分布 (multivariate Gaussian distribution)</strong>。它是我们目前所学的异常检测算法的延伸，它有一些优势也有一些劣势，它能捕捉到一些之前的算法检测不出来的异常。</p>
</blockquote>
<h3 id="异常检测算法无法解决的问题">异常检测算法无法解决的问题</h3><p>还是现以一个例子来解释：</p>
<p>假设在数据中心监控机器的例子中，我们有如下的内存和CPU使用数据：</p>
<p><img src="/img/17_05_26/001.png" alt=""></p>
<p>其中对于这两个维度的数据，都服从正态分布：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/002.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/003.png" alt=""></td>
</tr>
</tbody>
</table>
<p>如果现在在我们的测试集中，有一个异常数据点出现在下图的位置中：</p>
<p><img src="/img/17_05_26/004.png" alt=""></p>
<p>那么在这种情况下我们会发现，这一点对应的两个维度下的概率其实都不低，从$p(x)$的结果上，我们无法准确预测这个样本是否属于异常。</p>
<p>产生这个问题的实际原因其实是这样的，从$x_1$和$x_2$这两个维度来看，我们的正常数据及时大多数集中分布在这样一个范围内：</p>
<p><img src="/img/17_05_26/005.png" alt=""></p>
<p>但我们使用之前的异常检测算法，其实是以中心区域向外以正圆的形式扩散的。也就是说距离中心区域距离相等的点，对应的$p(x)$都是一样的，所以我们可能无法检测到这一个异常样本，因为它也处在一个$p(x)$比较大的范围内：</p>
<p><img src="/img/17_05_26/006.png" alt=""></p>
<p>所以，为了解决异常检测算法的这一问题，接下来我解释改良版的异常检测算法，要用到叫做<strong>多元高斯分布（多元正态分布）</strong>的东西。</p>
<h3 id="通过多元高斯分布改良异常检测算法">通过多元高斯分布改良异常检测算法</h3><p>在多元高斯分布中，对于n维特征$x \in R^n$，不要把模型$p(x_1)$,$p(x_2)$,…,$p(x_n)$分开，而要建立$p(x)$整体的模型。</p>
<p>多元高斯分布的参数包括向量$µ$和一个$n×n$的矩阵$Σ$。</p>
<p>$$<br>µ \in R^n  \\<br>Σ \in R^{n×n}<br>$$</p>
<p>$Σ$被称为<strong>协方差矩阵</strong>，它类似于我们之前学习PCA的时候所见到的协方差矩阵。</p>
<p>带入之后计算$p(x)$：</p>
<p>$$<br>p(x;µ,Σ)=<br>\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}<br>exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))<br>$$</p>
<blockquote>
<p>这个公式不用背，用的时候再去查。</p>
</blockquote>
<p>注意，公式中的$|Σ|$这一项，代表矩阵$Σ$的<strong>行列式</strong>。在Octave中可以用下面的代码来计算：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="title">det</span><span class="params">(Sigma)</span></span></div></pre></td></tr></table></figure>
<p>其实这个公式是什么样并不重要，更重要的是$p(x)$到底是什么样。</p>
<h3 id="多元高斯分布的样子">多元高斯分布的样子</h3><p>我们来对比一下不同的$µ$和不同的$Σ$组合后，对应的$p(x)$的形状。</p>
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}0.6&amp;0\\0&amp;0.6\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}2&amp;0\\0&amp;2\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/009.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/011.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/010.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/012.png" alt=""></td>
</tr>
</tbody>
</table>
<p>表格从上到下依次是三种情况对应的参数、三维图像以及俯视图。</p>
<p>$µ$作为均值，象征着中心区域对应的坐标点。$Σ$是协方差矩阵，它衡量的是特征$x_1$和$x_2$的方差。</p>
<p>从图中可以看出来，当缩小协方差$Σ$时，中心区域的凸起就会变得更细长；当扩大协方差$Σ$时，中心区域的凸起就会变得更扁平。因为概率分布的积分必须等于1，所以如果你缩小方差，就会变得细长，反之就会变得扁平。</p>
<p>接下来我们尝试对协方差中使用不同的数值，来观测$p(x)$形状的变化：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}0.6&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}2&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/013.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/015.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/014.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/016.png" alt=""></td>
</tr>
</tbody>
</table>
<p>在协方差$Σ$中，左上角元素对应的是$x_1$特征，右下角元素对应的是$x_2$特征。</p>
<p>在第二组图中，我们可以看到，当我们缩小$x_1$到原先的0.6倍而$x_2$保持原先的大小时，由于相当于是对特征$x_1$的方差进行了缩小，所以图像在$x_1$的方向上，会显得更细长。</p>
<p>在第三组图中，我们可以看到，当我们放大$x_1$到原先的2倍而$x_2$保持原先的大小时，由于相当于是对特征$x_1$的方差进行了放大，所以图像在$x_1$的方向上，会显得更扁平。</p>
<p>对于多元高斯分布来说，一个很棒的事情就是我们可以用它来<strong>对数据的相关性建模</strong>。也就是说，我们可以用它来给$x_1$和$x_2$高度相关的情况建立模型。具体来说，我们可以通过改变协方差$Σ$非对角线上的元素来得到不同的高斯分布：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0.5\\0.5&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0.8\\0.8&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/017.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/019.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/018.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/020.png" alt=""></td>
</tr>
</tbody>
</table>
<p>可以看出来，当我改变了非对角线上元素的值时，$p(x)$的图像也变得倾斜了；当我增大了这些元素时，这个倾斜的分布图像变得更细长了。</p>
<p>上面是我们把这些非对角线上元素设置为正数时的样子，那么如果我们把它们设置为负数时，会是什么样呢？</p>
<p>它们的倾斜方向会发生改变：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;-0.5\\-0.5&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;-0.8\\-0.8&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/021.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/023.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/022.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/024.png" alt=""></td>
</tr>
</tbody>
</table>
<p>如果我们改变$µ$，会对图像$p(x)$产生什么影响呢？</p>
<p>它们会发生平移：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0.5\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}1.5\\-0.5\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/025.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/027.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/026.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/028.png" alt=""></td>
</tr>
</tbody>
</table>
<p>$µ$有两个元素，第一个元素对应的是图像在$x_1$方向上的位移，第二个元素对应的是图像在$x_2$方向上的位移。当为正数时，是沿着增大的方向平移，反之是沿着缩小的方向平移。</p>
<h3 id="总结">总结</h3><p>不同的图片，能够帮助你了解<strong>多元高斯分布</strong>所能描述的概率分布是什么样。它最重要的优势就是能够描述当两个特征变量之间可能存在正相关或者是负相关关系的情况。</p>
<h2 id="通过多元高斯分布来处理异常检测问题">通过多元高斯分布来处理异常检测问题</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/DnNr9/anomaly-detection-using-the-multivariate-gaussian-distribution" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上一节中，我们谈到了多元高斯分布，而且也看到了一些例子通过改变参数$µ$和$Σ$来给不同的概率分布建模。</p>
<p>在这节中，我们使用它们来开发另一种异常检测算法。</p>
</blockquote>
<h3 id="知识回顾">知识回顾</h3><p>再回顾一下<strong>多元高斯分布（或者叫多元正态分布）</strong>：</p>
<p>有两个参数：$µ$和$Σ$。</p>
<p>$µ$是一个n维向量，协方差矩阵$Σ$是一个$n×n$的矩阵。</p>
<p>其对应的概率分布公式如下：</p>
<p>$$<br>p(x;µ,Σ)=<br>\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}<br>exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))<br>$$</p>
<p>随着你改变$µ$和$Σ$，你可以得到一系列不同的概率分布：</p>
<p><img src="/img/17_05_26/029.png" alt=""></p>
<h4 id="参数拟合">参数拟合</h4><p>接下来让我们谈一下<strong>参数拟合问题（参数估计问题）</strong>。</p>
<p>如果我有一组符合高斯分布的样本：</p>
<p>$$<br>｛x^{(1)},x^{(2)},…,x^{(m)}｝<br>$$</p>
<p>我们可以通过公式来得到参数$µ$和$Σ$：</p>
<p>$$<br>µ=\frac{1}{m}\sum^m_{i=1}x^{(i)}<br>$$</p>
<p>$$<br>Σ=\frac{1}{m}\sum^m_{i=1}(x^{(i)}-µ)(x^{(i)}-µ)^T<br>$$</p>
<h4 id="具体应用步骤">具体应用步骤</h4><p>有了这两个参数值，我们就可以把他们应用到具体的异常检测算法中了。具体步骤是这样的：</p>
<p>假设我们有如下的训练样本：</p>
<p><img src="/img/17_05_26/030.png" alt=""></p>
<ul>
<li>首先，用我们的训练集来拟合模型$p(x)$，得到参数$µ$和$Σ$：</li>
</ul>
<p>$$<br>µ=\frac{1}{m}\sum^m_{i=1}x^{(i)}<br>$$</p>
<p>$$<br>Σ=\frac{1}{m}\sum^m_{i=1}(x^{(i)}-µ)(x^{(i)}-µ)^T<br>$$</p>
<ul>
<li>然后，当你得到一个新的测试样本时，我们用下面的公式来计算其$p(x)$：</li>
</ul>
<p>$$<br>p(x;µ,Σ)=<br>\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}<br>exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))<br>$$</p>
<ul>
<li>最后，如果$p(x)&lt;ε$时，就把它标记为是一个异常样本，反之，如果$p(x)&gt;=ε$则不标记为异常样本。</li>
</ul>
<p>所以，如果使用多元高斯分布来解决异常检测问题，你可能会得到这样一个高斯分布的概率模型：</p>
<p><img src="/img/17_05_26/031.png" alt=""></p>
<p>所以，他可以正常的识别出之前用普通的异常检测算法无法正确检测的那个异常样本。</p>
<h3 id="多元高斯模型和原始模型的关系">多元高斯模型和原始模型的关系</h3><p>最后说一下<strong>多元高斯分布模型</strong>和原来的模型它们之间的关系。</p>
<p>原先的模型是这样的：</p>
<p>$$<br>p(x)=p(x_1;µ_1,σ_1^2)×p(x_2;µ_2,σ_2^2)×…×p(x_n;µ_n,σ_n^2)<br>$$</p>
<p>事实上，你可以证明我们原先的这种模型，是多元高斯模型的一种。它其实是一种等高线都沿着坐标轴方向的多元高斯分布，但这里我不给出证明过程：</p>
<p><img src="/img/17_05_26/032.png" alt=""></p>
<p>所以这三个图像，全都是你可以用原来的模型来拟合的高斯分布的例子。</p>
<p>其实这个模型对应于一种多元高斯分布的特例，具体来说这个特例被定义为约束$p(x)$的分布(也就是多元高斯分布$p(x)$)，使得它的概率密度函数的等高线是沿着轴向的。也就是要求<strong>协方差矩阵$Σ$的非对角线元素都为0</strong>。</p>
<p>所以你可以得到多元高斯分布$p(x)$看起来是上图中这三种样式的。你会发现，在这3个例子中，它们的轴都是沿着$x_1$，$x_2$的轴的。</p>
<p>因此，在<strong>协方差矩阵$Σ$的非对角线元素都为0</strong>的情况下，这两者是相同的：</p>
<p>$$<br>p(x)=p(x_1;µ_1,σ_1^2)×p(x_2;µ_2,σ_2^2)×…×p(x_n;µ_n,σ_n^2)<br>$$</p>
<p>$$<br>p(x;µ,Σ)=<br>\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}<br>exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))<br>$$</p>
<h3 id="何时使用多元高斯模型？何时使用原始模型？">何时使用多元高斯模型？何时使用原始模型？</h3><p>既然我们知道了原始的模型是多元高斯模型的一个特例，那么应该在什么时候用哪个模型呢？</p>
<p>事实情况是，原始模型比较常用，而多元高斯模型比较少用。</p>
<p>假设在你的样本中，$x_1$和$x_2$是线性相关的特征组合，下面是这两种算法在处理不正常的特征组合时的具体方式对比：</p>
<table>
<thead>
<tr>
<th style="text-align:center">原始模型</th>
<th style="text-align:center">多元高斯模型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">捕捉到这两个特征，建立一个新的特征$x_3$(比如$x_3=\frac{x_1}{x_2}$)，去尝试手工组合并改变这个新的特征变量，从而使得算法能很好的工作。</td>
<td style="text-align:center">自动捕捉不同特征变量之间的相关性。</td>
</tr>
<tr>
<td style="text-align:center">运算量小(更适用于特征变量个数$n$很大的情况)</td>
<td style="text-align:center">计算更复杂（Σ是$n×n$的矩阵，这里会涉及两个$n×n$的矩阵相乘的逻辑，计算量很大）</td>
</tr>
<tr>
<td style="text-align:center">即使训练样本数$m$很小的情况下，也能工作的很好</td>
<td style="text-align:center">必须满足$m&gt;n$，或者$Σ$不可逆（奇异矩阵）。这种情况下，还可以帮助你省去为了捕捉特征值组合而手动建立额外特征变量所花费的时间。</td>
</tr>
</tbody>
</table>
<h4 id="一种异常情况的应对">一种异常情况的应对</h4><p>当你在拟合多元高斯模型时，如果你发现协方差矩阵$Σ$是<strong>奇异的（不可逆的）</strong>，一般只有两种情况：</p>
<ul>
<li>第一种是它没有满足$m&gt;n$的条件</li>
<li>第二种情况是，你有冗余特征变量 <ul>
<li>冗余特征变量的意思是出现了以下两种情况的任意一种：<ul>
<li>出现了两个完全一样的特征变量（你可能不小心把同一个特征变量复制了两份）</li>
<li>如果你有$x_3=x_4+x_5$，这里$x_3$其实并没有包含额外的信息，相对于$x_4$和$x_5$来说，它就是冗余特征变量。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>这是你调试算法时的一个小知识，可能很少会遇到，但是一旦你发现$Σ$不可逆，那么首先需要从这两个方面来考虑解决方案。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/05/25/斯坦福机器学习课程 第九周 (2)构建一个异常检测系统/" itemprop="url">
                斯坦福机器学习课程 第九周 (2)构建一个异常检测系统
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-05-25T21:14:00+08:00" content="2017-05-25">
            2017-05-25
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/05/25/斯坦福机器学习课程 第九周 (2)构建一个异常检测系统/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/05/25/斯坦福机器学习课程 第九周 (2)构建一个异常检测系统/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="异常检测系统的开发与评估">异常检测系统的开发与评估</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Mwrni/developing-and-evaluating-an-anomaly-detection-system" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上一节中，我们推导了异常检测算法。在本节，我想介绍一下如何开发一个异常检测的应用来解决一个实际问题。</p>
</blockquote>
<p>在之前，我们已经提到了使用实数评估法的重要性。这样做的想法是，当你在用某个学习算法来开发一个具体的机器学习应用时，你常常需要做出很多决定，比如说选择用什么样的特征等等。而如果你找到某种评估算法的方式，比如直接返回一个数字，来告诉你算法的好坏，那么你做这些决定就显得更容易了。有了这样的数字，你就可以更好的确定某些特征是否需要在构建算法的时候考虑进来了，因为你可以通过对比算法在有这个特征和没这个特征的情况下，算法的具体表现，来决定是否要加入这个特征。</p>
<p>所以对于异常检测系统的评价方式很重要。</p>
<p>为了做到能评价一个异常检测系统，我们先假定已有了一些带标签的数据。</p>
<blockquote>
<p>我们要考虑的异常检测问题是一个非监督问题，使用的是无标签数据。但如果你有一些带标签的数据，能够指明哪些是异常样本，哪些是非异常样本，那么这就是我们要找的能够评价异常检测算法的标准方法。</p>
</blockquote>
<p>还是以飞机发动机的为例，现在假如你有了一些带标签数据，我们用$y=0$表示完全正常的样本，用$y=1$表示有异常的样本。</p>
<p>那么异常检测算法的推导和评价方法如下所示：</p>
<h3 id="数据集划分">数据集划分</h3><ul>
<li>我们先考虑训练样本。对于训练集，我们还是需要把数据看成是无标签的，通常来讲我们把这些样本都看成<strong>正常</strong>的，但可能有一些异常的也被分到你的训练集里，这也没关系（毕竟异常的是少数）：</li>
</ul>
<p>$$<br>x^{(1)},x^{(2)},…,x^{(m)}<br>$$</p>
<ul>
<li>接下来我们要定义交叉验证集和测试集。通过这两个集合我们将得到异常检测算法。</li>
</ul>
<p>$$<br>交叉验证集：(x^{(1)}_{cv},y^{(1)}_{cv}),…,(x^{(m)}_{cv},y^{(m)}_{cv})<br>$$</p>
<p>$$<br>测试集：(x^{(1)}_{test},y^{(1)}_{test}),…,(x^{(m)}_{test},y^{(m)}_{test})<br>$$</p>
<p>继续回到我们的例子中：</p>
<p>假如说我们有10000制造的引擎作为样本。就我们所知，这些样本都是正常没有问题的飞机引擎。同样地，如果有一小部分有问题的引擎也被混入了这10000个样本，别担心，没有关系，我们假设这10000个样本中大多数都是没有问题的引擎。而且实际上从过去的经验来看，无论是制造了多少年引擎的工厂，在10000个引擎中都会得到大概20个有问题的引擎。对于异常检测的典型应用来说，异常样本的个数(也就是$y=1$的样本)，基本上很多都是20到50个，并且通常我们的正常样本的数量要大得多。</p>
<p>有了这组数据，把数据分为训练集、交叉验证集和测试集的一种典型的分法如下：</p>
<p>我们把这10000个正常的引擎放6000个到无标签的<strong>训练集</strong>中，我叫它“无标签训练集”，但其实所有这些样本都对应$y=0$的情况。</p>
<p>我们要用这6000个训练样本来拟合$p(x)$。</p>
<p>$$<br>p(x)=p(x_1;μ_1, σ_1^2)…p(x_n;μ_n, σ_n^2)<br>$$</p>
<p>因此我们就是要用这6000个样本来计算参数$μ_1,σ_1^2…μ_n,σ_n^2$。</p>
<p>然后我们将剩余的4000个样本一半放入<strong>交叉验证集</strong>，另一半放入<strong>测试集</strong>中。同时我们还有20个异常的发动机样本，同样也把它们进行一个分割：放10个到验证集中，剩下10个放入测试集中。</p>
<blockquote>
<p>注意：不要把交叉验证集和测试集混在一起使用，这样效果并不好。</p>
</blockquote>
<h3 id="异常检测算法的推导和评估方法">异常检测算法的推导和评估方法</h3><p>有了训练集、交叉验证集和测试集，异常检测算法的推导和评估方法如下：</p>
<p>首先我们使用训练样本来拟合模型$p(x)$。</p>
<p>然后我们预设一个比较小的$ε$，对于$p(x)&lt;ε$的样本视为异常样本，然后分别在测试集合交叉验证集上进行测试和验证。我们知道在测试集合交叉验证集上是存在$y=1$的异常样本的，只不过量比较少而已。</p>
<p>这里其实我们可以把异常检测算法想象成是对交叉验证集和测试集中的$y$进行预测，这与监督学习有些类似，因为我们在对有标签的数据进行预测。所以我们可以通过对标签预测正确的次数来评价算法的好坏。</p>
<p>当然这些标签会比较偏斜，因为$y=0$(也就是正常的样本)肯定是比出现$y=1$(也就是异常样本)的情况更多。这跟我们在监督学习中用到的评价度量方法非常接近。</p>
<p>那么用什么评价度量好呢？</p>
<p>因为数据是非常偏斜的，所以通过分类准确度来衡量算法并不是一个好的度量法。我们<strong><a href="http://t.cn/RSh83NE" target="_blank" rel="external">之前的课程</a></strong>中也有提到过，如果你有一个比较偏斜的数据集，那么总是预测$y=0$它的分类准确度自然会很高。</p>
<p>因此我们应该算出以下数据来更科学的衡量算法的好坏：</p>
<ul>
<li>我们应该算出<strong>真阳性</strong>、<strong>假阳性</strong>、<strong>假阴性</strong>和<strong>真阴性</strong>的比率来作为评价度量值</li>
<li>我们也可以算出<strong>查准率</strong>和<strong>召回率</strong></li>
<li>计算出$F_1-score$，通过一个很简单的数字来总结出查准和召回的大小。</li>
</ul>
<p>通过这些方法，你就可以评价你的异常检测算法在交叉验证和测试集样本中的表现。</p>
<h4 id="ε是怎么得到的呢？">ε是怎么得到的呢？</h4><p>现在还有一个问题没有说明，那就是参数$p(x)&lt;ε$中的$ε$是如何求得的？</p>
<p>如果你有一组交叉验证集样本，一种选择参数$ε$的方法就是通过尝试多个不同的$ε$，然后选出一个使得$F_1-score$最大的$ε$，这个$ε$就是在交叉验证集上表现最好的。</p>
<p>更一般来说,我们使用训练集、测试集和交叉验证集的方法是当我们需要作出决定时，比如要包括哪些特征或者说要确定参数$ε$取多大合适，我们就可以不断地用交叉验证集来评价这个算法，然后决定我们应该用哪些特征，以及选择哪一个$ε$。</p>
<p>所以就是在交叉验证集中评价算法，然后选出一组特征，或者找到能符合我们要求的ε的值后，我们就能用测试集来最终评价算法的表现了。</p>
<h2 id="异常检测_VS_监督学习">异常检测 VS 监督学习</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Rkc5x/anomaly-detection-vs-supervised-learning" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上一节，我们谈到如何评价一个异常检测算法。我们先是用了一些带标签的数据，以及一些我们知道是异常或者正常的样本(用$y=1$或$y=0$来表示)。</p>
<p>这就引出了这样一个问题：既然我们有了带标签的数据，那么为什么我们不直接用监督学习的方法（比如逻辑回归或者神经网络）呢？</p>
<p>这一节，就来介绍一下什么时候应该用异常检测算法，什么时候用监督学习算法是更有成效的。</p>
</blockquote>
<p>下面这张表格对比了什么时候应该用什么算法：</p>
<table>
<thead>
<tr>
<th style="text-align:center">异常检测</th>
<th style="text-align:center">监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">负样本量很少(通常是在0到20个之间),正样本很多的时候</td>
<td style="text-align:center">正负样本都很多的时候</td>
</tr>
<tr>
<td style="text-align:center">有多种不同的异常类型时（因为对任何算法来说，从大量正样本中去学习到异常具体是什么，都是困难的）；未知的异常与我们已知的异常完全不一样时。</td>
<td style="text-align:center">有足够多的正样本来让你的算法学习到对应的特征，并且在未知的正样本中，也和已知的样本是类似的。</td>
</tr>
</tbody>
</table>
<p>这就是在遇到具体情况时，要选择异常检查还是监督学习的方式。</p>
<p>其实关键区别就是<strong>在异常检测算法中我们只有一小撮正样本</strong>，因此监督学习算法不能从这些样本中学到太多东西。</p>
<hr>
<p>关于上面表格中，有关<strong>异常检测</strong>中的不同类型的异常情况，我们用之前的垃圾邮件的例子来说明。</p>
<p>在那个例子中，垃圾邮件的类型其实也有很多种。有的是想卖东西给你、有的是想钓出你的密码(这种就叫钓鱼邮件)、还有其他一些类型的垃圾邮件…但对于垃圾邮件的问题，我们能得到绝大多数不同类型的垃圾邮件，因为我们有大量的垃圾邮件样本的集合。因此这也是为什么我们通常把垃圾邮件问题看作是监督学习问题的原因，虽然垃圾邮件的种类通常有太多太多 。</p>
<p>因此，我们可以看看一些异常检测的应用和监督学习应用的比较，我们不难发现对于欺诈检测(fraud detection)，如果你掌握了许多种不同类型的诈骗方法，并且只有相对较小的训练集（只有很少一部分用户有异常行为）那我会使用异常检测算法。当然，有时候欺诈检测的方法也可能会偏向于使用监督学习算法，但是如果你并没有看到许多在你网站上进行异常行为的用户样本，那么欺诈检测通常还是被当做是一个异常检测算法，而不是一个监督学习算法。</p>
<h2 id="选择使用什么特征">选择使用什么特征</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/LSpXm/choosing-what-features-to-use" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>前面我们讲到了异常检测算法，并且我们也讨论了如何评估一个异常检测算法。</p>
<p>事实上当你应用异常检测时，对它的效率影响最大的因素之一，是你使用什么特征变量。那么在本节，我将给出一些关于如何设计或选择异常检测算法的特征变量建议。</p>
</blockquote>
<h3 id="对不服从高斯分布的数据进行转换">对不服从高斯分布的数据进行转换</h3><p>在我们的异常检测算法中，我们做的事情之一就是使用正态(高斯)分布来对特征向量建模。通常情况下，我们都需要用直方图来可视化这些数据，如下图：</p>
<p><img src="/img/17_05_25/001.png" alt=""></p>
<p>这么做的原因是为了在使用算法之前，确保我们的数据看起来是服从高斯分布的（当然即使你的数据并不是高斯分布，它也基本上可以良好地运行，但最好转换成高斯分布的样式之后在带入计算）。</p>
<p>如果你的样本的某个特征展示效果完全不像一个正态分布的形状：</p>
<p><img src="/img/17_05_25/002.png" alt=""></p>
<p>那么我们就需要对数据进行一些转换，来确保这些数据能看起来更像高斯分布。这样你的算法才能效果更好。</p>
<p>一般情况下，我们都会对原始数据尝试求对数或者开根号操作进行转换，下图是通过对数来转换的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x$</th>
<th style="text-align:center">$log(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_25/002.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/003.png" alt=""></td>
</tr>
</tbody>
</table>
<p>你也可以尝试使用以下的方式来带入：</p>
<p>$$<br>x←log(x)  \\<br>x←log(x + c) \\<br>x←\sqrt x<br>$$</p>
<p>选择哪一个都可以，唯一的原则就是保证转换后的分布看起来更像高斯分布(正态分布)一些。</p>
<hr>
<p>下面是对于一个不服从高斯分布的数据进行转换的过程：</p>
<table>
<thead>
<tr>
<th style="text-align:center">原始数据</th>
<th style="text-align:center">$x^{0.5}$</th>
<th style="text-align:center">$x^{0.2}$</th>
<th style="text-align:center">$x^{0.1}$</th>
<th style="text-align:center">$x^{0.05}$</th>
<th style="text-align:center">$log(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_25/004.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/005.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/006.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/009.png" alt=""></td>
</tr>
</tbody>
</table>
<p>我们对原始数据尝试了不同的转换之后，图像最终趋于了正太分布的样式。在Octive中实现的过程如下：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hist(<span class="name">x</span>,<span class="number">50</span>)</div><div class="line">hist(<span class="name">x</span>.^<span class="number">0.5</span>,<span class="number">50</span>)</div><div class="line">hist(<span class="name">x</span>.^<span class="number">0.2</span>,<span class="number">50</span>)</div><div class="line">hist(<span class="name">x</span>.^<span class="number">0.1</span>,<span class="number">50</span>)</div><div class="line">hist(<span class="name">x</span>.^<span class="number">0.05</span>,<span class="number">50</span>)</div><div class="line">hist(<span class="name">log</span>(<span class="name">x</span>),<span class="number">50</span>)</div><div class="line">xNew = log(<span class="name">x</span>)<span class="comment">;</span></div></pre></td></tr></table></figure>
<p>最终我们选择了$log(x)$来代替原来的$x$。</p>
<h3 id="异常检测算法的特征变量的获取">异常检测算法的特征变量的获取</h3><p>对于异常检测算法的特征变量的获取的方法，其实和之前学习的误差分析步骤是类似的。</p>
<ul>
<li>首先我们先训练处一个异常检测学习算法。</li>
<li>然后在一组交叉验证集上运行算法。</li>
<li>然后找出那些异常样本。</li>
<li>然后我们尝试其他的特征变量，看是否能让我们的算法变得更好。</li>
</ul>
<h4 id="举例说明">举例说明</h4><p>我们用一个具体的例子来说明上面的过程：</p>
<p>在异常检测中，我们希望$p(x)$的值对正常样本来说是比较大的，而对异常样本来说，值是很小的。但一个很常见的问题是$p(x)$是具有可比性的（即对于两者都很大）。</p>
<p>这是我的一组无标签数据：</p>
<p><img src="/img/17_05_25/010.png" alt=""></p>
<p>这里我只有一个特征变量$x_1$，我要用高斯分布来拟合它。</p>
<p>假设我们绘制出它的高斯分布，如下图所示：</p>
<p><img src="/img/17_05_25/011.png" alt=""></p>
<p>假设我们遇到了一个有异常的样本：</p>
<p><img src="/img/17_05_25/012.png" alt=""></p>
<p>但是，从图中我们能看出这个样本在这一特征下的$p(x_1)$并不低，我们无法从这一特征下区分出这一异常样本。</p>
<p>如果我们引入另一个特征$x_2$，图像如下：</p>
<p><img src="/img/17_05_25/013.png" alt=""></p>
<p>再来看看我们的异常样本，出现在了这两个特征所分布的区域的外侧：</p>
<p><img src="/img/17_05_25/014.png" alt=""></p>
<p>这个时候，我们的异常检测算法就会给出很小的值来验证这一点代表的样本属于异常样本。</p>
<blockquote>
<p><strong>总结</strong>:选择异常检测需要考虑的特征时，先找出异常样本，然后尝试通过引入新的特征来验证对异常样本的识别的准确性。如果有所提高，就可以考虑引入这个特征。</p>
</blockquote>
<h4 id="关于特征选择的思考">关于特征选择的思考</h4><p>最后我想与你分享一些我平时在为异常检查算法选择特征变量时的一些思考。</p>
<p>通常，我会选择那些既不是特别大也不是特别小的特征变量。以数据中心异常计算机的监测的例子为例，我们有以下四个特征：</p>
<p>$$<br>x_1=机器内存使用  \\<br>x_2=硬盘资源使用  \\<br>x_3=CPU使用  \\<br>x_4=网络情况<br>$$</p>
<p>我假设CPU使用情况和网络情况呈线性关系，正常情况下如果其中一个服务器正在运行时，CPU负载和流量都很大。</p>
<p>现在，假设有一种异常情况，就是流量消耗很小，但CPU负载却很高，因为可能是机器遇到了某个死循环导致CPU负载飙升，因此我们可以定义一个新的特征变量来更好的说明这一情况：</p>
<p>$$<br>x_5=\frac{CPU 负载}{流量消耗}<br>$$</p>
<p>同样，你也可以尝试使用下面这种特征变量：</p>
<p>$$<br>x_6=\frac{(CPU 负载)^2}{流量消耗}<br>$$</p>
<p>其实，解决这类问题的思路就是尝试组合新的特征，从而能更好的检测异常情况。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/05/23/斯坦福机器学习课程 第九周 (1)密度估计/" itemprop="url">
                斯坦福机器学习课程 第九周 (1)密度估计
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-05-23T21:59:00+08:00" content="2017-05-23">
            2017-05-23
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/05/23/斯坦福机器学习课程 第九周 (1)密度估计/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/05/23/斯坦福机器学习课程 第九周 (1)密度估计/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="动机:异常检测">动机:异常检测</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/V9MNG/problem-motivation" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在接下来的一系列课程中，我将向大家介绍<strong>异常检测(Anomaly detection)问题</strong>。这是机器学习算法的一个常见应用。这种算法的一个有趣之处在于它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。</p>
</blockquote>
<p>首先，我们举一个例子来解释什么是异常检测。</p>
<p>假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)。而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如你可能测量了引擎运转时产生的热量，或者引擎的振动等等（我有一些朋友很早之前就开始进行这类工作，在实际工作中他们确实会从真实的飞机引擎采集这些特征变量）：</p>
<p>$$<br>x_1 = 产生的热量 \\<br>x_2 = 振动的强度 \\<br>…<br>$$</p>
<p>这样一来，如果你生产了m个引擎的话，你就有了一个从$x^{(1)}$到$x^{(m)}$的数据集：</p>
<p>$$<br>｛x^{(1)}，x^{(2)}，…，x^{(m)}｝<br>$$</p>
<p>也许你会将这些数据绘制成图表，看起来就是这个样子：</p>
<p><img src="/img/17_05_23/001.png" alt=""></p>
<p>图中每个叉都是你的无标签数据。</p>
<p>假设有一天，你有一个新的飞机引擎从生产线上流出，这个引擎的数据为$x_{test}$。那么所谓的<strong>异常检测问题</strong>就是检查这个新的飞机引擎是否存在某种异常。</p>
<p>比如说，如果你的新引擎对应的点落在这里：</p>
<p><img src="/img/17_05_23/002.png" alt=""></p>
<p>从数据上来看，它看起来像我们之前见过的引擎，因此我们可以直接认为它是正常的。然而如果你的新飞机引擎的$x_{test}$对应的点在这外面：</p>
<p><img src="/img/17_05_23/003.png" alt=""></p>
<p>那么我们可以认为这是一个异常的引擎。</p>
<h3 id="异常检测问题更正式的定义">异常检测问题更正式的定义</h3><p><strong>异常检测问题</strong>更正式一些的定义如下：</p>
<p>假设我们有$m$个<strong>正常的</strong>样本数据$｛x^{(1)}，x^{(2)}，…，x^{(m)}｝$，我们需要一个算法来告诉我们一个新的样本数据$x_{test}$是否异常。</p>
<p>我们要采取的方法是：给定无标签的训练集，对数据集$x$建立一个概率分布模型$p(x)$。当我们建立了$x$的概率模型之后，我们就会说，对于新的飞机引擎$x_{test}$，如果概率$p$低于阈值$ε$：</p>
<p>$$<br>p(x_{test}) \lt ε<br>$$</p>
<p>那么就将其标记为异常。</p>
<p>因此当我们看到一个新的引擎在我们根据训练数据得到的$p(x_{test})$模型中概率非常低时，我们就将其标记为异常；反之如果$p(x_{test})$大于给定的阈值$ε$，我们就认为它是正常的。</p>
<hr>
<p>因此在上面的飞机引擎的例子中，对于给定的训练集，对于图中的中心区域的$p(x)$会很大；而稍微远离中心区域的点概率会小一些；更远的地方的点，它们的概率将更小；在外面的点将成为异常点：</p>
<table>
<thead>
<tr>
<th style="text-align:center">中心区域$p(x)$很大</th>
<th style="text-align:center">$p(x)$略小</th>
<th style="text-align:center">$p(x)$很小</th>
<th style="text-align:center">异常点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_23/004.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/005.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/006.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/007.png" alt=""></td>
</tr>
</tbody>
</table>
<h3 id="异常检测的应用场景">异常检测的应用场景</h3><h4 id="欺诈检测">欺诈检测</h4><p>异常检测最常见的应用就是欺诈检测了。</p>
<p>假设你有很多用户，你的每个用户都在从事不同的活动。你可以对不同的用户活动计算特征变量，然后建立一个用来表示用户行为对应的特征向量出现的概率的模型（用来表示用户表现出各种行为的可能性的模型）。</p>
<p>因此假设你看到某个用户在网站上行为的特征变量是这样的：</p>
<p>$$<br>\begin{align*}<br>x_1&amp;:是用户登录的频率 \\<br>x_2&amp;:是用户访问某个页面的次数 \\<br>x_3&amp;:是用户在论坛上发帖的次数 \\<br>x_4&amp;:是用户的打字速度（有些网站是可以记录的）<br>\end{align*}<br>$$</p>
<p>因此你可以根据这些数据建一个模型$p(x)$，然后你可以通过这个模型来发现你网站上的行为奇怪的用户。你只需要看哪些用户的$p(x)\ltε$即可，接下来你就可以对这些用户的档案做进一步筛选，或者要求这些用户 验证他们的身份，从而让你的网站防御异常行为或者欺诈行为。</p>
<p>这种技术将会找到行为不正常的用户，而不仅仅是有欺诈行为的用户。然而这就是许多许多在线购物网站常常用来识别异常用户的技术。</p>
<h4 id="工业领域查找异常产品">工业领域查找异常产品</h4><p>异常检测的另一个例子是在工业生产领域，事实上我们上面已经谈到过飞机引擎的问题，你可以通过异常检测找到异常的飞机引擎，然后要求进一步细查这些引擎的质量。</p>
<h4 id="计算机监控">计算机监控</h4><p>第三个应用是数据中心的计算机监控。实际上我有些朋友正在从事这类工作。</p>
<p>如果你正在管理一个计算机集群或者一个数据中心，其中有许多计算机。那么我们可以为每台计算机计算特征变量，例如计算机的内存消耗、硬盘访问量、CPU负载或者一些更加复杂的特征（例如一台计算机的CPU负载与网络流量的比值）。</p>
<p>那么给定正常情况下数据中心中计算机的特征变量，你可以建立$p(x)$模型，通过它来找到运行不正常的计算机。</p>
<p>目前这种技术正在被各大数据中心使用，用来监测大量计算机可能发生的异常。</p>
<h2 id="高斯分布(正态分布)">高斯分布(正态分布)</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/ZYAyC/gaussian-distribution" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在本节我将介绍<strong>高斯分布</strong>（也称为<strong>正态分布</strong>）。</p>
<p>如果你已经对高斯分布非常熟悉了，那么也许你可以直接跳过这一节。但是如果你不确定或者你已经有段时间没有接触高斯分布了，那么请从头到尾看完这一节。在下一节中，我们将应用高斯分布来推导一套异常检测算法。</p>
</blockquote>
<h3 id="高斯分布的定义">高斯分布的定义</h3><p>假设$x$是一个实数随机变量（即：$x \in R$），如果x的概率分布服从高斯分布：其中均值为$μ$，方差为$σ^2$，那么将它记作：</p>
<p>$$<br>x \sim N(μ,σ^2)<br>$$</p>
<blockquote>
<p>这里的$\sim$符号读作：”服从…分布”。大写字母$N$表示Normal (正态)，有两个参数，其中$μ$表示均值，$σ^2$表示方差。</p>
</blockquote>
<p>如果我们将高斯分布的概率密度函数绘制出来，它看起来将是这样一个钟形的曲线：</p>
<p><img src="/img/17_05_23/008.png" alt=""></p>
<p>这个钟形曲线有两个参数，分别是$μ$和$σ$。其中$μ$控制这个钟形曲线的中心位置，$σ$控制这个钟形曲线的宽度。</p>
<p>从图中可以看出来，$x$取中心区域的值的概率相当大，因为高斯分布的概率密度在这里很大；而$x$取远处和更远处数值的概率将逐渐降低，直至消失。</p>
<p>高斯分布的数学公式如下：</p>
<p>$$<br>p(x;μ,σ^2)=<br>\frac{1}{\sqrt{2π}σ}exp(-\frac{(x-μ)^2}{2σ^2})<br>$$</p>
<p>其实我们并不需要记住这个公式，当我们真的需要用到它时，我们总可以查资料找到它。</p>
<h3 id="高斯分布中，$μ$和$σ$的关系">高斯分布中，$μ$和$σ$的关系</h3><p>我们举例来说明一下高斯分布中$μ$和$σ$这两个参数之间的关系：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$μ=0$，$σ=1$</th>
<th style="text-align:center">$μ=0$，$σ=0.5$</th>
<th style="text-align:center">$μ=0$，$σ=2$</th>
<th style="text-align:center">$μ=3$，$σ=0.5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_23/009.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/010.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/011.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/012.png" alt=""></td>
</tr>
</tbody>
</table>
<p>值得提醒的是，在高斯分布的图像中，不管曲线的形状如何，曲线围城的总面积都是1。所以如果$σ$很大，就意味着数据的离散化程度越大，中间区域就会变宽，但由于总概率为1，所以高度会降低；反之如果$σ$很小，就意味着数据的离散化程度越小，中间区域就会变窄，但由于总概率为1，所以高度会升高。</p>
<h3 id="参数估计问题">参数估计问题</h3><p>接下来让我们来看参数估计问题。</p>
<p>假设我们有以下数据集：</p>
<p>$$<br>｛x^{(1)},x^{(2)},…,x^{(m)}｝ \ \ \ \ \ \ \  x^{(i)} \in R<br>$$</p>
<p>其对应的数据在图像中如下：</p>
<p><img src="/img/17_05_23/013.png" alt=""></p>
<p>如果这些数据是服从正态分布的：</p>
<p>$$<br>x \sim N(μ,σ^2)<br>$$</p>
<p>但我们只有数据，并不知道参数$μ$和$σ$的具体值。那么参数估计问题，就是在寻找这些参数具体值的问题。</p>
<h4 id="高斯分布的参数估计公式">高斯分布的参数估计公式</h4><p>具体来说，高斯分布中的参数估计公式如下：</p>
<p>$$<br>μ=\frac{1}{m}\sum_{i=1}^mx^{(i)}<br>$$</p>
<p>$$<br>σ^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-μ)^2<br>$$</p>
<p>可以看出来，$μ$是在对所有m个样本求均值，$σ^2$实际上是对所有样本与均值做差再取平方后得到的平均大小。</p>
<blockquote>
<p>再提一下，如果你精通统计学，你可能听过<strong>极大似然估计</strong>，那么这里的估计实际就是对$μ$和$σ^2$的极大似然估计。如果你不知道，也无所谓。</p>
<p>还有一点，如果你在学习统计学时，可能会见到这个式子：$σ^2=\frac{1}{m-1}\sum_{i=1}^m(x^{(i)}-μ)^2$，但在机器学习领域，大家习惯使用$σ^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-μ)^2$，其实在实际情况中，具体使用$\frac{1}{m}$还是$\frac{1}{m-1}$其实区别很小，只要你有一个稍大的数据集。这两个版本的公式在理论特性和数学特性上稍有不同，但在实际应用中，他们的区别甚小，几乎可以忽略不计。</p>
</blockquote>
<h2 id="异常检测的具体算法">异常检测的具体算法</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/C8IJp/algorithm" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上节课中，我们谈到了高斯分布。在本节我将应用高斯分布开发异常检测算法。</p>
</blockquote>
<p>假如说我们有一个无标签的训练集，其中共有$m$个训练样本，并且这里的训练集里的每一个样本都是$n$维的特征，因此你的训练集应该是$m$个$n$维的特征构成的样本矩阵：</p>
<p>$$<br>｛x^{(1)},…,x^{(m)}｝   \\<br>x \in R^n<br>$$</p>
<p>对于我们的异常检测算法，我们要从数据中建立一个$p(x)$概率模型。由于$x$是一个向量，因此：</p>
<p>$$<br>p(x)=p(x_1)p(x_2)p(x_3)…p(x_n)<br>$$</p>
<p>我们假定特征$x_1$服从高斯正态分布:</p>
<p>$$<br>x_1 \sim N(μ_1,σ^2_1)<br>$$</p>
<p>根据上节学到的知识，你可以得出对应的$μ_1$和$σ_1$:</p>
<p>$$<br>μ_1=\frac{1}{m}\sum_{i=1}^mx^{(i)}_1<br>$$</p>
<p>$$<br>σ^2_1=\frac{1}{m}\sum_{i=1}^m(x^{(i)}_1-μ_1)^2<br>$$</p>
<p>这样$p(x_1)$就可以写成这样一个高斯分布:</p>
<p>$$<br>p(x_1)=p(x_1;μ_1,σ^2_1)<br>$$</p>
<p>同样地，我假设$x_2$也服从高斯分布，可以得出：</p>
<p>$$<br>p(x_2)=p(x_2;μ_2,σ^2_2)<br>$$</p>
<p>与此类似$x_3$服从另外一个高斯分布:</p>
<p>$$<br>p(x_3)=p(x_3;μ_3,σ^2_3)<br>$$</p>
<p>直到$x_n$:</p>
<p>$$<br>p(x_n)=p(x_n;μ_n,σ^2_n)<br>$$</p>
<p>因此可以得出:</p>
<p>$$<br>\begin{align*}<br>p(x)<br>&amp;= p(x_1;μ_1,σ^2_1)p(x_2;μ_2,σ^2_2)p(x_3;μ_3,σ^2_3)…p(x_n;μ_n,σ^2_n) \\<br>&amp;= Π_{j=1}^np(x_j;μ_j,σ^2_j)<br>\end{align*}<br>$$</p>
<p>其中$Π$（读作pai，是$π$的大写形式）类似$∑$符号，只不过这里将连加换成了连乘。顺便要说的是，估计$p(x)$的分布问题，通常被称为<strong>密度估计</strong>问题。</p>
<blockquote>
<p>注意：对于熟悉统计学的同学来说，上面的式子实际上就对应于一个从$x_1$到$x_n$的独立的假设。但实际应用中，无论这些特征是否相互独立，这些算法的效果都还不错。</p>
</blockquote>
<h3 id="异常检测算法步骤总结">异常检测算法步骤总结</h3><p>让我们来总结一下<strong>异常检测</strong>算法的具体步骤：</p>
<ul>
<li>1.从样本中选择一些能体现出异常行为的特征$x_i$。</li>
</ul>
<blockquote>
<p>我们可以尝试找出一些特征，比如在你的系统里，那些能看出用户异常行为或者欺诈行为的特征。</p>
</blockquote>
<ul>
<li>2.分别计算出每个特征的参数$μ_1,…,μ_n,σ^2_1,…,σ^2_n$。</li>
</ul>
<p>$$<br>μ=<br>\begin{equation}<br>\left[<br>\begin{matrix}<br>     μ_1 \\<br>     μ_2 \\<br>     ┋     \\<br>     μ_n<br>\end{matrix}<br>\right]<br>\end{equation}<br>=<br>\frac{1}{m}\sum_{i=1}^mx^{(i)}<br>$$</p>
<p>$$<br>σ^2=<br>\begin{equation}<br>\left[<br>\begin{matrix}<br>     σ^2_1 \\<br>     σ^2_2 \\<br>     ┋     \\<br>     σ^2_n<br>\end{matrix}<br>\right]<br>\end{equation}<br>=<br>\frac{1}{m}\sum_{i=1}^m(x^{(i)}-μ)^2<br>$$</p>
<p>其中：</p>
<p>$$<br>μ_j=\frac{1}{m}\sum_{i=1}^mx^{(i)}_j<br>$$</p>
<p>$$<br>σ^2_j=\frac{1}{m}\sum_{i=1}^m(x^{(i)}_j-μ_j)^2<br>$$</p>
<blockquote>
<p>对$m$个无标签数据分别计算出他们每个特征的期望$μ$和方差$σ^2$。<strong>注意，这里$μ$和$σ$都是m维度的向量</strong>，而$μ_j$和$σ_j$都是其中对应的第$j$个元素。</p>
</blockquote>
<ul>
<li>3.给定一个新的样本$x$，计算出它对应的$p(x)$:</li>
</ul>
<p>$$<br>p(x)=Π_{j=1}^np(x_j;μ_j,σ^2_j)=Π_{j=1}^n\frac{1}{\sqrt{2π}σ_j}exp(-\frac{(x_j-μ_j)^2}{2σ^2_j})<br>$$</p>
<p>通过判断$p(x)&lt;ε$，来判断是否有异常发生。</p>
<blockquote>
<p>给定一个用户行为的样本，如何知道用户行为是否异常呢？我们将用户行为数据带入到$p(x)$的计算中来，如果这个结果非常小，那么我们就将这个行为标注为异常行为。</p>
</blockquote>
<h3 id="异常分析例子">异常分析例子</h3><p>假如说我们有下面这样的数据集：</p>
<p><img src="/img/17_05_23/014.png" alt=""></p>
<p>从图中我们可以看出，数据集有两个特征$x_1$和$x_2$。</p>
<p>其中特征$x_1$对应的是水平方向的数据，它的均值是5，标准差是2；$x_2$对应的是竖直方向上的数据，它的均值是3，标准差是1：</p>
<p>$$<br>μ_1=5,σ_1=2<br>$$</p>
<p>$$<br>μ_2=3,σ_2=1<br>$$</p>
<p>这两个特征对应的分布如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$p(x_1;μ_1,σ^2_1)$</th>
<th style="text-align:center">$p(x_2;μ_2,σ^2_2)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_23/015.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/016.png" alt=""></td>
</tr>
</tbody>
</table>
<p>如果绘制出$p(x)$的图像，那么这个图像如下：</p>
<p><img src="/img/17_05_23/017.png" alt=""></p>
<p>通过图像，我们可以得出具体的某一点对应的高度值。</p>
<p>假如$x_1=2$，$x_2=2$那么就是这个点:</p>
<p><img src="/img/17_05_23/018.png" alt=""></p>
<p>在3-D表面图上的高度就代表$p(x)$的值。而这个$p(x)$完整的写出来就是下面的形式：</p>
<p>$$<br>p(x)=p(x_1;μ_1,σ^2_1)p(x_2;μ_2,σ^2_2)<br>$$</p>
<p>那么有了这个表达式，我们如何鉴定新的样本是否异常呢？</p>
<p>要回答这个问题，我们可以先给计算机设某个无穷小的数值$ε$，假如我设置$ε=0.02$(我会在后面讲到如何选取$ε$的值)。</p>
<p>现在我们有两个样本，分别为$x_{test}^{(1)}$和$x_{test}^{(2)}$：</p>
<p><img src="/img/17_05_23/019.png" alt=""></p>
<p>我们用上面的式子来计算出$p(x_{test}^{(1)})$，可以发现这是一个比较大的数，具体大小是大于等于$ε$的，所以对于$x_{test}^{(1)}$的检测结果是不属于异常。同样对于$p(x_{test}^{(2)})$，我们发现这是一个很小的数，具体值是小于$ε$的，所以我们说$x_{test}^{(2)}$属于异常数据。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/05/12/斯坦福机器学习课程 第八周 (2)降维/" itemprop="url">
                斯坦福机器学习课程 第八周 (2)降维
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-05-12T07:51:58+08:00" content="2017-05-12">
            2017-05-12
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/05/12/斯坦福机器学习课程 第八周 (2)降维/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/05/12/斯坦福机器学习课程 第八周 (2)降维/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><blockquote>
<p>在这个模块中，我们将介绍<strong>主成分分析（PCA）</strong>，并显示它可以用于数据压缩，加快学习算法，以及可视化的复杂数据集。</p>
</blockquote>
<h1 id="动机">动机</h1><h2 id="动机I：数据压缩">动机I：数据压缩</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/0EJ6A/motivation-i-data-compression" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>本节我将开始介绍第二种无监督学习问题，它叫<strong>降维(dimensionality reduction)</strong>。</p>
<p>我们希望使用降维的一个主要原因是数据压缩。我们会在后几节中看到，数据压缩不仅通过压缩数据使得数据占用更少的计算机内存和硬盘空间，它还能给算法提速。</p>
</blockquote>
<p>首先我们来介绍什么是降维。</p>
<h3 id="例子">例子</h3><p>举一个例子，假如我们有一个有很多很多很多特征变量的数据集：</p>
<p><img src="/img/17_05_12/001.png" alt=""></p>
<p>这里为了方便展示，只画了其中两个。</p>
<p>假设我们不知道这两个特征量。其中$x_1$是某个物体的长度，以厘米为单位；另一个$x_2$是它以英寸为单位的长度。所以这是一个非常冗余的数据，与其用两个特征变量$x_1$和$x_2$，它们都是测量到的长度，或许我们应该把这个数据降到一维，只用一个长度的数据。</p>
<p>这个例子可能看起来好像是我生造的，但这个厘米英寸的例子其实还真不是那么无聊。我在工业界看到的情况也是大同小异。</p>
<blockquote>
<p>如果你有上百或者上千的特征变量，很容易就会忘记你到底有什么特征变量，而且有时候可能有几个不同的工程师团队。一队工程师可能给你200个特征变量，第二队工程师可能再给你300个特征变量，然后第三队工程师给你500个特征变量。所以你一共有1000个特征变量，这样就很难搞清哪个队给了你什么特征变量。实际上得到这样冗余的特征变量并不难。</p>
</blockquote>
<p>所以如果以厘米计的长度被取整到最近的厘米整数，以英寸计的长度被取整到最近的英寸整数。这就是为什么这些样本没有完美地在一条直线上。就是因为取整所造成的误差。</p>
<p><img src="/img/17_05_12/002.png" alt=""></p>
<p>这种情况下，如果我们可以把数据降到一维而不是二维，就可以减少冗余。</p>
<h3 id="降维含义">降维含义</h3><p>让我们再详细讲讲从二维降到一维到底意味着什么。</p>
<h4 id="二维降到一维">二维降到一维</h4><p>让我给这些样本涂上不同的颜色涂上不同的颜色：</p>
<p><img src="/img/17_05_12/003.png" alt=""></p>
<p>在这个例子中降低维度的意思是：我希望找到一条线，基本所有数据映射到这条线上。这样做之后，我就可以直接测量这条线上每个样本的位置。我想把这个新特征叫做$z_1$。</p>
<p><img src="/img/17_05_12/004.png" alt=""></p>
<p>要确定这条线上的位置，我只需要一个数字。这就是说新特征变量$z_1$能够表示这条绿线上每一个点的位置。</p>
<p>在之前如果想要表示一个样本点，我需要一个二维向量$(x_1,x_2)$，但是现在我可以用一个一维向量$z_1$来表示这个样本点：</p>
<p><img src="/img/17_05_12/005.png" alt=""></p>
<p>总结一下，在把所有训练样本映射到一条线上之后，我就能做到只用一个数字来表示每个训练样本的位置。这是一个对原始训练样本的近似。相对于之前需要用两个数字来表示一个样本而言，现在我只需要一个数字就可以表示了。这样就减少了一半的内存需求或者硬盘需求。</p>
<p>更重要的是，数据压缩还会让我们的学习算法运行地更快。</p>
<h4 id="三维降到二维">三维降到二维</h4><p>现在，我展示一个把三维数据降到二维的例子。</p>
<p><img src="/img/17_05_12/006.png" alt=""></p>
<blockquote>
<p>顺便说一下，在更典型的降维例子中，我们可能有1000维的数据，我们可能想降低到100维，但是因为我在这里能可视化的展示数据的维度是有限制的，所以我要用的例子是三维到二维的。</p>
</blockquote>
<p>我们有一个图上这样的数据集，我有一个样本$x^{(i)}$的集合，$x^{(i)}$是一个三维实数的点，所以我的样本是三维的：</p>
<p>$$<br>x^{(i)} \in R^3<br>$$</p>
<p>实际上，这些样本点，差不多都处于同一平面上。降维在这里的作用，就是把所有的数据，都投影到一个二维的平面内。所以，我们要对所有的数据进行投影，使得它们落在这个平面上：</p>
<p><img src="/img/17_05_12/007.png" alt=""></p>
<p>最后为了表示一个点在平面上的位置，我们需要两个数来表示平面上一个点的位置。这两个数可能叫做$z_1$和$z_2$：</p>
<p><img src="/img/17_05_12/008.png" alt=""></p>
<p>这也意味着我们现在可以用一个二维向量$z$来表示每一个训练样本了：</p>
<p><img src="/img/17_05_12/009.png" alt=""></p>
<p>$$<br>z^{(i)} \in R^2<br>$$</p>
<hr>
<p>为了更好的理解降维的过程，现在让我们用3D绘图来重现上面的整个过程：</p>
<p><img src="/img/17_05_12/010.png" alt=""></p>
<p>我们走的过程是这样的：左边是原始数据集，中间是投影到2D的数据集，右边是以$z_1$和$z_2$为坐标轴的2D数据集。</p>
<p>我们来更详细地看一下：</p>
<p>原始数据集是这样的：</p>
<p><img src="/img/17_05_12/011.gif" alt=""></p>
<p>可以看出来，大部分数据差不多可能都落在某个2D平面上，或者说距离某个2D平面不远。</p>
<p>所以我们可以把它们投影到2D平面上。下面是投影后的效果：</p>
<p><img src="/img/17_05_12/012.gif" alt=""></p>
<p>你可以看到所有的数据落在一个平面上，因为我们把所有的东西都投影到一个平面上了。所以我们现在只需要两个数:$z_1$和$z_2$来表示点在平面上的位置即可：</p>
<p><img src="/img/17_05_12/013.png" alt=""></p>
<p>这就是把数据从三维降到二维的过程。</p>
<p>这就是降维以及如何使用它来压缩数据的过程。</p>
<h2 id="动机II：可视化数据">动机II：可视化数据</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/t6pYD/motivation-ii-visualization" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上节中，我们讲到一种通过数据降维来进行数据压缩的方法。在本节我将会讲到第二种数据降维的应用，那就是<strong>可视化数据</strong>。</p>
<p>对于大多数的机器学习应用，它真的可以帮助我们来开发高效的学习算法，但前提是我们能更好地理解数据。降维就是数据可视化的一种方法。</p>
</blockquote>
<p>假如我们已经收集了大量的有关全世界不同国家的统计数据集：</p>
<p><img src="/img/17_05_12/014.png" alt=""></p>
<p>第一个特征$x_1$是国家的国内生产总值；第二个特征$x_2$是一个百分比，表示人均占有的GDP；第三个特征$x_3$是人类发展指数；第四个特征$x_4$是预期寿命；…直到$x_{50}$</p>
<p>在这里我们有大量的国家的数据，对于每个国家有50个特征。我们有这样的众多国家的数据集，为了使得我们能更好地来理解数据，我们需要对数据进行可视化展示。这里我们有50个特征，但绘制一幅50维度的图是异常困难的，因此我们需要对数据进行降维，然后再可视化。</p>
<p>具体做法如下：</p>
<p>我们使用特征向量$x^{(i)}$来表示每个国家。$x^{(i)}$有着50个维度。我们需要对这50个特征降维之后，我们可以用另一种方式来代表$x^{(i)}$：使用一个二维的向量$z$来代替之前50维的$x$。</p>
<p><img src="/img/17_05_12/015.png" alt=""></p>
<p>$$<br>z^{(i)} \in R^2<br>$$</p>
<p>我们用$z_1$和$z_2$这两个数来总结50个维度的数据，我们可以使用这两个数来绘制出这些国家的二维图，使用这样的方法尝试去理解二维空间下不同国家在不同特征的差异会变得更容易。</p>
<p>在降维处理时，我们用$z_1$来表示那些象征着国家整体情况的数据，例如”国家总面积”、”国家总体经济水平”等；用$z_2$来表示象征着人均情况的数据，例如”人均GDP”，”人均幸福感”等。</p>
<p>降维处理之后，将数据按照这两个维度展示如下：</p>
<p><img src="/img/17_05_12/016.png" alt=""></p>
<p>在图中，右侧的点，象征着国家整体经济比较好的国家；上方的点，象征着人均经济比较好、人均幸福感较高、人均寿命较长…的国家。</p>
<hr>
<p>那么具体我们要如何去压缩数据达到降维的效果呢？在下一节视频中我们将会开始开发一种特别的算法。简称<strong>PCA</strong>或者<strong>主成分分析 </strong>。这个算法允许我们进行数据可视化，同时可以进行早先我们提到的一些有关数据压缩方面的应用。</p>
<h1 id="PCA">PCA</h1><h2 id="主成分分析（PCA）相关概念">主成分分析（PCA）相关概念</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/GBFTt/principal-component-analysis-problem-formulation" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>对于降维问题来说，目前最流行最常用的算法是<strong>主成分分析法(Principal Componet Analysis, PCA）</strong>。</p>
<p>在本节中，我想首先开始讨论PCA问题的公式描述，也就是说，我们用公式准确地精确地描述：我们想让PCA来做什么。</p>
</blockquote>
<h3 id="PCA的执行过程2D_-&gt;_1D">PCA的执行过程2D -&gt; 1D</h3><p>假设我们有这样的一个数据集:</p>
<p><img src="/img/17_05_12/017.png" alt=""></p>
<p>这个数据集含有二维实数空间内的样本X。</p>
<p>假设我想对数据进行降维，从二维降到一维。也就是说我想找到一条直线将数据投影到这条直线上，那怎么找到一条好的直线来投影这些数据呢？ </p>
<p>这样的一条直线也许是个不错的选择：</p>
<p><img src="/img/17_05_12/018.png" alt=""></p>
<p>你认为这是一个不错的选择的原因是：如果你观察投影到直线上的点的位置，我们发现每个点到它们对应的投影到直线上的点之间的距离非常小。（也就是说这些蓝色的线段非常的短）：</p>
<p><img src="/img/17_05_12/019.png" alt=""></p>
<p>所以，正式的说<strong>PCA</strong>所做的就是<strong>寻找一个低维的面(在这个例子中，其实是一条直线）数据投射在上面，使得这些蓝色小线段的平方和达到最小值</strong>。这些蓝色线段的长度被叫做<strong>投影误差</strong>。</p>
<p>所以<strong>PCA</strong>所做的就是寻找一个投影平面，对数据进行投影，使得这个能够最小化。</p>
<p>另外在应用<strong>PCA</strong>之前，通常的做法是先进行<strong>均值归一化</strong>和<strong>特征规范化</strong>，使得特征$x_1$和$x_2$均值为0，数值在可比较的范围之内。</p>
<blockquote>
<p>在这个例子里，我已经这么做了。但是在后面我还将回过来讨论更多有关PCA背景下的特征规范化和均值归一化问题。</p>
</blockquote>
<hr>
<p>我们正式一点地写出<strong>PCA</strong>的目标是这样的：</p>
<p>如果我们将数据从二维降到一维的话，我们需要试着寻找一个向量$u^{(i)}$，该向量属于$n$维空间中的向量（在这个例子中是二维的），我们将寻找一个对数据进行投影的方向，使得<strong>投影误差能够最小</strong>（在这个例子里，我们把PCA寻找到这个向量记做$u^{(1)}$）：</p>
<p><img src="/img/17_05_12/020.png" alt=""></p>
<p>所以当我把数据投影到这条向量所在的直线上时，最后我将得到非常小的重建误差。</p>
<blockquote>
<p>另外需要说明的时无论PCA给出的是这个$u^{(1)}$是正还是负都没关系。因为无论给的是正的还是负的$u^{(1)}$它对应的直线都是同一条，也就是我将投影的方向。</p>
</blockquote>
<p>这就是将二维数据降到一维的例子。</p>
<p>更一般的情况是我们有$n$维的数据想降到$k$维。在这种情况下我们不仅仅只寻找单个的向量（$u^{(1)}$）来对数据进行投影，我们要找到$k$个方向($u^{(k)}$)来对数据进行投影，从而最小化投影误差。</p>
<h3 id="PCA的执行过程3D_-&gt;_2D">PCA的执行过程3D -&gt; 2D</h3><p>下面的例子中，假设我有一些三维数据点：</p>
<p><img src="/img/17_05_12/021.png" alt=""></p>
<p>我想要做的是是寻找两个向量$u^{(1)}$和$u^{(2)}$：</p>
<p><img src="/img/17_05_12/022.png" alt=""></p>
<p>这两个向量一起定义了一个二维平面，我将把数据投影到这个二维平面上。</p>
<blockquote>
<p>如果你精通线性代数，那么这里更正式的定义是：我们将寻找一组向量$u^{(1)}$，$u^{(2)}$，…，$u^{(k)}$，我们将要做的是将数据投影到这$k$个向量展开的线性子空间上。</p>
<p>但是如果你不熟悉线性代数，那就想成是寻找$k$个方向（而不是之寻找一个方向）对数据进行投影。</p>
</blockquote>
<p>所以对于3D降维到2D的这个例子来说，寻找一个$k$维的平面，就是在寻找二维的平面。</p>
<hr>
<p>因此<strong>PCA</strong>做的就是：<strong>寻找一组$k$维向量(一条直线、或者平面、或者诸如此类等等)对数据进行投影，来最小化正交投影误差。</strong></p>
<h3 id="PCA和线性回归的关系">PCA和线性回归的关系</h3><p>最后一个我有时会被问到的问题是：<strong>PCA和线性回归有怎么样的关系？</strong></p>
<p>因为当我解释<strong>PCA</strong>的时候，我有时候会画出这样看上去有点像线性回归的图：</p>
<p><img src="/img/17_05_12/023.png" alt=""></p>
<p>但是，事实上<strong>PCA不是线性回归</strong>。尽管看上去有一些相似，但是它们确实是两种不同的算法。</p>
<h4 id="不同点_之一">不同点 之一</h4><p>如果我们做线性回归，我们做的是在给定某个输入特征$x$的情况下预测某个变量$y$的数值。因此对于线性回归，我们想做的是拟合一条直线，来最小化点和直线之间的平方误差：</p>
<p><img src="/img/17_05_12/024.png" alt=""></p>
<p>所以我们要最小化的是，上图中蓝线幅值的平方。注意我画的这些蓝色的垂直线，这是垂直距离。它是某个点与通过假设的得到的其预测值之间的距离。</p>
<p>与此想反，PCA要做的是最小化这些样本点与直线的最短距离(直角距离)：</p>
<p><img src="/img/17_05_12/025.png" alt=""></p>
<p>这是一种非常不同的效果。</p>
<h4 id="不同点_之二">不同点 之二</h4><p>更更更一般的是，当你做线性回归的时候，有一个特别的变量$y$作为我们即将预测的值，线性回归所要做的就是用$x$的所有的值来预测$y$。然而在PCA中，没有这么一个特殊的变量$y$是我们要预测的。我们所拥有的是特征$x_1$,$x_2$,…,$x_n$，所有的这些特征都是被同样地对待。</p>
<p>在上面那个从3维降到2维的例子中，原先的3个特征$x_1$,$x_2$,$x_3$都是被同样地对待的，没有特殊的变量$y$需要被预测。</p>
<hr>
<p>因此，PCA不是线性回归。尽管有一定程度的相似性，使得它们看上去是有关联的，但它们实际上是非常不同的算法。</p>
<p>因此，希望你们能理解PCA是做什么的：它是寻找到一个低维的平面，对数据进行投影，以便最小化投影误差平方的（最小化每个点与投影后的对应点之间的距离的平方值）。</p>
<h2 id="PCA算法_实现过程">PCA算法 实现过程</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/GBFTt/principal-component-analysis-problem-formulation" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>本节，将介绍<strong>PCA</strong>算法的具体细节，学完本节后，你就应该知道<strong>PCA</strong>的实现过程，并且应用PCA来给你的数据降维了。</p>
</blockquote>
<h3 id="数据预处理">数据预处理</h3><p>在使用PCA之前，我们通常会有一个数据预处理的过程。</p>
<p>拿到某组有m个无标签样本的训练集，一般先进行<strong>均值归一化(mean normalization)</strong>。这一步很重要。然后还可以进行<strong>特征缩放(feature scaling)</strong>，这根据你的数据而定。</p>
<blockquote>
<p>这跟我们之前在<strong>监督学习</strong>中提到的<strong>均值归一</strong>和<strong>特征缩放</strong>是一样的。</p>
</blockquote>
<h4 id="数据预处理第一步：均值归一化(mean_normalization)">数据预处理第一步：均值归一化(mean normalization)</h4><p>对于<strong>均值归一</strong>，我们首先应该计算出每个特征的均值$μ$，然后我们用$x-μ$来替换掉$x$。这样就使得所有特征的均值为0。</p>
<p><strong>举例说明：</strong></p>
<p>比如说，如果$x_1$表示房子的面积，$x_2$表示房屋的卧室数量，然后我们可以把每个特征进行缩放，使其处于同一可比的范围内。</p>
<p>同样地，跟之前的监督学习类似，我们可以首先计算出每个特征的均值：</p>
<p>$$<br>μ_j=\frac{1}{m}\sum_{i=1}^{m}x_j^{(i)}<br>$$</p>
<p>然后每个样本值对应的特征减去其对应的均值：</p>
<p>$$<br>x_j^{(i)} ← x_j^{(i)}-μ_j<br>$$</p>
<p>将所有的特征替换为这种形式的结果。这样就保证了所有特征的均值为0。</p>
<h4 id="数据预处理第二步：特征缩放(feature_scaling)">数据预处理第二步：特征缩放(feature scaling)</h4><p>然后，由于不同特征的取值范围都很不一样，我们还需要进行<strong>特征缩放</strong>。</p>
<p>我们需要将每个特征的取值范围都划定在同一范围内，因此对于均值化处理之后的特征值$x_j^{(i)}-μ_j$，我们还需要做进一步处理：</p>
<p>$$<br>x_j^{(i)} ← \frac{x_j^{(i)}-μ_j}{s_j}<br>$$</p>
<p>这里$s_j$表示特征$j$度量范围，即该特征的最大值减去最小值。</p>
<h3 id="PCA算法">PCA算法</h3><p>接下来就正式进入PCA的算法部分。</p>
<p>在之前的视频中，我们已经知道了PCA的原理。PCA是在试图找到一个低维的子空间，然后把原数据投影到子空间上，并且最小化平方投影误差的值（投影误差的平方和，即下图中蓝色线段长度的平方和）：</p>
<p><img src="/img/17_05_12/026.png" alt=""></p>
<p>那么应该怎样来计算这个子空间呢? 实际上这个问题有完整的数学证明来解释如何找到这样的子空间，不过这个数学证明过程是非常复杂的，同时也超出了本课程的范围。但如果你推导一遍这个数学证明过程，你就会发现要找到$u^{(1)}$的值，也不是一件很难的事。但在这里，我不会给出证明，我只是简单描述一下实现PCA所需要进行的步骤。</p>
<p>假如说我们想要把数据从$n$维降低到$k$维，我们首先要做的是计算出下面这个协方差矩阵(通常用$∑$来表示)：</p>
<p>$$<br>∑=\frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T<br>$$</p>
<blockquote>
<p>很不幸的是，这个希腊符号$∑$和求和符号重复了。希望你对这里不要产生混淆。</p>
</blockquote>
<p>计算出这个协方差矩阵后，假如我们把它存为Octave中的一个名为<code>Sigma</code>的变量，我们需要做的是计算出<code>Sigma</code>矩阵的<strong>特征向量(eigenvectors)</strong>。</p>
<p>在Octave中，你可以使用如下命令来实现这一功能：</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="name">U</span>,S,V] = svd(<span class="name">Sigma</span>)<span class="comment">;</span></div></pre></td></tr></table></figure>
<blockquote>
<p>顺便说一下，<code>svd</code>表示<strong>奇异值分解(singular value decomposition)</strong>，这是某种更高级的奇异值分解，这是比较高级的线性代数的内容。你不必掌握这些，但实际上<code>Sigma</code>是一个协方差矩阵。有很多种方法来计算它的特征向量。</p>
<p>如果你线性代数学得很好，或者你之前听说过特征向量的话，那也许知道在Octave中还有另一个<code>eig</code>命令，可以用来计算特征向量。实际上<code>svd</code>命令和<code>eig</code>命令将得到相同的结果。但<code>svd</code>其实要更稳定一些，所以我一般选择用<code>svd</code>，不过我也有一些朋友喜欢用<code>eig</code>函数。</p>
<p>在这里对协方差矩阵<code>Sigma</code>使用<code>eig</code>和<code>svd</code>时，你会得到同样的答案。这是因为协方差均值总满足一个数学性质，称为<strong>对称正定(symmetric positive definite)</strong>，其实你不必细究这个具体是什么意思，只要知道这种情况下，使用<code>eig</code>和<code>svd</code>结果是一样的就可以了。</p>
</blockquote>
<p>好了，这就是你需要了解的一点线性代数知识，如果有任何地方不清楚的话不必在意。你只需要知道上面这行Octave代码就行了。</p>
<p>如果你用除了Octave或者MATLAB之外的其他编程环境，你要做的是找到某个可以计算svd，即奇异值分解的函数库文件。在主流的编程语言中，应该有不少这样的库文件。我们可以用它们来计算出协方差矩阵的$U$ $S$ $V$矩阵。</p>
<hr>
<p>我再提几个细节问题。</p>
<p>这个协方差矩阵<code>Sigma</code>应该是一个$n×n$的矩阵，通过定义可以发现这是一个$n×1$的向量，和它自身的转置（一个$1×n$的向量）相乘得到的结果，这个结果自然是一个$n×n$的矩阵。</p>
<p>然后把这n个$n×n$的矩阵加起来，当然还是$n×n$矩阵。</p>
<p>然后svd将输出三个矩阵，分别是$U$ $S$ $V$。你真正需要的是$U$矩阵。</p>
<p>$U$矩阵也是一个$n×n$矩阵：</p>
<p><img src="/img/17_05_12/027.png" alt=""></p>
<p>实际上$U$矩阵的列元素就是我们需要的$u^{(1)}$,$u^{(1)}$等等。</p>
<p>如果我们想将数据的维度从$n$降低到$k$的话，我们只需要提取前$k$列向量。这样我们就得到了$u^{(1)}$到$u^{(k)}$，也就是我们用来投影数据的$k$个方向。</p>
<p>我们取出$U$矩阵的前$k$列得到一个新的，由$u^{(1)}$到$u^{(k)}$组成的矩阵$U_{reduce}$：</p>
<p>$$<br>\begin{equation}<br>U_{reduce}=\left[<br>\begin{matrix}<br>|&amp;|&amp;|&amp;…&amp;|\\<br>|&amp;|&amp;|&amp;…&amp;|\\<br>u^{(1)}&amp;u^{(2)}&amp;u^{(3)}&amp;…&amp;u^{(k)}\\<br>|&amp;|&amp;|&amp;…&amp;|\\<br>|&amp;|&amp;|&amp;…&amp;|\\<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>这是一个$n × k$维的矩阵。</p>
<p>然后我们用这个$U_{reduce}$来对我的数据进行<strong>降维</strong>。我们定义：</p>
<p>$$<br>\begin{equation}<br>z=\left[<br>\begin{matrix}<br>|&amp;|&amp;|&amp;…&amp;|\\<br>|&amp;|&amp;|&amp;…&amp;|\\<br>u^{(1)}&amp;u^{(2)}&amp;u^{(3)}&amp;…&amp;u^{(k)}\\<br>|&amp;|&amp;|&amp;…&amp;|\\<br>|&amp;|&amp;|&amp;…&amp;|\\<br>\end{matrix}<br>\right]<br>^{T}x<br>\\<br>=<br>\left[<br>\begin{matrix}<br>-&amp;-&amp;u^{(1)}&amp;…&amp;-\\<br>-&amp;-&amp;u^{(2)}&amp;…&amp;-\\<br>-&amp;-&amp;u^{(3)}&amp;…&amp;-\\<br>.&amp;.&amp;.&amp;…&amp;.\\<br>-&amp;-&amp;u^{(k)}&amp;…&amp;-\\<br>\end{matrix}<br>\right]<br>x<br>\end{equation}<br>$$</p>
<p>$$<br>z \in R^k<br>$$</p>
<p>其中$\left[<br>\begin{matrix}<br>-&amp;-&amp;u^{(1)}&amp;…&amp;-\\<br>-&amp;-&amp;u^{(2)}&amp;…&amp;-\\<br>-&amp;-&amp;u^{(3)}&amp;…&amp;-\\<br>.&amp;.&amp;.&amp;…&amp;.\\<br>-&amp;-&amp;u^{(k)}&amp;…&amp;-\\<br>\end{matrix}<br>\right]$是$k×n$的矩阵，$x$是$n×1$的矩阵，因此$z$是$k×1$的矩阵。</p>
<p>这里的$x$可以是训练集中的样本，也可以是交叉验证集中的样本，也可以是测试集样本。</p>
<h3 id="总结">总结</h3><p>总结一下，这就是PCA的全过程：</p>
<ul>
<li><p>首先进行均值归一化</p>
<ul>
<li>保证所有的特征量都是均值为0的。</li>
</ul>
</li>
<li><p>然后可以选择进行特征缩放</p>
<ul>
<li>如果不同特征量的范围跨度很大的话，你确实需要进行特征缩放这一步。</li>
</ul>
</li>
<li><p>在以上的预处理之后，我们计算出这个协方差<code>Sigma</code>矩阵：</p>
</li>
</ul>
<p>$$<br>Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)})(x^{(i)})^T<br>$$</p>
<ul>
<li>然后我们可以应用<code>svd</code>函数来计算出<code>U S V</code>矩阵:</li>
</ul>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="name">U</span>,S,V] = svd(<span class="name">Sigma</span>)<span class="comment">;</span></div></pre></td></tr></table></figure>
<ul>
<li>然后，我们取出$U$矩阵的前$k$列元素组成新的$U_{reduce}$矩阵：</li>
</ul>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">Ureduce</span> = U(:,<span class="number">1</span>:k);</div></pre></td></tr></table></figure>
<ul>
<li>最后这个式子给出了我们从原来的特征$x$变成降维后的$z$的过程:</li>
</ul>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">z</span> = Ureduce`*x;</div></pre></td></tr></table></figure>
<blockquote>
<p>另外，跟<strong>k均值算法</strong>类似，如果你使用<strong>PCA</strong>的话，你的$x$应该是$n$维实数。所以没有$x_0 = 1$这一项。</p>
<p>有一件事儿我没做：$u^{(1)},u^{(2)}…u^{(k)}$通过将数据投影到$k$维的子平面上确实使得投影误差的平方和为最小值，但是我并没有证明这一点，因为这已经超出了这门课的范围。</p>
<p>幸运的是PCA算法能够用不多的几行代码就能实现它。</p>
</blockquote>
<h1 id="应用PCA">应用PCA</h1><h2 id="对压缩数据的还原">对压缩数据的还原</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/X8JoQ/reconstruction-from-compressed-representation" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在前面的视频中我们介绍了<strong>PCA (主成分分析)</strong>作为压缩数据的算法，你会发现它能将高达一千维度的数据压缩到只有一百个维度；或者将三维数据压缩到两个维度的情况。</p>
<p>如果有一个这样的压缩算法，那么也应该有一种方法可以从压缩过的数据近似地回到原始高维度的数据。</p>
<p>假设有一个已经被压缩过的$z^{(i)}$它有100个维度，怎样使它回到其最初的表示$x^{(i)}$也就是压缩前的1000维的数据呢？ </p>
<p>在本节，我将会告诉你如何做到。</p>
</blockquote>
<p>在PCA算法中，我们有下面这些样本：</p>
<p><img src="/img/17_05_12/028.png" alt=""></p>
<p>我们让这些样本投影在一维平面$z_1$上，并且明确地指定其位置：</p>
<p><img src="/img/17_05_12/029.png" alt=""></p>
<p>那么给出一个一维实数点$z$我们能否，让$z$重新变成原来的二维实数点$x$呢？</p>
<p>即做到：</p>
<p>$$<br>z \in R → x \in R^2<br>$$</p>
<hr>
<p>我们知道:</p>
<p>$$<br>z = U^T_{reduce}x<br>$$</p>
<p>如果想得到相反的情形，方程应这样变化:</p>
<p>$$<br>x_{approx} = U_{reduce}z<br>$$</p>
<p>为了检查维度，在这里$U_{reduce}$是一个$n×k$矩阵，$z$就是一个$k×1$维向量。将它们相乘得到的就是$n×1$维。</p>
<p>所以$x_{approx}$是一个$n$维向量。</p>
<p>同时根据PCA的意图，投影的平方误差不能很大。也就是说$x_{approx}$将会与最开始用来导出$z$的原始$x$很接近。用图表示出来就是这样：</p>
<p><img src="/img/17_05_12/030.png" alt=""></p>
<p>这已经与原始数据非常近似了。</p>
<p>这就是用低维度的特征数据$z$还原到未被压缩的特征数据的过程。我们找到一个与原始数据$x$近似的$x_{approx}$。我们也称这一过程为<strong>原始数据的重构(reconstruction)</strong>。</p>
<h2 id="选择主成分的数量k">选择主成分的数量k</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/S1bq1/choosing-the-number-of-principal-components" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在PCA算法中，我们把n维特征变量降维到k维特征变量。这个数字k是PCA算法的一个参数。这个数字k也被称作<strong>主成分的数量</strong>。在本节中我会给你们一些参考，告诉你们人们是怎样思考如何选择PCA的参数k的。</p>
</blockquote>
<h3 id="算法原理：最小化平均平方映射误差">算法原理：最小化平均平方映射误差</h3><p>为了选择参数k（也就是要选择<strong>主成分的数量</strong>），这里有几个有用的概念：</p>
<p>PCA所做的是尽量最小化<strong>平均平方映射误差 (Average Squared Projection Error) </strong>。</p>
<p>因此PCA就是要将下面这个量最小化：</p>
<p>$$<br>\frac{1}{m}\sum_{i=1}^m||x^{i}-x_{approx}^{(i)}||^2<br>$$</p>
<p>即最小化$x$和其在低维表面上的映射点之间的距离的平方。这就是平均平方映射误差。</p>
<p>同时我们还要定义一下<strong>数据的总变差(Total Variation)</strong>：</p>
<p>$$<br>\frac{1}{m}\sum_{1=m}^m||x^{(i)}||^2<br>$$</p>
<p>数据的总变差 (Total Variation) 是这些样本的长度的平方的均值。它的意思是 “平均来看，我的训练样本距离零向量（原点）多远？”。</p>
<p>当我们去选择k值的时候，我们通过平均平方映射误差除以数据的总变差来表示数据的变化有多大。我们想要这个比值能够小于1%：</p>
<p>$$<br>\frac{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2<br>}{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2<br>}<br>\le0.01<br>$$</p>
<p>大部分人在考虑，选择k的方法时，不是直接选择k值，而是这里的数字应该设置为多少：</p>
<p><img src="/img/17_05_12/031.png" alt=""></p>
<p>它应该是0.01还是其它的数？如果选择了0.01，那么用PCA的语言说就是保留了99%的差异性。</p>
<p>数字0.01是人们经常用的一个值，另一个常用的值是0.05。如果选择了0.05，就意味着95%的差异性被保留了。从95到99是人们最为常用的取值范围。</p>
<p>你可能会惊讶的发现，对于许多数据集，即使保留了99%的差异性，可以大幅地降低数据的维度。因为大部分现实中的数据，许多特征变量都是高度相关的。所以实际上大量压缩数据是可能的，而且仍然会保留99%或95%的差异性。</p>
<h3 id="具体实现">具体实现</h3><p>那么你该如何实现它呢？</p>
<h4 id="原始的算法">原始的算法</h4><p>有一种方式是从1开始，依次递增k的值，尝试检查差异性是否达到预设值。</p>
<p>例如：</p>
<ul>
<li>尝试$k=1$时的PCA。</li>
<li>计算出$U_{reduce}，z^{(1)}，z^{(2)}，…，z^{(m)}，x^{(1)}_{approx}，…，x^{(m)}_{approx}$</li>
<li>检查是否满足：</li>
</ul>
<p>$$<br>\frac{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2<br>}{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2<br>}<br>\le0.01<br>$$</p>
<p>如果满足条件，我们就用$k=1$；但如果不满足，那么我们接下来尝试$k=2$，然后我们要重新走一遍这整个过程。</p>
<p>以此类推一直试到上面不等式成立为止。</p>
<h4 id="一种更快的算法">一种更快的算法</h4><p>可以想象，上面这种方式非常低效。每次尝试使用新的$k$值带入计算时，整个计算过程都需要重新执行一遍，还好我没有一种更快捷方便的计算方式。</p>
<p>当你调用<code>svd</code>来计算PCA时，你会得到三个矩阵<code>[U,S,V]</code>:</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="name">U</span>,S,V]=svd(<span class="name">Sigma</span>)</div></pre></td></tr></table></figure>
<p>除了之前提到的<code>U</code>矩阵之外，当你对协方差的矩阵<code>Sigma</code>调用<code>svd</code>时，我没还会得到中间的这个<code>S</code>矩阵。<code>S</code>矩阵是一个$n×n$的对角矩阵，它只有在对角线上的元素不为0，其余的元素都是0。并且显而易见，它是一个方阵：</p>
<p>$$<br>\begin{equation}<br>S=\left[<br>\begin{matrix}<br>s_{11}&amp;0&amp;0&amp;…&amp;0\\<br>0&amp;s_{22}&amp;0&amp;…&amp;0\\<br>0&amp;0&amp;s_{33}&amp;…&amp;0\\<br>┋&amp;┋&amp;┋&amp;…&amp;┋\\<br>0&amp;0&amp;0&amp;…&amp;s_{nn}\\<br>\end{matrix}<br>\right]<br>\end{equation}<br>$$</p>
<p>可以证明的是（我不会在此证明）实际上对于一个给定的k值，可以通过这个$S$矩阵方便的计算出差异性那一项的值：</p>
<p>$$<br>\frac{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2<br>}{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2<br>}<br>=<br>1-\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}<br>$$</p>
<hr>
<p>例如，假设差异性要满足小于$0.01$，那么可以得出：</p>
<p>$$<br>\frac{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2<br>}{<br>\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2<br>}<br>=<br>1-\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}<br>\le0.01<br>$$</p>
<p>即：</p>
<p>$$<br>\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}<br>\ge0.99<br>$$</p>
<p>那么你可以从1开始，慢慢增大$k$的值，来计算上面这个不等式，直到满足为止即可（得到满足上面不等式的最小$k$值）。</p>
<hr>
<p>通过这种方式，你只需要<strong>调用一次<code>svd</code>函数</strong>，通过<code>svd</code>给出的<code>S</code>矩阵你就可以通过依次增加$k$值的方式来求解了。这样以来就大幅的提升了计算效率。</p>
<h3 id="总结-1">总结</h3><p>总结一下，使用PCA算法时寻找合适$k$值的方法：</p>
<ul>
<li>首先对协方差矩阵<code>Sigma</code>调用一次<code>svd</code>：</li>
</ul>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="name">U</span>,S,V] = svd(<span class="name">Sigma</span>)</div></pre></td></tr></table></figure>
<ul>
<li>然后使用下面的不等式求得满足条件的最小$k$值：</li>
</ul>
<p>$$<br>\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}<br>\ge0.99<br>$$</p>
<hr>
<p>顺便说一下，即使你想要手动挑选$k$值，如果你想要向别人解释你实现的PCA的性能具体如何，那么一个好方法就是算出这个值：</p>
<p>$$<br>\frac{\sum_{i=1}^k S_{ii}}{\sum_{i=1}^n S_{ii}}<br>$$</p>
<p>它会告诉你百分之多少的差异性被保留了下来。</p>
<p>如果你把这个数值展现出来，那么熟悉PCA的人们就可以通过它来更好地理解你用来代表原始数据的压缩后的数据近似得有多好。因为有99%的差异性被保留了。</p>
<p>这就是一个<strong>平方投影误差的测量指标</strong>。它可以带给你对于数据压缩后是否与原始数据相似带来一种很好的直观感受。</p>
<h2 id="应用PCA的建议">应用PCA的建议</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/RBqQl/advice-for-applying-pca" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在之前的课程中，我已经提到过<strong>PCA</strong>有时可以用来提高机器学习算法的速度。在本节，我将讲解如何在实际操作中来实现。同时列举一些例子来说明PCA在具体应用过程中的使用建议。</p>
</blockquote>
<h3 id="PCA应用场景总结">PCA应用场景总结</h3><p>迄今为止我们讨论过的有关PCA的应用中有如下应用场景：</p>
<ul>
<li>数据压缩<ul>
<li>减少内存或者磁盘空间的使用</li>
<li>提升学习算法的效率（k值的选择是关键）</li>
</ul>
</li>
<li>数据可视化<ul>
<li>将数据降维到二/三维度进行可视化展示</li>
</ul>
</li>
</ul>
<h3 id="通过PCA来提高学习算法的速度">通过PCA来提高学习算法的速度</h3><p>举例说明，假如你正在用机器学习来处理图片数据。假设每张输入的图片尺寸是$100×100$的，那么对于每张图片来说，都有10000个像素点。假设样本$x^{(i)}$是包含了10000像素强度值的特征向量，即：</p>
<p>$$<br>x^{(i)}\in R<br>$$</p>
<p>那么对于我们的样本数据集来说：</p>
<p>$$<br>(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})<br>$$</p>
<p>每个样本中，对应的$x^{(i)}$都是10000维的特征向量。</p>
<p>可想而知，这么高维度的数据带入到逻辑回归、神经网络、支持向量机或者任何别的算法中，学习算法运行的都会很慢。</p>
<p>幸运的是，通过使用PCA，我们能够<strong>降低数据的维数，从而使得算法能够更加高效地运行</strong>。这就是PCA提高算法运算效率的原理。</p>
<h4 id="降维步骤">降维步骤</h4><p>首先我们需要检查带标签的训练数据集，并提取出输入数据。我们只需要提取出$x$并暂时把$y$放在一边。这一步我们会得到一组无标签的训练集：</p>
<p>$$<br>x^{(1)},x^{(2)},…,x^{(m)}\in R^{10000}<br>$$</p>
<p>从$x^{(1)}$到$x^{(m)}$，每个样本都是10000维的数据。然后我们应用PCA降维，我们会得到一个降维后的1000维的数据集：</p>
<p>$$<br>z^{(1)},z^{(2)},…,z^{(m)}\in R^{1000}<br>$$</p>
<p>这样我们就得到了一个新的训练集：</p>
<p>$$<br>(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),…,(z^{(m)},y^{(m)})<br>$$</p>
<p>现在，我可以将这个已经降维的数据集输入到学习算法中，来得出假设函数，并把降维后的数据作为输入带入，做出预测。</p>
<p>以<strong>逻辑回归</strong>为例：</p>
<p>逻辑回归中，我们得到的假设函数如下：</p>
<p>$$<br>h_{\theta}(z) = \frac{1}{1+e^{-\theta^{T}z}}<br>$$</p>
<p>我们将$z$向量作为输入带入，并得出一个预测值。</p>
<p>最后，如果你有一个新的样本$x$，那么你所要做的是将你的测试样本$x$通过同样的PCA降维之后，你会得到这个样本所对应的$z$。然后将这个$z$值带入到这个假设函数中进行预测。</p>
<blockquote>
<p><strong>注意（重要）：</strong></p>
<p>最后要注意一点，PCA定义了从$x$到$z$的对应关系，这种对应关系只可以通过在训练集上运行PCA定义出来。</p>
<p>具体来讲，这种PCA所学习出的对应关系，所做的就是计算出一系列的参数。这些参数这就是<strong>特征缩放</strong>和<strong>均值归一化</strong>以及降维矩阵$U_{reduce}$。但是对于降维矩阵$U_{reduce}$中的数据，我们需要使我们的参数唯一地适应<strong>训练集</strong>，而不是适应交叉验证或者测试集。因此我们通过在训练集中找到了降维矩阵$U_{reduce}$，我们就可以将同样的对应关系应用到其他样本中了，比如交叉验证数集样本，或者用在测试数据集中。</p>
<p>总结一下，当你在运行PCA的时候，只是在训练集那一部分来进行的，而不是在交叉验证的数据集或者测试集上运行。在训练集上运行PCA后，得到了从$x$到$z$的映射，然后你就可以将这个映射应用到交叉验证数据集，和测试数据集中。</p>
</blockquote>
<p>通过这个例子中的这种方式，我们讨论了将数据从上万维降到千维。在实际应用场景中，我们经常发现，将数据降维到原有维度的五分之一或者十分之一，就分类的精确度而言，降维后的数据对学习算法几乎没有什么影响。如果我们将降维用在低维数据上，我们的学习算法会运行得更快。</p>
<h3 id="PCA的错误使用">PCA的错误使用</h3><p>有一个值得提醒的频繁被误用的PCA应用场景，那就是使用它来避免过拟合。</p>
<p>具体原因是将高维度数据降维处理后，相较于原先的数据，会更不容易出现过拟合的现象。例如我们将10000维的数据降到了1000维，那么降维后的1000维数据相较于降维前的10000维数据更不容易产生过拟合。</p>
<p>因此有人认为PCA是一种避免过拟合的方法，但在这里，我需要强调一下，<strong>为了解决过拟合问题而使用PCA是不适合的！并且我不建议这么做。</strong></p>
<p>如果你比较担心过拟合问题，那么你应该使用正则化方法，而不是使用PCA来对数据进行降维。</p>
<blockquote>
<p><strong>PCA会丢失信息：</strong>如果你仔细想想PCA的工作原理，你会发现它并不需要使用数据的标签，你只需要设定好输入数据$x^{(i)}$，同时使用这个方法来寻找更低维度的数据近似，在这个过程中，PCA实际上已经把某些信息舍弃掉了。</p>
</blockquote>
<p>舍弃掉一些数据，并在你对数据标签$y$值毫不知情的情况下对数据进行降维，所以这或许是一个使用PCA方法的可行之路。如果保留99%的方差，即保留绝大部分的方差，那也是舍弃了某些有用的信息。事实证明，当你在保留99%或者95%或者其它百分比的方差时，结果表明只使用正则化对于避免过拟合，会带来比较好的效果。</p>
<p>同时对于过拟合问题，正则化效果也会比PCA更好，因为当你使用线性回归或者逻辑回归或其他的方法配合正则化时，这个最小化问题实际就变成了y值是什么，才不至于将有用的信息舍弃掉。然而PCA不需要使用到这些标签，它更容易将有价值信息舍弃。</p>
<p>总之，<strong>使用PCA的目的是加速学习算法，但不应该用它来避免过拟合</strong>。</p>
<h3 id="两个建议">两个建议</h3><h4 id="真的需要PCA吗？">真的需要PCA吗？</h4><p>有时候人们正在设计机器学习系统，或许会写下像这样的计划：</p>
<p><strong>设计一个机器学习系统：</strong></p>
<ul>
<li>收集训练数据集$｛(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})｝$</li>
<li>运行PCA将数据$x^{(i)}$降维到$z^{(i)}$</li>
<li>对降维后的数据训练逻辑回归算法$｛(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),…,(z^{(m)},y^{(m)})｝$</li>
<li>在测试集上测试结果：将$x_{test}^{(i)}$映射到$z_{test}^{(i)}$。对映射后的数据集$｛(z_{test}^{(1)},y_{test}^{(1)}),…,(z_{test}^{(m)},y_{test}^{(m)})｝$运行假设函数$h_{\theta}(z)$</li>
</ul>
<p>通常在一个项目的初期，有些人便直接写出这样的项目计划。</p>
<p>在写下这样一个使用PCA方法的项目计划前，一个非常好的问题是：<strong>如果我们在整个项目中不使用PCA效果会怎样？</strong> </p>
<p>通常人们不会去思考这个问题，尤其是当人们提出一个复杂的项目并且其中使用了PCA或其它方法时，我经常建议大家在使用PCA之前，首先要想清楚你自己做的是什么，以及你想要做什么。这也是你首先需要在原始数据$x^{(i)}$上考虑的问题。并且根据具体情况来分析是否适合使用PCA，还是直接将原始数据带入到学习算法中。</p>
<h4 id="不要一开始就带入PCA">不要一开始就带入PCA</h4><p>同时我也建议一开始不要将PCA方法就直接放到算法里，先使用原始数据$x^{(i)}$看看效果。只有一个原因让我们相信算法出现了问题，那就是你的学习算法收敛地非常缓慢，占用内存或者硬盘空间非常大，所以你想来压缩数据。只有当你的$x^{(i)}$效果不好的时候，那么就考虑用PCA来进行压缩数据。</p>
<p>因为我常常看到某些人在项目开始时便将PCA考虑进去，有时他们并没有仔细思考他们做了什么使得结果表现地好，更没有考虑在不用PCA下的情景会是什么样的效果。如果某个数据不使用PCA也可以工作的很好，但我们对于这些数据使用PCA耗费了大量时间，这是不值得的。</p>
<p>然而，尽管有这些需要注意的地方，PCA仍旧是一种不可思议的有用的算法。PCA的使用频率也很高，大部分时候我都用它来加快学习算法。但我认为PCA通常都是被用来压缩数据以减少内存使用或硬盘空间占用的、或者用来可视化数据的。</p>
<p>同时PCA也是一种强有力的无监督学习算法。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/04/26/【翻译】TextRank-对文本排序/" itemprop="url">
                【翻译】TextRank:对文本排序
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-04-26T22:12:58+08:00" content="2017-04-26">
            2017-04-26
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/04/26/【翻译】TextRank-对文本排序/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/04/26/【翻译】TextRank-对文本排序/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><blockquote>
<p>本文翻译自：<a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" target="_blank" rel="external">http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf</a></p>
</blockquote>
<h2 id="摘要">摘要</h2><p>本篇论文中，我们将介绍TextRank算法，这是一种针对于文本处理的基于图的排序模型算法，并且展示这个模型是如何能够被成功的应用在自然语言应用中。特别的，我们提出了两个创新的无监督方法用于关键词和句子提取，并且展示我们得到的测试结果与先前公布的基准结果的对比。</p>
<h2 id="1_介绍">1 介绍</h2><p>基于图的排序算法，像Kleinberg的HITS算法（Kleinberg，1999）或Google的PageRank算法（Brin和Page，1998）已成功应用于引文分析，社交网络和万维网链接结构分析。可以说，这些算法通过提供一种依靠Web架构师的集体知识而不是网页的单独内容分析的网页排名机制,可以作为网页搜索技术领域触发的范式转换的关键要素。简而言之，基于图的排序算法是通过考虑从整个图形递归计算的全局信息，而不是仅依赖于局部顶点特定信息来决定图中的顶点的重要性的一种方式。</p>
<p>对从自然语言文档中提取的词汇或语义图，使用类似的思路，可以得出一种基于图形的排名模型，这种模型可以应用于各种自然语言处理的应用程序中，其中从整个文本中获取的知识用于得出本地 排名/选择 的决策。 这种面向文本的排序方法，可以应用于关键短语的自动提取，摘要提取，以及词义消歧的任务（Mihalcea et等等，2004）。</p>
<p>在这篇论文中，我们会介绍基于从自然语言文本中提取的图形，来介绍TextRank图的排序模型。我们调查并评估了TextRank对于由无监督关键词和句子提取，这两种语言处理任务的应用，并展示使用TextRank获得的结果与在这些领域开发的最先进的系统进行比较。</p>
<h2 id="2_TextRank模型">2 TextRank模型</h2><p>基于图的排序算法本质上是基于从整个图形递归绘制的全局信息，来决定图中顶点的重要性的一种方式。基于图表的排名模式实现的基本思想是“投票”或“推荐”。当一个顶点链接到另一个顶点时，它主要是在为另一个顶点投票。顶点投射的投票数越多，顶点的重要性就越高。此外，顶点投票的重要性决定了投票本身的重要性，而这一信息也被排名模型考虑在内。因此，与顶点相关联的分数，取决于为其投的票，以及投射这些投票的顶点的分数来确定。</p>
<p>正式的，令集合$G=(V,E)$是具有顶点$V$和边$E$集合的有向图，其中$E$是$V×V$的子集。对于给定的顶点$V_i$，令$In(V_i)$是指向它的顶点集（前辈），并且令$Out(V_i)$是顶点$V_i$指向（后继）的顶点集合。顶点$V_i$的分数定义如下（Brin和Page，1998）：</p>
<p><img src="/img/17_04_26/001.png" alt=""></p>
<p>其中$d$是0和1之间的阻尼因子，它作用于将从给定顶点跳转到图中的另一个随机顶点的概率集成到模型中。在网页浏览的上下文中，这种基于图表的排名算法实现了“随机冲浪者模型”，其中用户以概率$d$随机点击链接，并以概率$1-d$跳转到一个全新的页面。因子$d$通常设置为0.85（Brin和Page，1998），这也是我们在具体实现中所使用的值。</p>
<p>从图中分配给每个节点的任意值开始，迭代计算直到达到低于给定的阈值。运行算法之后，得到一个与每个顶点相关联的分数，这表示图中顶点的“重要性”。请注意，TextRank运行到完成后获得的最终值，不受初始值的选择的影响，初始值的选择只会影响到收敛的迭代次数。</p>
<p>重要的是要注意，尽管本文中描述的TextRank应用程序依赖于从Google的PageRank（Brin和Page，1998）导出的算法，但是其他基于图表的排序算法，例如 HITS（Kleinberg，1999）或位置函数（Herings等，2001）也可以轻松地整合到TextRank模型中（Mihalcea，2004）。</p>
<h3 id="2-1_无向图">2.1 无向图</h3><p>虽然传统上应用于有向图，但是也可以将基于递归图的排序算法应用于无向图，在这种情况下，顶点的出度（out-degree）等于顶点的入度（in-degree）。对于松散连接的图形，随着边缘数量与顶点数量成比例，无向图趋向于具有更多的逐渐收敛曲线。</p>
<p>图1绘制了具有250个顶点和250个边缘的随机生成图的收敛曲线，收敛阈值为0.0001。随着图形的连通性增加（即较大数量的边缘），通常在较少迭代之后实现收敛，并且有向和无向图的收敛曲线实际上是重叠的。</p>
<p><img src="/img/17_04_26/002.png" alt=""></p>
<p>图1：基于图的收敛曲线排名：有向/无向，加权/未加权图，250个顶点，250个边。</p>
<h3 id="2-2_权重图">2.2 权重图</h3><p>在网页浏览的上下文中，页面中包含多个或部分链接到另一个页面是不寻常的，因此，基于图的原始PageRank定义是假设未加权的图。</p>
<p>然而，在我们的模型中的图是由自然语言文本构造的，并且也可以由包括从文本中提取的单元（顶点）之间的多个或部分链接构造。因此，可以在模型中指示并将两个顶点$V_i$和$V_j$之间的连接的“强度”作为加到连接两个顶点的对应边缘的权重$w_{ij}$来指示并合并到该模型中。因此，我们引入了一个新的基于图的排名公式，在计算与图中顶点相关的分数时考虑了边权重。请注意，可以定义类似的公式来整合顶点权重。</p>
<p><img src="/img/17_04_26/003.png" alt=""></p>
<p>图1绘制了2.1节中相同样本图的收敛曲线，其中对边进行了0到10的随机加权。尽管与未加权相比，最终顶点得分（因此排名）显着不同，但是对于加权和未加权图，收敛次数和收敛曲线的形状几乎相同。</p>
<h3 id="2-3_文本作为图">2.3 文本作为图</h3><p>为了使基于图的排序算法能够应用于自然语言文本，我们必须构建一个表示文本的图形，并将具有有意义关系的单词或其他文本实体进行互连。 根据手头的应用，各种尺寸和特征的文本单位可以作为图中的顶点添加，例如。 单词，搭配，整个句子或其他。 类似地，它是指示用于绘制任何两个这样的顶点之间的连接的关系的类型，例如。 词汇或语义关系，语境重叠等。</p>
<p>无论添加到图形中的元素的类型和特征如何，将基于图的排序算法应用于自然语言文本包括以下主要步骤：</p>
<ul>
<li>1.识别最佳定义手头任务的文本单位，并将其作为顶点添加到图形中。</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/04/20/斯坦福机器学习课程 第八周 (1)聚类/" itemprop="url">
                斯坦福机器学习课程 第八周 (1)聚类
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-04-20T22:32:58+08:00" content="2017-04-20">
            2017-04-20
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/04/20/斯坦福机器学习课程 第八周 (1)聚类/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/04/20/斯坦福机器学习课程 第八周 (1)聚类/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="无监督学习介绍">无监督学习介绍</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/czmip/unsupervised-learning-introduction" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>接下来，我将介绍<strong>聚类</strong>这一概念。保证精彩！因为这是我们第一个<strong>无监督学习算法</strong>。我们要从未标记的数据中进行学习, 而不是从已标记的数据。 </p>
</blockquote>
<p>什么是无监督学习算法呢？</p>
<p>之前，在本课程的开始阶段，我曾简短介绍过无监督学习算法。现在，我想将<strong>无监督学习算法</strong>与<strong>监督学习算法</strong>做个对照。</p>
<h3 id="无监督学习算法与监督学习算法对比">无监督学习算法与监督学习算法对比</h3><p>下面是一个监督学习的例子：</p>
<p><img src="/img/17_04_20/001.png" alt=""></p>
<p>$$<br>训练集：｛(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),(x^{(3)},y^{(3)}),…,(x^{(m)},y^{(m)})｝<br>$$</p>
<p>这是一组附有标记的训练数据集，我们想要找出一个决策边界，来将两者分开：</p>
<p><img src="/img/17_04_20/002.png" alt=""></p>
<p>在这种监督式学习中，我们针对一组标记的训练数据提出一个适当的假设。</p>
<hr>
<p>相比之下，在无监督学习案例中，我们面对的是一组<strong>无标记</strong>的训练数据，数据之间不具任何关联的标记。</p>
<p>所以我们得到的数据看起来是下面这样的：</p>
<p><img src="/img/17_04_20/003.png" alt=""></p>
<p>$$<br>训练集：｛x^{(1)},x^{(2)},x^{(3)},…,x^{(m)}｝<br>$$</p>
<p>所以，在无监督学习中，我们将这种未标记的训练数据送入特定的算法，然后我们要求算法替我们分析出数据的结构。</p>
<p>就此数据而言，其中一种可能的结构是所有的数据大致地划分成两个类（或组），这种划分的算法称为<strong>聚类算法()</strong>：</p>
<p><img src="/img/17_04_20/004.png" alt=""></p>
<p>除此之外，无监督学习还包含其他各式各样的算法，用以寻找其他类型的结构。我们下面将会一一介绍。目前，我们先介绍聚类。</p>
<h3 id="聚类">聚类</h3><p>稍早前，我已经提到几个应用实例：</p>
<p><img src="/img/17_04_20/005.png" alt=""></p>
<ul>
<li>图1是细分市场，将所有用户划分至不同的细分市场组，以便于营销或服务。</li>
<li>图2是社交分析体系，比如在社交网络中观察一群人，看他们和谁有电子邮件来往，或者查找一群相互有联系的人。</li>
<li>图3是用聚类来组织运算集群或组织数据中心，因为，如果你知道在集群中，哪些计算机的数据中心倾向于一起工作，你可以用它重新组织你的资源，网络的布局，以及数据中心和通信。</li>
<li>图4是使用聚类算法来试图理解星系的形成，和其中的天文细节。</li>
</ul>
<p>总之，聚类是我们学到的第一个无监督学习算法。在接下来的内容中，我将谈论聚类的具体实现方式。</p>
<h2 id="K-Means_算法">K-Means 算法</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/93VPG/k-means-algorithm" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在聚类问题中，我们有未加标签的数据。我们希望有一个算法能够自动的把这些数据分成<strong>有紧密关系的子集</strong>，或是<strong>簇</strong>。<strong>K均值 (K-means)算法</strong>是现在最为广泛使用的聚类方法。那么在这个视频中，我将会告诉你，什么是K均值算法以及它是怎么运作的。</p>
</blockquote>
<p>K均值算法最好用图来表达。如图所示：</p>
<p><img src="/img/17_04_20/006.png" alt=""></p>
<p>现在有一些<strong>没加标签</strong>的数据，而我想将这些数据分成两个<strong>簇</strong>。</p>
<p>现在我执行K均值算法 方法是这样的 </p>
<p>首先我随机选择两个点，这两个点叫做<strong>聚类中心 (cluster centroids) </strong>:</p>
<p><img src="/img/17_04_20/007.png" alt=""></p>
<p>为什么要两个点呢？因为我希望聚出两个类。</p>
<p>K均值是一个迭代方法，它要做两件事情：</p>
<ul>
<li>第一是<strong>簇分配</strong>。</li>
<li>第二个是<strong>移动聚类中心</strong>。</li>
</ul>
<p>接下来介绍这两个步骤具体是在做什么。</p>
<h3 id="K-Means_第一步：簇分配">K-Means 第一步：簇分配</h3><p>在K均值算法的每次循环中，第一步是要进行<strong>簇分配</strong>。这就是说，我要遍历所有的样本（就是图上所有的绿色的点），然后依据每一个点是更接近红色的这个中心、还是蓝色的这个中心，来将每个数据点分配到两个不同的聚类中心中。</p>
<p>具体来讲，就是对数据集中的所有点，依据他们更接近红色这个中心、还是蓝色这个中心，进行染色。染色之后的结果如图所示：</p>
<p><img src="/img/17_04_20/008.png" alt=""></p>
<p>以上就是簇分配的步骤。</p>
<h3 id="K-Means_第二步：移动聚类中心">K-Means 第二步：移动聚类中心</h3><p>K均值的另一部分，是要<strong>移动聚类中心</strong>。</p>
<p>具体的操作方法是这样的：我们将两个聚类中心（也就是红色的叉和蓝色的叉）移动到和它一样颜色的那堆点的均值处。</p>
<p>那么我们要做的是找出所有红色的点，计算出它们的均值位置，然后我们就把红色点的聚类中心移动到这里。对蓝色的点也同样计算平均位置，然后移动蓝色聚类中心到该平均位置处。</p>
<p><img src="/img/17_04_20/009.png" alt=""></p>
<h3 id="K-Means_第三步：重复执行上面两步">K-Means 第三步：重复执行上面两步</h3><p>然后我们就会进入下一个<strong>簇分配</strong>。我们重新检查所有没有标签的样本，依据它离红色中心还是蓝色中心更近一些，重新将它染成红色或是蓝色。</p>
<p><img src="/img/17_04_20/010.png" alt=""></p>
<p>然后我们再次<strong>移动聚类中心</strong>。计算蓝色点的均值，以及红色点的均值，然后移动两个聚类中心：</p>
<p><img src="/img/17_04_20/011.png" alt=""></p>
<p>然后再做一遍<strong>簇分配</strong>和<strong>移动聚类中心</strong>操作：</p>
<p><img src="/img/17_04_20/012.png" alt=""></p>
<p>实际上，如果你从这一步开始，一直迭代下去，聚类中心是不会变的；并且 那些点的颜色也不会变。在这时，我们就能说<strong>K均值方法已经收敛了</strong>。</p>
<h3 id="K-Means的规范化描述">K-Means的规范化描述</h3><p>我们来用更加规范的格式描述K均值算法。</p>
<p>K均值算法接受两个输入：</p>
<ul>
<li>第一个是参数$K$，表示你想从数据中聚类出的簇的个数。</li>
</ul>
<blockquote>
<p>稍后会讲到选择$K$的方法</p>
</blockquote>
<ul>
<li>第二个输入参数是训练集$｛x^{(1)},x^{(2)},…,x^{(m)}｝$</li>
</ul>
<blockquote>
<p>因为这是非监督学习，我们的数据集中不需要$y$，同时在非监督学习的 K均值算法里，我们约定$x^{(i)}$是一个$n$维向量，这就是“训练样本是$n$维而不是$n+1$维”的原因（按照惯例，排除$x_0=1$这一项）。</p>
</blockquote>
<p><strong>K均值算法：</strong></p>
<p><img src="/img/17_04_20/013.png" alt=""></p>
<ul>
<li><p>第一步：随机初始化$K$个<strong>聚类中心</strong>，记作$μ_1$,$μ_2$一直到$μ_K$。 </p>
</li>
<li><p>第二步：</p>
<ul>
<li><p>K均值内部循环执行以下步骤：</p>
<ul>
<li><p>簇分配</p>
<p> 首先对于每个训练样本，我们用变量$c^{(i)}$表示$K$个聚类中心中最接近$x^{(i)}$的那个中心的下标（具体的类别），这就是簇分配。</p>
<blockquote>
<p>大写的$K$表示所有聚类中心的个数，小写的$k$则表示某个聚类中心的下标。</p>
</blockquote>
<p>  我们希望的是：<strong>在所有K个中心中，找到一个$k$使得$x_i$到$μ_k$的距离是$x^{(i)}$到所有的聚类中心的距离中最小的那个</strong>，这就是计算$c_i$的方法。</p>
<p>  这里还有另外的表示$c_i$的方法：我用<strong>范数</strong>的形式$||x^{(i)}-μ_k||$来表示，这是第$i$个训练样本到聚类中心$μ_k$的距离。</p>
<p>  接下来我要做的是找出$k$的值，让这个式子$||x^{(i)}-μ_k||$最小，然后将 $c^{(i)}$ 赋值为$k$。</p>
<p>  出于惯例，人们更喜欢用距离的平方$||x^{(i)}-μ_k||^2$来表示$x^{(i)}$距聚类中心$μ_k$的距离。所以我们可以认为 $c^{(i)}$ 的类别是属于距样本$x^{(i)}$的距离的平方最小的那个聚类中心的。 当然使距离的平方最小或是距离最小，都能让我们得到相同的$c^{(i)}$，但是我们通常还是使用距离的平方，因为这是约定俗成的。</p>
</li>
<li><p>移动聚类中心</p>
<p>  对于每个聚类中心：$k$从1循环到$K$，将$μ_k$赋值为这个簇的均值。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>举个栗子：</strong></p>
<p>某一个聚类中心，比如说是$μ_2$被分配了一些训练样本：1,5,6,10 这个表明$c^{(1)}=2$，$c^{(5)}=2$，$c^{(6)}=2$，$c^{(10)}=2$。如果我们从<strong>簇分配</strong>那一步得到了这些结果，这表明，样本1,5,6,10被分配给了聚类中心2；然后在移动聚类中心这一步中，我们计算出这四个的平均值，即计算$x_{(1)}+x_{(5)}+x_{(6)}+x_{(10)}$，然后计算它们的平均值。这时$μ_2$就是一个$n$维的向量，因为$x^{(1)}$，$x^{(5)}$，$x^{(6)}$，$x^{(10)}$ 都是$n$维的向量。这样聚类中心$μ_2$的移动就结束了。</p>
<h3 id="异常情况">异常情况</h3><p>现在，我要问的问题是：</p>
<p>既然我们要让$μ_k$移动到分配给它的那些点的均值处，那么如果存在一个没有点分配给它的聚类中心，那怎么办? </p>
<p>通常在这种情况下，我们就直接移除那个聚类中心。如果这么做了，最终将会得到$K-1$个簇，而不是$K$个簇。</p>
<p>但如果你就是需要$K$个簇，尽管存在没有点分配给它的聚类中心，你所要做的是，重新随机找一个聚类中心。（但是直接移除那个中心，是更为常见的方法。不过在实际过程中，这个问题不会经常出现。）</p>
<hr>
<p>在这个视频结束之前，我还想告诉你<strong>K均值算法</strong>的另外一个常见应用：<strong>应对没有很好分开的簇(non-separated clusters)</strong>。</p>
<p>到目前为止，我们的K均值算法都是基于一些像图中所示的数据：</p>
<p><img src="/img/17_04_20/014.png" alt=""></p>
<p>有很好的隔离开来的三个簇，但是事实情况是，K均值经常会用于一些这样的数据：</p>
<p><img src="/img/17_04_20/015.png" alt=""></p>
<p>看起来并没有很好的分开几个簇。</p>
<p>这是一个关于T恤的大小的应用的例子。假设你是T恤制造商，你找到了一些人，想把T恤卖给他们，然后你搜集了一些这些人的身高和体重的数据。我猜，身高体重更重要一些。然后你可能收集到了上图中一些关于人们身高和体重的样本，然后你想确定一下T恤的大小。</p>
<p>假设我们要设计三种不同大小的t恤：小号、中号、和大号，那么小号应该是多大的?中号呢?大号呢?</p>
<p>使用K均值算法进行聚类，是一种解决这个问题的方法。就像我展示的那样，而且K均值可能将这些数据聚成三个簇：</p>
<p><img src="/img/17_04_20/016.png" alt=""></p>
<p>所以说，尽管这些数据原本看起来并没有三个分开的簇，但是从某种程度上讲，K均值仍然能将数据分成几个类。你能做的就是看这第一群人，然后查看他们的身高和体重，试着去设计对这群人来说比较合身的小号衣服；以及设计一个中号的衣服；设计一个大号的衣服。</p>
<p>这就是一种<strong>市场细分</strong>的例子。当你用K均值方法将你的市场分为三个不同的部分，你就能够区别对待你三类不同的顾客群体，从而更好的适应他们不同的需求。就像大、中、小，三种不同大小的衣服那样。</p>
<p><img src="/img/17_04_20/017.png" alt=""></p>
<p>这就是K均值算法，而且你现在应该已经知道如果去实现，K均值算法并且利用它解决一些问题。在下面的视频中，我想把K均值算法 研究的更深入一些，然后讨论一下如何能让K均值表现得更好一些的问题。</p>
<h2 id="优化目标">优化目标</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/G6QWt/optimization-objective" target="_blank" rel="external">视频地址</a></p>
<p>在大多数我们已经学到的<strong>监督学习</strong>算法中(例如线性回归，逻辑回归，以及更多的算法）都有一个优化目标函数，即需要通过算法进行最小化的代价函数。</p>
<p>事实上，<strong>K均值</strong>也有这样一个<strong>优化目标函数</strong>（或者说是代价函数）。</p>
<p>了解和使用这个<strong>K均值</strong>的优化目标函数有两方面的目的：</p>
<ul>
<li>首先这将能帮助我们调试学习算法，确保K均值算法是在正确运行中。</li>
<li>第二个也是最重要的一个目的是，<strong>K均值</strong>优化目标函数将帮助我们找到更好的簇，并且避免局部最优解。（后面会讲到）</li>
</ul>
<hr>
<p>另外顺便提一下，当K均值正在运行时，我们将对两组变量进行跟踪：</p>
<p>首先是$c^{(i)}$:</p>
<p>$$<br>c^{(i)}=表示 K 个聚类中心中最接近x^{(i)}的那个中心的索引（即当前样本x^{(i)}所归为的那个簇的索引）<br>$$</p>
<p>其次是$μ_k$:</p>
<p>$$<br>μ_k=表示 第k个簇的聚类中心 （μ_k\in R^n）<br>$$</p>
<p>顺便再提一句，K均值中我们用大写$K$来表示<strong>簇的总数</strong>，用小写$k$来表示<strong>聚类中心的序号</strong>。因此，小写$k$的范围如下：</p>
<p>$$<br>k\in｛1,2,…,K｝<br>$$</p>
<p>除此以外，还有另一个符号，我们用$μ_c^{(i)}$：</p>
<p>$$<br>μ_c^{(i)}=表示x^{(i)}所属的那个簇的聚类中心<br>$$</p>
<p>举个例子来解释：</p>
<p>假如说$x^{(i)}$被划为了第5个簇，这就是说$x^{(i)}$被分配到了第5个簇，也就是$c^{(i)}=5$。因此$μ_c^{(i)}=μ_5$。所以这里的 $μ_c^{(i)}$就是第5个簇的聚类中心。</p>
<p>而也正是我的样本$x^{(i)}$所属的第5个簇有了这样的符号表示，现在我们就能写出K均值聚类算法的优化目标了。</p>
<h3 id="K均值的代价函数">K均值的代价函数</h3><p>以下便是K均值算法需要最小化的<strong>代价函数</strong>：</p>
<p>$$<br>J(c^{(1)},…,c^{(m)},μ_1,…,μ_K)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-μ_c^{(i)}||^2<br>$$</p>
<p>代价函数$J$的参数$c^{(1)},…,c^{(m)}$以及$μ_1,…,μ_K$，随着算法的执行过程，这些参数将不断变化。</p>
<p>函数的右边给出了优化目标，即每个样本$x^{(i)}$到它所属的聚类中心距离的平方值。$x^{(i)}$就是训练样本的位置，$μ_c^{(i)}$是$x^{(i)}$样本所属的聚类中心的位置。</p>
<p>例如:</p>
<p><img src="/img/17_04_20/018.png" alt=""></p>
<p>如图，图中$x^{(i)}$样本被划分到了$μ_5$这个聚类中心，那么$||x^{(i)}-μ_c^{(i)}||^2$这个距离的平方，也就是在求样本点$x^{(i)}$到$μ_5$之间的距离的平方。</p>
<hr>
<p>K均值的目标就是要最小化代价函数：</p>
<p>$$<br>\min\limits_{c^{(1)},…,c^{(m)},\\μ_1,…,μ_K}<br>J(c^{(1)},…,c^{(m)},μ_1,…,μ_K)<br>$$</p>
<p>在K均值算法中，有时候也叫做<strong>失真代价函数(distortion cost function)</strong>。</p>
<hr>
<p>现在，我们已经理解了K均值算法的原理就是最小化代价函数$J$的过程。我们也可以用这个原理，来试着调试我们的学习算法，保证我们对K均值算法的实现过程是正确的。</p>
<p>在下一节视频中，我们将一起看看如何帮助K均值找到更好的簇，同时避免局部最优解。</p>
<h2 id="随机初始化">随机初始化</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/drcBh/random-initialization" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在本节课中，我们讨论一下如何初始化K均值聚类方法。</p>
<p>更重要的是，这将引导我们讨论<strong>如何避开局部最优来构建K均值聚类方法</strong>。</p>
</blockquote>
<p>对于我们之前讨论过的K均值聚类算法：</p>
<p><img src="/img/17_04_20/019.png" alt=""></p>
<p>其中我们之前没有讨论得太多的是这一步：</p>
<p><img src="/img/17_04_20/020.png" alt=""></p>
<p>即随机初始化聚类中心$μ_1,μ_2,…,μ_{K} \in R^n$。</p>
<p>如何初始化聚类中心这一步，有几种不同的方法可以用来随机初始化聚类中心。但是，事实证明，有一种可能是效果最好的方法。接下来我就将这个方法介绍给你。</p>
<hr>
<p>这里展示了我通常是如何初始化我的聚类中心的。</p>
<ul>
<li><p>1.确保$K&lt;m$</p>
<ul>
<li>当运行K均值方法时，你需要有一个聚类中心数值$K$，$K$值要比训练样本的数量$m$小，即$K&lt;m$。</li>
</ul>
</li>
<li><p>2.随机初始化</p>
<ul>
<li>随机挑选$K$个训练样本，然后我要做的是设定$μ_1,…,μ_k$让它们等于这$K$个样本。</li>
</ul>
</li>
</ul>
<hr>
<p>下面用一个具体例子来说明：</p>
<p>我们假设$K=2$，那么我们就需要找到两个聚类中心。为了初始化聚类中心，我要做的是随机挑选几个样本。比如说，我挑选了下面这两个点作为聚类中心：</p>
<p><img src="/img/17_04_20/021.png" alt=""></p>
<p>此时的这个例子看起来划分的相当不错，但是有时候我可能不会那么幸运。也许我最后会挑选到下面这样的两个点作为聚类中心：</p>
<p><img src="/img/17_04_20/022.png" alt=""></p>
<p>通过对上面两种初始化情况的对比，你可能会猜到K均值算法在它们两种情况下，会得到不同的结果。这取决于聚类簇的随机初始化方法。K均值方法最后可能得到不同的结果，尤其是如果K均值方法落在局部最优的时候。</p>
<p>如果给你一些这样的数据：</p>
<p><img src="/img/17_04_20/023.png" alt=""></p>
<p>这看起来好像有3个聚类：</p>
<p><img src="/img/17_04_20/024.png" alt=""></p>
<p>那么，如果你运行K均值方法，如果它最后得到一个局部最优，这可能是真正的全局最优，你可能会得到这样的聚类结果：</p>
<p><img src="/img/17_04_20/025.png" alt=""></p>
<p>但是如果你运气特别不好，随机初始化K均值方法也可能会卡在不同的局部最优上面：</p>
<p><img src="/img/17_04_20/026.png" alt=""></p>
<p>对于左边的这种情况，相当于将左下方和上方的样本分为了一类，将右下方的样本分了两类：</p>
<p><img src="/img/17_04_20/027.png" alt=""></p>
<p>对于右边的这种情况，相当于将下面的样本整体的分为了一类，将上方的样本分为了两类。</p>
<p>因此，如果你担心K均值方法会遇到上面这种局部最优的问题，并且你想提高K均值方法找到最有可能的聚类的几率的话，我们能做的就是尝试多次随机的初始化，而不是仅仅初始化一次K均值方法就希望它会得到很好的结果。</p>
<p>我们能做的是：<strong>初始化K均值很多次，并运行K均值方法很多次，通过多次尝试来保证我们最终能得到一个足够好的结果。一个尽可能局部或全局最优的结果。</strong></p>
<p>这才是正确的随机初始化聚类中心的方法。</p>
<h3 id="具体介绍">具体介绍</h3><p>具体来说，假如我决定运行K均值方法一百次，那么我就需要执行这个循环100次。这是一个相当典型的次数数字，有时会是从50到1000之间的数字。</p>
<p>假设说有决定运行K均值方法100次，那么这就意味着我们要随机初始化K均值方法100次。对于这100次随机初始化的每一次，我们都需要运行K均值方法。我们会得到一系列聚类结果和一系列聚类中心之后，我们可以计算<strong>失真函数</strong>$J$。用我们得到的这些聚类结果和聚类中心来计算这样一个结果函数。</p>
<p>完成整个100次迭代之后，你会得到这100种聚类数据的这些方法。最后你要做的是，在所有这100种用于聚类的方法中，选取能够给我们代价最小的一个，给我们最低畸变值的一个。</p>
<p>事实证明，如果你运行K均值方法时，所用的聚类数相当小。比如聚类数是从2到10之间的任何数的话，做多次的随机初始化，通常能够保证你能有一个较好的局部最优解，保证你能找到更好的聚类数据。但是如果K非常大的话，比如K比10大很多，就不太可能会有太大的影响。事实上，这种情况下有可能你的第一次随机初始化就会给你相当好的结果。</p>
<p>做多次随机初始化可能会给你稍微好一点的结果，但是不会好太多。但是在这样一个聚类数相对较小的体系里，特别是如果你有2个或者3个或者4个聚类的话，随机初始化会有较大的影响。可以保证你在最小化失真函数的时候，得到一个很小的值。并且能得到一个很好的聚类结果。</p>
<p>这就是K均值的随机初始化的方法。</p>
<h2 id="选择簇的数量">选择簇的数量</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Ks0E9/choosing-the-number-of-clusters" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在本节中，我想讨论一下K-均值聚类的最后一个细节，就是确定聚类的数目，即如何去选择$K$的值。</p>
<p>说实话，这个问题没有一个非常标准的解答。目前用来决定聚类数目的最常用的方法，仍然是通过看可视化的图，或者看聚类算法的输出结果，或者其他一些东西来手动地决定聚类的数目。</p>
<p>但是，我确实经常被别人问到这样的问题：“你是如何来选择聚类的数目的？” 我只能告诉你一些人们现在对这个问题的思考，人们现在对这个问题的最为常见的做法，实际上仍是手动选择聚类的数目。</p>
</blockquote>
<hr>
<p>选择聚类的数目可能不总是那么容易，大部分情况下，对于数据集中有多少个聚类中心通常是模棱两可的。</p>
<p>看到这样一个数据集：</p>
<p><img src="/img/17_04_20/028.png" alt=""></p>
<p>有些人可能会看到四个聚类，那么这就意味着需要使用$K=4$：</p>
<p><img src="/img/17_04_20/029.png" alt=""></p>
<p>或者有些人可能会看到两个聚类，这就意味着$K=2$：</p>
<p><img src="/img/17_04_20/030.png" alt=""></p>
<p>可能其他人会看到3个聚类。</p>
<p>在我看来它的真实的类别数实际上确实是模棱两可的，所以我并不认为这里有一个正确答案。这就是无监督学习的一部分。没有给我们标签，所以不会总有一个清晰的答案。这就是为什么，做一个能够自动选择聚类数目的算法，是非常困难的原因之一。</p>
<h3 id="肘部法则_(Elbow_Method)">肘部法则 (Elbow Method)</h3><p>当人们讨论选择聚类数目的方法时，可能会提及一个叫做<strong>肘部法则 (Elbow Method)</strong>的方法。现在我来介绍一下它，之后会提及到它的一些优点和缺点。</p>
<h4 id="肘部法则引入">肘部法则引入</h4><p>那么对于肘部法则，我们所需要做的是改变K的值（也就是聚类类别的总数）。</p>
<p>我们用K值为1来运行K-均值聚类算法。这就意味着所有的数据都会分到一个类里。然后计算代价函数（或者说计算畸变）$J$，并将其画在这儿：</p>
<p><img src="/img/17_04_20/031.png" alt=""></p>
<p>然后我们选用两个聚类来运行K-均值聚类算法。可能用了多个随机的初始中心，也可能没用。那么有两个聚类的话，我们很可能得到一个较小的畸变值，把它画在这儿：</p>
<p><img src="/img/17_04_20/032.png" alt=""></p>
<p>然后用三个聚类来运行K-均值聚类。你很有可能得到更小的畸变值，把它画在这儿：</p>
<p><img src="/img/17_04_20/033.png" alt=""></p>
<p>之后再让聚类数目等于4、5来运行K-均值聚类，最后我们就能得到一条曲线，它展示了随着聚类数量的增多，畸变值是如何下降的。我们可能会得到一条这样的曲线：</p>
<p><img src="/img/17_04_20/034.png" alt=""></p>
<p>看到这条曲线，肘部法则会说：“我们来看这个图，这里看起来像是一个很清楚的肘点”。</p>
<p><img src="/img/17_04_20/035.png" alt=""></p>
<p>这就类比于人的手臂。这就是肘部法则。</p>
<p>在这里，你会发现这样一种模式：K从1变化到2、再从2到3时，畸变值迅速下降；然后在3的时候，到达一个肘点。此后畸变值就下降得非常慢。这样看起来，也许使用3个类是聚类数目的正确选择。这是因为那个点是曲线的肘点。就是说畸变值快速地下降，直到$K=3$这个点，在这之后就下降得非常慢，那么我们就选$K=3$。</p>
<h4 id="肘部法则局限性">肘部法则局限性</h4><p>当你应用肘部法则的时候，如果你得到了一个像上面这样的图，那么这非常好，这是一种用来选择聚类个数的合理方法。而事实证明肘部法则并不那么常用，其中一个原因是如果你把这种方法用到一个聚类问题上，事实上你最后得到的曲线通常看起来是更加模棱两可的，就像这样：</p>
<p><img src="/img/17_04_20/036.png" alt=""></p>
<p>如果你看到这条曲线，也许没有一个清晰的肘点，而畸变值像是连续下降的，也许3是一个好选择，也许4是一个好选择，也许5也不差。如果实际情况中，你遇到的肘点的位置并不明确，这使得用这个方法来选择聚类数目变得较为困难。</p>
<h4 id="肘部法则小结">肘部法则小结</h4><p>简单小结一下肘部法则：它是一个值得尝试的方法，但是我不会期待它在任何问题上都有很高的表现。</p>
<h3 id="通过下游来决定聚类数量">通过下游来决定聚类数量</h3><p>最后，有另外一种方法来考虑如何选择K的值。</p>
<p>通常人们使用K-均值聚类算法是为了某些后面的用途，或者说某种下游的目的。而要求得一些聚类也许你会用K-均值聚类算法来做市场分割。例如我们之前谈论的T恤尺寸的例子，也许你会用K-均值聚类来让电脑的聚类变得更好，或者可能为了某些别的目的学习聚类，等等。如果那个后续下游的目的（比如市场分割）能给你一个评估标准，那么通常来说决定聚类数量的 更好的办法是，看不同的聚类数量能为后续下游的目的提供多好的结果。</p>
<p>我们来看一个具体的例子：</p>
<p>我们再看一下T恤尺寸这个例子。我想要决定“我是需要3种T恤尺寸么吗？”</p>
<p>所以我选择$K=3$。我可能有小号、中号、大号三类T恤，或者我可以选择$K=5$，那么我可能有特小号、小号、中号、大号和特大号尺寸的T恤。所以，你可能有3种、4种或者5种T恤尺寸。</p>
<p>因此如果我用$K=3$来运行K-均值聚类，我得到的结果可能是这样的：</p>
<p><img src="/img/17_04_20/037.png" alt=""></p>
<p>然而，果我用$K=5$来运行K-均值聚类的话，我得到的结果可能是这样的：</p>
<p><img src="/img/17_04_20/038.png" alt=""></p>
<p>这个例子给了我们对于选择聚类数目问题的另一种方法。</p>
<p>具体来说，你要做的是从T恤生意的角度来思考这个事情，然后问“如果我有5个分段 ，那么我的T恤有多适合我的顾客？那么我的T恤有多适合我的顾客？我可以卖出多少T恤？我的顾客将会有多高兴呢？”</p>
<p>从T恤生意的角度去考虑，其中真正有意义的是：我是需要更多的T恤尺寸 来更好地满足我的顾客？还是说我需要更少的T恤尺寸，我制造的T恤尺码就更少，我就可以将它们更便宜地卖给顾客？因此T恤的销售业务的观点 可能会提供给你一个决定采用3个类还是5个类的方法。</p>
<h3 id="簇数量选择_总结">簇数量选择 总结</h3><p>总结一下：大部分时候聚类数目仍然是通过手动人工输入或我们的洞察力来决定，一种可以尝试的方法是使用肘部法则，使用肘部法则但是我不会总是 期望它能表现得好。选择聚类数目的更好方法是去问一下你运行K-均值聚类是为了什么目的？然后想一想聚类的数目是多少才适合你运行K-均值聚类的后续目的。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/04/11/斯坦福机器学习课程 第七周 (3)使用SVM/" itemprop="url">
                斯坦福机器学习课程 第七周 (3)使用SVM
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-04-11T22:47:58+08:00" content="2017-04-11">
            2017-04-11
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/04/11/斯坦福机器学习课程 第七周 (3)使用SVM/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/04/11/斯坦福机器学习课程 第七周 (3)使用SVM/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="使用SVM">使用SVM</h2><blockquote>
<p>这段视频中，我们将介绍使用SVM时，我们实际上需要做些什么。</p>
</blockquote>
<p><strong>支持向量机</strong>是一个特定的优化问题，但是我不建议你自己去手动实现这一算法来求解参数$\theta$。就像如今只有很少的人，或者说根本没有人会考虑自己写代码，来实现对矩阵求逆，或求一个数的平方根等。我们只需要调用库函数来实现这些功能即可。</p>
<p>同样，可以用于解决SVM优化问题的软件很复杂，而且已经有专门研究数值优化很多年的学者在做这个，因此你需要使用好的软件库来做这个。我强烈建议使用一个高度优化的软件库，而不是尝试自己去实现它。</p>
<p>这里推荐两个我最常用到的库：liblinear和libsvm。</p>
<p>尽管你不需要自己去实现SVM，但你也需要做以下几件事：</p>
<ul>
<li>选择参数$C$</li>
<li>选择<strong>核函数</strong>（相似度函数）</li>
</ul>
<h3 id="核函数的选择">核函数的选择</h3><h4 id="线性核函数（无核函数）">线性核函数（无核函数）</h4><p>关于<strong>核函数</strong>其中一个选择是<strong>不用任何核函数</strong>（不用任何核函数也叫作<strong>线性核函数</strong>）:</p>
<p>即对于预测结果$y=1$，满足$\theta^Tx\ge0$。</p>
<blockquote>
<p>例如这种情况下当$\theta_0 + \theta_1x_1 + … + \theta_nx_n \ge 0$时，预测$y=1$。</p>
</blockquote>
<p>对<strong>线性核函数</strong>这个术语，你可以把它理解为这个版本的SVM。它只是给你一个标准的线性分类器，因此对某些问题来说，它是一个合理的选择：</p>
<p>具体来说，当你的特征数量$n$很大，但数据量$m$很小时，由于数据量不足，在这种情况下如果使用其他核函数，你可能会<strong>过拟合</strong>，因此，此时<strong>线性核函数</strong>是一个合理的选择。</p>
<h4 id="高斯核函数">高斯核函数</h4><p>$$<br>f_i=exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}),<br>$$</p>
<p>这是我们之前见过的高斯核函数。如果你选择这个核函数，那么你需要选择一个$$\sigma^2$$。</p>
<p>我们之前讨论如何权衡偏差、方差的时候谈论过：</p>
<ul>
<li>如果$\sigma^2$很大，那么你就有可能得到一个<strong>较高偏差较低方差</strong>的分类器。</li>
<li>如果$\sigma^2$很小，那么你就有可能得到一个<strong>较低偏差较高方差</strong>的分类器。</li>
</ul>
<p>那么，什么时候选择高斯核函数呢？</p>
<p>如果你原来的特征变量$x$是$n$维的，而且<strong>$n$很小，样本数量$m$很大时</strong>，高斯核函数会是一个不错的选择。</p>
<h5 id="如何使用高斯核函数">如何使用高斯核函数</h5><p>在很多SVM的软件包中，如果你需要使用SVM时，你需要提供一个核函数。</p>
<p>具体地说，如果你决定使用高斯核函数，那么你需要做的就是根据你所用的SVM软件包，来提供一个用于计算核函数的特定特征的方法:</p>
<p><img src="/img/17_04_11/001.png" alt=""></p>
<p>然后它将自动地生成所有特征变量。</p>
<blockquote>
<p><strong>注意</strong>：如果你有大小很不一样的特征变量，在使用高斯核函数之前，对它们进行<strong>归一化</strong>是很重要的。</p>
</blockquote>
<p>假设你在计算$x$和$l$之间的范数(就是高斯核函数的分子项)：</p>
<p>$||x-l||^2$</p>
<p>这个式子所表达的含义如下：</p>
<p>$$<br>v=x-l<br>$$</p>
<p>$$<br>\begin{align*}<br>||v||^2 &amp;= v_1^2 + v_2^2 + … + v_n^2 \\<br>&amp;= (x_1-l_1)^2 + (x_2-l_2)^2 + … + (x_n-l_n)^2<br>\end{align*}<br>$$</p>
<p>其中$v$、$x$、$l$都是向量，由于$x$是$n$维的，所以$v$也是$n$维的。</p>
<p>现在如果你的特征变量取值范围很不一样。例如房价预测中，$x_1$表示1000平方英尺，$x_2$表示卧室数量为2，那么$(x_1-l_1)$可能相较于$(x_2-l_2)$大很多。</p>
<p>因此，为了让SVM更好的工作，我们需要对特征变量进行<strong>归一化</strong>处理。这将会保证SVM能够同等地关注到所有不同的特征变量。</p>
<h4 id="选择其他核函数">选择其他核函数</h4><p>当你尝试使用SVM时，目前你能用到的核函数就是<strong>线性核函数</strong>和<strong>高斯核函数</strong>，这里有一个警告：</p>
<p>不是所有你可能提出来的相似度函数都是有效的核函数。线性核函数，高斯核函数，以及其他人有时会用到的其他的核函数，他们全部需要满足一个技术条件，这个条件叫做<strong>摩赛尔定理(Mercer`s Theorem)</strong>。</p>
<p>因为支持向量机算法的实现有许多巧妙的数值优化技巧，为了有效地求解参数$\theta$，在最初的设想里，有一个这样的决定，将我们的注意力仅仅限制在可以满足<strong>摩赛尔定理</strong>的核函数上，这个定义做的是：确保所有的SVM包能够使用大量的优化方法，并且快速地得到参数$\theta$。所以，大多数人最后要么使用线性核函数、要么使用高斯核函数。也有一些其他的核函数是满足<strong>摩赛尔定理</strong>的，而我个人是很少很少使用其他核函数的。</p>
<p>所以，我只是简单提及一下你可能会遇到的其他核函数，他们有：</p>
<ul>
<li><p><strong>多项式核函数（Polynomial kernel）</strong>：</p>
<ul>
<li><p>将$x$和$l$之间的相似度，定义为$(x^Tl)^2$：</p>
<p>$$<br>k(x,l)=(x^Tl)^2<br>$$</p>
<p>这就是一个$x$和$l$相似度的估量，如果$x$和$l$每一项很接近，那么这个内积就会很大。</p>
<p>这是一个有些不寻常的核函数，它并不常用，但你可能会见到有人使用它的变体形式，比如：</p>
<p>$$<br>k(x,l)=(x^Tl)^3<br>$$</p>
<p>$$<br>k(x,l)=(x^Tl + 1)^3<br>$$</p>
<p>$$<br>k(x,l)=(x^Tl + 5)^4<br>$$</p>
<p>这些都是多项式核函数的变形形式。</p>
<p>多项式核函数实际上有两个参数，一个是加在后面的常数项，如上面最后式子中的5；另一个是多项式的次数，如上面最后式子中的4。</p>
<p>因此，多项式核函数的更一般的形式是：</p>
<p>$$<br>k(x,l)=(x^Tl + constant)^{degree}<br>$$</p>
<p>这种核函数并不像高斯核函数那样频繁的使用，通常他只用在当$x$和$l$都是严格的非负数时。这样以确保内积值永远不会是负数。</p>
<p>这捕捉到了这样一个直观感觉：如果$x$和$l$之间非常相似，也许它们之间的内积会很大。</p>
<p>它们也有其他的一些性质，但是人们通常用得不多。</p>
</li>
</ul>
</li>
</ul>
<p>你也有可能会碰到其他一些更难懂的核函数，比如：</p>
<ul>
<li><strong>字符串核函数（String kernel）</strong>:<ul>
<li>如果你的输入数据是文本字符串，或者其他类型的字符串，有时会用到这个核函数。</li>
</ul>
</li>
<li><strong>卡方核函数（chi-square kernel）</strong></li>
<li><strong>直方图交叉核函数（histogram intersection kernel）</strong></li>
<li>…</li>
</ul>
<p>你可以用它们来度量不同对象之间的相似性。</p>
<p>例如，你在做一些文本分类问题，在这个问题中，输入变量$x$是一个字符串，我们想要通过字符串核函数来找到两个字符串间的相似度（但是我个人很少用这些更加难懂的核函数，我想我平生可能用过一次卡方核函数，可能用过一次或两次直方图交叉核函数，我甚至没用过字符串核函数）。</p>
<hr>
<h3 id="两个细节">两个细节</h3><p>我想要在这个视频里讨论最后两个细节。</p>
<h4 id="多类分类">多类分类</h4><p>在多类分类中，你有K个类别：</p>
<p>$$<br>y \in ｛1,2,3,…,K｝<br>$$</p>
<p>对应下图：</p>
<p><img src="/img/17_04_11/002.png" alt=""></p>
<p>很明显，这里$K=4$。</p>
<p>那么怎样让SVM输出下面这种各个类别间合适的判定边界呢？</p>
<p><img src="/img/17_04_11/003.png" alt=""></p>
<p>大部分SVM软件包已经内置了多类分类的函数了，因此，如果你用的是这种软件包，你可以直接使用内置函数。</p>
<p>另一种方式就是使用<strong>一对多(one-vs-all)方法</strong>。这个我们在讲逻辑回归的时候讨论过，所以，你要做的就是要训练$K$个SVM，每一个SVM把一个类同其他类区分开。这种操作会给你$K$个参数向量：</p>
<p>$$<br>\theta^{(1)},\theta^{(2)},…,\theta^{(K)}<br>$$</p>
<p>最后，这就与我们在逻辑回归中用到的一对多方法一样，选取使得$(\theta^{(i)})^Tx$最大的类$i$即可。</p>
<blockquote>
<p>其实大多数软件包都已经内置了多类分类的函数，因此你不必重新造轮子。</p>
</blockquote>
<h4 id="逻辑回归_vs_SVM">逻辑回归 vs SVM</h4><p>关于<strong>逻辑回归</strong>和<strong>SVM</strong>，我想讨论的是，你什么时候应该用哪个呢？</p>
<p>假设$n$是特征变量的个数，$m$是训练样本数：</p>
<ul>
<li>如果$n$(相对于$m$)大很多时，使用<strong>逻辑回归</strong>，或者使用<strong>无核函数的SVM（线性核函数）</strong>。<br>  比如你有一个文本分类的问题，特征数量$n=10000$，而且如果你的训练集大小为$m=10$，在这个问题中，你有10000个特征变量，对应10000个词，但是你只有10个训练样本。这种情况下就比较适合使用逻辑回归或者线性核函数的SVM了。</li>
<li>如果$n$较小，$m$是中等大小，（例如$n$为1到1000之间的值，$m$为10到10000之间的值）那么使用<strong>高斯核函数的SVM</strong>效果好一些。</li>
<li>如果$n$很小，$m$很大时（例如$n=1000$,$m=100000+$），那么高斯核函数的SVM运行起来会很慢，这种情况下，需要<strong>尝试手动地创建更多的特征变量，然后使用逻辑回归或者无核函数的SVM（线性核函数）</strong>。</li>
</ul>
<p>逻辑回归和不带核函数的SVM它们都是非常相似的算法，他们会做相似的事情，并且表现也相似，但是根据你实现的具体情况，其中一个可能会比另一个更加有效。</p>
<p>但是SVM的威力会随着你用不同的核函数而发挥出来。</p>
<h4 id="什么时候使用神经网络？">什么时候使用神经网络？</h4><p>最后，神经网络应该在什么时候使用呢？</p>
<p>对于上面所有的情况，一个设计得很好的神经网络也很可能会非常有效，它的一个缺点（或者说不使用神经网络的原因）是：神经网络训练起来可能会很慢。但是如果你有一个非常好的SVM实现包，它会运行得比较快，比神经网络快很多。</p>
<blockquote>
<p>SVM的优化问题，实际上是一个<strong>凸优化</strong>问题。因此好的SVM优化软件包总是会找到全局最小值，或者接近它的值。<br>对于SVM，你不需要担心局部最优。在实际应用中，局部最优对神经网络来说不是非常大，但是也不小。所以使用SVM，你不用考虑这部分问题。</p>
</blockquote>
<hr>
<h3 id="总结">总结</h3><p>最后，如果你觉得上面这些使用参考有一些模糊，在面临实际问题时，仍然不能完全确定具体使用哪个算法更好，这个其实很正常。</p>
<p>当我遇到机器学习问题时，有时确实不清楚具体使用哪个算法更好，但是正如你在之前的视频中看到的，算法确实很重要，但是通常更重要的是：<strong>你有多少数据</strong>，<strong>你有多熟练</strong>，<strong>是否擅长做误差分析和调试学习算法</strong>，<strong>想出如何设计新的特征变量</strong>，<strong>想出如何设计新的特征变量</strong>，以及<strong>找出应该输入给学习算法的其它特征变量</strong>等方面。通常这些方面会比你使用<strong>逻辑回归</strong>还是<strong>SVM</strong>更加重要。</p>
<p>但是<strong>SVM</strong>仍然被广泛认为是<strong>最强大的学习算法之一</strong>，最强大的学习算法之一，而且SVM在一个区间内是一个非常有效地学习复杂非线性函数的方法。因此，有了<strong>逻辑回归</strong>、<strong>神经网络</strong>、<strong>SVM</strong>这三个学习算法，我想你已经具备了在广泛的应用里构建最前沿的机器学习系统的能力。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/03/21/通过AspectJ代码注入来实现scheme跳转条件的检查判断/" itemprop="url">
                通过AspectJ代码注入来实现scheme跳转条件的检查判断
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-03-21T22:48:58+08:00" content="2017-03-21">
            2017-03-21
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Android/" itemprop="url" rel="index">
                  <span itemprop="name">Android</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/03/21/通过AspectJ代码注入来实现scheme跳转条件的检查判断/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/21/通过AspectJ代码注入来实现scheme跳转条件的检查判断/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>scheme跳转是Android通过外部链接打开APP指定页面的一种常见的实现方式，如果只是简单的跳转，那么不需要做什么额外的判断就可以打开指定页面了，但我们的产品中有一个需求就是：在通过scheme跳转打开某一个页面的时候，需要判断一些前置条件，如果前置条件满足的情况下，才能执行跳转，如果前置条件不满足，那么需要缓存本次跳转，直到需要满足的条件被触发时，才去执行跳转。</p>
<p>简单说来就是下面这张图：</p>
<p><img src="/img/17_03_21/001.png" alt=""></p>
<p>一开始我想到的处理方式是通过在Activity的基类里的<code>onCreate()</code>方法之前做判断逻辑，如果符合条件，则正常执行，如果条件不满足，则执行<code>finish()</code>。</p>
<p>虽然可以满足需求，但这样的代码侵入性太高，逻辑必须侵入到Activity的基类中，容易与基类中其他逻辑产生耦合。</p>
<p>因此为了解决这个方式，我想到的解决方式就是使用AOP的方式，在<code>onCreate()</code>之前注入我们的判断逻辑的代码，最后的使用方式可以简化到仅仅使用一行注解来添加判断条件：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@SchemeCheck</span>(conditions = &#123;Condition.LOGIN&#125;)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TargetActivityA</span> <span class="keyword">extends</span> <span class="title">Activity</span> </span>&#123;</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;</div><div class="line">        <span class="keyword">super</span>.onCreate(savedInstanceState);</div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这样一来，我们就不用考虑在Activity的基类中写这些可能产生冗余的逻辑了。</p>
<p>最终实现效果如下：</p>
<ul>
<li>判断<strong>登录</strong>条件</li>
</ul>
<video width="300px" autoplay="autoplay" loop="loop" id="video" controls preload="none"><br>  <source id="mp4" src="/img/17_03_21/002.mp4" type="video/mp4"><br>  <p>Your user agent does not support the HTML5 Video element.</p><br></video>

<ul>
<li>判断<strong>下载</strong>条件</li>
</ul>
<video width="300px" autoplay="autoplay" loop="loop" id="video" controls preload="none"><br>  <source id="mp4" src="/img/17_03_21/003.mp4" type="video/mp4"><br>  <p>Your user agent does not support the HTML5 Video element.</p><br></video>

<ul>
<li>判断<strong>登录并且下载</strong></li>
</ul>
<video width="300px" align="center" autoplay="autoplay" loop="loop" id="video" controls preload="none"><br>  <source id="mp4" src="/img/17_03_21/004.mp4" type="video/mp4"><br>  <p>Your user agent does not support the HTML5 Video element.</p><br></video>

<ul>
<li>判断登录或者下载</li>
</ul>
<video width="300px" autoplay="autoplay" loop="loop" id="video" controls preload="none"><br>  <source id="mp4" src="/img/17_03_21/005.mp4" type="video/mp4"><br>  <p>Your user agent does not support the HTML5 Video element.</p><br></video>

<p>接下来介绍一下我是如何实现的。</p>
<h2 id="如何实现">如何实现</h2><p>首先关于<strong>依赖注入</strong>以及<strong>AspectJ</strong>的相关使用，我参考了以下文章以及代码，具体使用方式我就不再赘述：</p>
<ul>
<li><a href="http://www.jianshu.com/p/0fa8073fd144" target="_blank" rel="external">【翻译】Android中的AOP编程</a>  这一篇对AOP概念进行了介绍，并且通过AspectJ仿照<a href="https://github.com/JakeWharton/hugo" target="_blank" rel="external">Hugo</a>实现了一个AOP的Demo。</li>
<li>JakeWharton大神的<a href="https://github.com/JakeWharton/hugo" target="_blank" rel="external">hugo</a> 项目，通过AspectJ实现的日志工具。</li>
<li><a href="http://blog.csdn.net/crazy__chen/article/details/52014672" target="_blank" rel="external">使用AspectJ在Android中实现Aop</a>这一篇文章是对上面的文章和hugo项目的总结。</li>
</ul>
<hr>
<p>下面是我的实现：</p>
<p>首先，定义注解：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="variable">@Retention</span>(RetentionPolicy.RUNTIME)</div><div class="line"><span class="variable">@Target</span>(&#123;ElementType.TYPE&#125;)</div><div class="line">public <span class="variable">@interface</span> SchemeCheck &#123;</div><div class="line">    Condition<span class="selector-attr">[]</span> <span class="selector-tag">conditions</span>();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>值得提醒的是，这里<code>@Retention</code>要定义为<code>RetentionPolicy.RUNTIME</code>，因为我们要在运行时检查注解的参数，来判断scheme的触发条件。如果你写成了<code>RetentionPolicy.CLASS</code>或者<code>RetentionPolicy.SOURCE</code>就检查不到了。</p>
<p><code>Condition</code>是我们定义的需要判断的条件枚举：</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">public <span class="class"><span class="keyword">enum</span> <span class="title">Condition</span> &#123;</span></div><div class="line">    NULL(null),</div><div class="line">    LOGIN(new LoginCondition()),</div><div class="line">    DOWNLOAD_BOOK(new BookDownloadCondition()),</div><div class="line">    LOGIN_OR_DOWNLOAD_BOOK(new LoginOrBookDownloadCondition());</div><div class="line"></div><div class="line">    BaseCondition condition;</div><div class="line">    Condition(BaseCondition condition) &#123;</div><div class="line">        this.condition = condition;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public BaseCondition getCondition() &#123;</div><div class="line">        <span class="keyword">return</span> condition;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>Condition</code>可以通过<code>getCondition()</code>来获取到具体的继承自<code>BaseCondition</code>的Condition对象：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BaseCondition</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">isSatisfied</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> String <span class="title">unSatisfiedInfo</span><span class="params">()</span></span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>由于是Demo，我们的登录条件暂时写死，到时候换成你具体的业务逻辑即可：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginCondition</span> <span class="keyword">extends</span> <span class="title">BaseCondition</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> isLogin = <span class="keyword">false</span>;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSatisfied</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> isLogin;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">unSatisfiedInfo</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="keyword">return</span> <span class="string">"请先登录"</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>有了这些，我们就可以写注入代码了：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Aspect</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SchemeCheckAspect</span> </span>&#123;</div><div class="line">    <span class="meta">@Pointcut</span>(<span class="string">"within(@demo.com.aj.anno.SchemeCheck *)"</span>)</div><div class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">withinAnnotatedClass</span><span class="params">()</span> </span>&#123;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Pointcut</span>(<span class="string">"execution(!synthetic * *(..)) &amp;&amp; withinAnnotatedClass()"</span>)</div><div class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">methodInsideAnnotatedType</span><span class="params">()</span> </span>&#123;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Pointcut</span>(<span class="string">"execution(@demo.com.aj.anno.SchemeCheck * *(..)) || methodInsideAnnotatedType()"</span>)</div><div class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">method</span><span class="params">()</span> </span>&#123;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Around</span>(<span class="string">"method()"</span>)</div><div class="line">    <span class="keyword">public</span> <span class="function">Object <span class="title">weaveJoinPoint</span><span class="params">(ProceedingJoinPoint joinPoint)</span> <span class="keyword">throws</span> Throwable </span>&#123;</div><div class="line">        Signature signature = joinPoint.getSignature();</div><div class="line">        String methodName = signature.getName();</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (TextUtils.equals(methodName, <span class="string">"onCreate"</span>)) &#123;</div><div class="line">            SchemeCheck anno = (SchemeCheck) signature.getDeclaringType().getAnnotation(SchemeCheck.class);</div><div class="line">            <span class="keyword">if</span> (anno != <span class="keyword">null</span>) &#123;</div><div class="line">                Condition[] conditions = anno.conditions();</div><div class="line">                Object point = joinPoint.getThis();</div><div class="line">                <span class="keyword">if</span> (point != <span class="keyword">null</span> &amp;&amp; point <span class="keyword">instanceof</span> Activity) &#123;</div><div class="line">                    Activity activity = (Activity) point;</div><div class="line">                    handleScheme(activity, conditions);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="function"><span class="keyword">return</span> joinPoint.<span class="title">proceed</span><span class="params">()</span></span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 检查前置条件</div><div class="line">     *</div><div class="line">     * <span class="doctag">@return</span> 是否通过检查</div><div class="line">     */</div><div class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">boolean</span> <span class="title">checkCondition</span><span class="params">(Condition[] conditions)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span> (conditions != <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="keyword">for</span> (Condition condition : conditions) &#123;</div><div class="line">                BaseCondition conditionObj = condition.getCondition();</div><div class="line">                <span class="keyword">if</span> (conditionObj != <span class="keyword">null</span></div><div class="line">                        &amp;&amp; !conditionObj.isSatisfied()) &#123;</div><div class="line">                    Toast.makeText(App.getContext(), conditionObj.unSatisfiedInfo(), Toast.LENGTH_SHORT).show();</div><div class="line">                    SchemeManager.Cache.save(condition);</div><div class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 通过验证</div><div class="line">     */</div><div class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">passSatisfy</span><span class="params">()</span> </span>&#123;</div><div class="line">        SchemeManager.Cache.clear();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 处理前置条件的检查结果</div><div class="line">     *</div><div class="line">     * <span class="doctag">@param</span> isPass 检查是否通过</div><div class="line">     */</div><div class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">handleCheck</span><span class="params">(Activity activity, <span class="keyword">boolean</span> isPass)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span> (isPass) &#123;</div><div class="line">            passSatisfy();</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            <span class="keyword">if</span> (activity != <span class="keyword">null</span>) &#123;</div><div class="line">                activity.finish();</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     *</div><div class="line">     * 处理scheme相关的事情</div><div class="line">     *</div><div class="line">     * <span class="doctag">@param</span> activity</div><div class="line">     * <span class="doctag">@param</span> conditions</div><div class="line">     * <span class="doctag">@return</span> 是否通过验证</div><div class="line">     */</div><div class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">boolean</span> <span class="title">handleScheme</span><span class="params">(Activity activity, Condition[] conditions)</span> </span>&#123;</div><div class="line">        <span class="keyword">boolean</span> passCheck = <span class="keyword">true</span>;</div><div class="line">        <span class="keyword">if</span> (isStartByScheme(activity)) &#123;</div><div class="line">            passCheck = checkCondition(conditions);</div><div class="line">            handleCheck(activity, passCheck);</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            passCheck = <span class="keyword">true</span>;</div><div class="line">            passSatisfy();</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> passCheck;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 检查是否是由scheme开启</div><div class="line">     *</div><div class="line">     * <span class="doctag">@return</span></div><div class="line">     */</div><div class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">boolean</span> <span class="title">isStartByScheme</span><span class="params">(Activity activity)</span> </span>&#123;</div><div class="line">        <span class="keyword">boolean</span> isScheme = <span class="keyword">false</span>;</div><div class="line">        <span class="keyword">if</span> (activity != <span class="keyword">null</span>) &#123;</div><div class="line">            Intent intent = activity.getIntent();</div><div class="line">            <span class="keyword">if</span> (intent != <span class="keyword">null</span>) &#123;</div><div class="line">                isScheme = intent.getBooleanExtra(KEY_IS_SCHEME, <span class="keyword">false</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> isScheme;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可以看到，我们在<code>onCreate()</code>方法开始之前，我们注入了scheme的判断逻辑，当条件满足时，直接执行了后面的逻辑；当条件不满足时，将不满足的条件进行缓存，并且<code>finish()</code>当前Activity。当正常执行了scheme跳转之后，清空缓存。</p>
<p>其中<code>SchemeManager</code>的逻辑如下：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SchemeManager</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String KEY_IS_SCHEME = <span class="string">"isScheme"</span>;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Cache</span> </span>&#123;</div><div class="line">        <span class="keyword">public</span> <span class="keyword">static</span> String next = <span class="string">""</span>;</div><div class="line">        <span class="keyword">public</span> <span class="keyword">static</span> Condition unSatisfiedCondition;</div><div class="line"></div><div class="line">        <span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">void</span> <span class="title">save</span><span class="params">(Condition condition)</span> </span>&#123;</div><div class="line">            unSatisfiedCondition = condition;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span></span>&#123;</div><div class="line">            next = <span class="string">""</span>;</div><div class="line">            unSatisfiedCondition = <span class="keyword">null</span>;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">void</span> <span class="title">reExecuteScheme</span><span class="params">(Activity activity, Condition condition)</span> </span>&#123;</div><div class="line">        <span class="keyword">if</span> (Cache.unSatisfiedCondition != <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="comment">// 当缓存的条件与当前重复执行时触发的条件一致时，再次执行scheme</span></div><div class="line">            <span class="keyword">if</span> (Cache.unSatisfiedCondition == condition) &#123;</div><div class="line">                String scheme = Cache.next;</div><div class="line">                executeScheme(activity, scheme);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">void</span> <span class="title">executeScheme</span><span class="params">(Activity activity, String schemeStr)</span> </span>&#123;</div><div class="line">        Cache.next = schemeStr;</div><div class="line">        Class&lt;? <span class="keyword">extends</span> Activity&gt; <span class="keyword">target</span> = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">switch</span> (schemeStr) &#123;</div><div class="line">            <span class="keyword">case</span> <span class="string">"startA"</span>:</div><div class="line">                <span class="keyword">target</span> = TargetActivityA.class;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> <span class="string">"startB"</span>:</div><div class="line">                <span class="keyword">target</span> = TargetActivityB.class;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> <span class="string">"startC"</span>:</div><div class="line">                <span class="keyword">target</span> = TargetActivityC.class;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> <span class="string">"startD"</span>:</div><div class="line">                <span class="keyword">target</span> = TargetActivityD.class;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (<span class="keyword">target</span> != <span class="keyword">null</span>) &#123;</div><div class="line">            <span class="comment">// 如果当前页和目标页面是同一个页面  则清空缓存  防止递归调用产生死循环</span></div><div class="line">            <span class="keyword">if</span> (<span class="keyword">target</span>.equals(activity.getClass())) &#123;</div><div class="line">                Cache.clear();</div><div class="line">                <span class="keyword">return</span>;</div><div class="line">            &#125;</div><div class="line">            Intent intent = <span class="keyword">new</span> Intent(activity, <span class="keyword">target</span>);</div><div class="line">            intent.putExtra(KEY_IS_SCHEME, <span class="keyword">true</span>);</div><div class="line">            activity.startActivity(intent);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里包含有缓存管理的逻辑，以及<strong>执行scheme</strong>和<strong>重复执行scheme</strong>的逻辑。</p>
<p>其中，由于是Demo，执行scheme的逻辑用固定的字符串来代表scheme链接，这里需要你来替换为你自己的业务逻辑，因为你可能有解析scheme参数并且传递到目标Activity的逻辑。</p>
<p>scheme缓存保存了scheme链接和最后一次没有通过的条件枚举值，当下一次条件被触发时，通过<code>reExecuteScheme</code>来执行缓存中的scheme，达到继续跳转的效果。</p>
<p>例如，在登录成功的地方，可以执行:</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">reExecuteScheme(<span class="keyword">this</span>, Condition.LOGIN);</div></pre></td></tr></table></figure>
<p>来达到登录成功时继续触发<strong>因未登录导致的scheme跳转失败的scheme跳转</strong>（好绕啊…）。</p>
<h2 id="愉快的调用">愉快的调用</h2><p>接下来我们在需要加入scheme跳转判断的Activity的类上加入注解判断即可，例如我们在下面的Activity上加入<strong>登录</strong>以及<strong>词书下载</strong>的条件判断：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="variable">@SchemeCheck</span>(conditions = &#123;Condition.LOGIN, Condition.DOWNLOAD_BOOK&#125;)</div><div class="line">public class TargetActivityC extends Activity &#123;</div><div class="line"></div><div class="line">    <span class="selector-tag">public</span> <span class="selector-tag">static</span> <span class="selector-tag">void</span> <span class="selector-tag">start</span>(Activity activity) &#123;</div><div class="line">        activity<span class="selector-class">.startActivity</span>(new Intent(activity, TargetActivityC.class));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="variable">@Override</span></div><div class="line">    protected void onCreate(Bundle savedInstanceState) &#123;</div><div class="line">        super<span class="selector-class">.onCreate</span>(savedInstanceState);</div><div class="line">        setContentView(R<span class="selector-class">.layout</span><span class="selector-class">.activity_c</span>);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>是不是很简单呢？</p>
<h2 id="注意！这里有坑">注意！这里有坑</h2><p>目前发现的一个坑就是我们的加入scheme跳转判断的子类Activity中必须有<code>onCreate</code>方法才可以正常执行，因为<code>AspectJ</code>只能判断当前子类中所触发的子类的方法。也就是说如果即使你对Activity的基类的<code>onCreate</code>进行了一层封装，完成了<code>onCreate</code>的所有工作，子类也需要复写一下<code>onCreate</code>。就像这样：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line">   <span class="keyword">protected</span> <span class="function"><span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;</div><div class="line">       <span class="keyword">super</span>.onCreate(savedInstanceState);</div><div class="line">   &#125;</div></pre></td></tr></table></figure>
<p>否则会crash。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/2/">&raquo;</a>
  </nav>

 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://dannylee1991.github.io/images/avatar.jpg" alt="DannyLee佳楠" itemprop="image"/>
          <p class="site-author-name" itemprop="name">DannyLee佳楠</p>
        </div>
        <p class="site-description motion-element" itemprop="description">一只在迈向机器学习道路上狂奔的程序猿.</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">112</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">16</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">19</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DannyLee1991" target="_blank">GitHub</a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee佳楠</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dannylee1991"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
