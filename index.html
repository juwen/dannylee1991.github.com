<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿." />



  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta property="og:type" content="website">
<meta property="og:title" content="DannyLee">
<meta property="og:url" content="http://dannylee1991.github.io/index.html">
<meta property="og:site_name" content="DannyLee">
<meta property="og:description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DannyLee">
<meta name="twitter:description" content="一只在迈向机器学习道路上狂奔的程序猿.">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>

<!--baidu统计-->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2f967e5ec4f276411160d27aeace7722";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  <title> DannyLee </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">DannyLee</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu menu-left">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      

      
      
        <li class="menu-item menu-item-search">
          <a href="#" class="st-search-show-outputs">
            <i class="menu-item-icon icon-next-search"></i> <br />
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'ss9-_Hsd4DyhyGw4m99P','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/03/03/【Tensorflow r1.0 文档翻译】Tensorflow原理导论/" itemprop="url">
                【Tensorflow r1.0 文档翻译】Tensorflow原理导论
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-03-03T21:19:58+08:00" content="2017-03-03">
            2017-03-03
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/03/03/【Tensorflow r1.0 文档翻译】Tensorflow原理导论/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/03/【Tensorflow r1.0 文档翻译】Tensorflow原理导论/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>代码：<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/" target="_blank" rel="external">tensorflow/examples/tutorials/mnist/</a></p>
<p>这篇教程的目的是为了展示如何使用TensorFlow来训练并评估一个简单的<strong>前馈神经网络(feed-forward neural network)</strong>用来识别MNIST手写数字数据集。本教程的目标读者是有兴趣使用TensorFlow的有经验的机器学习用户。</p>
<p>这部分教程不是为了教授普通的机器学习。</p>
<p>请确保您已按照说明<a href="https://www.tensorflow.org/install/index" target="_blank" rel="external">安装了TensorFlow</a>。</p>
<h2 id="教程文件">教程文件</h2><p>本教程引用以下文件：</p>
<table>
<thead>
<tr>
<th style="text-align:left">文件</th>
<th style="text-align:left">目标</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py" target="_blank" rel="external"><code>mnist.py</code></a></td>
<td style="text-align:left">构建一个完全连接的MNIST模型的代码。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py" target="_blank" rel="external"><code>fully_connected_feed.py</code></a></td>
<td style="text-align:left">利用下载的数据集训练构建好的MNIST模型的主要代码，以数据反馈字典（feed dictionary）的形式作为输入模型。</td>
</tr>
</tbody>
</table>
<p>只需要运行<code>fully_connected_feed.py</code>文件，就可以开启训练：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">python</span> fully_connected_feed.<span class="keyword">py</span></div></pre></td></tr></table></figure>
<h2 id="准备数据">准备数据</h2><p>MNIST是机器学习中的经典问题。这个问题是查看28x28像素的手写数字灰度图像，并确定图像表示的数字，数字范围是0到9。</p>
<p><img src="/img/17_03_03/001.png" alt=""></p>
<p>更多的信息，参加<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">Yann LeCun’s MNIST page</a>或者<a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" target="_blank" rel="external">Chris Olah’s visualizations of MNIST</a>。</p>
<h3 id="下载">下载</h3><p>在<code>run_training()</code>方法的开始部分，<code>input_data.read_data_sets()</code>方法会确保你的本地训练文件夹中，已经下载了正确的数据，然后将这些数据解压并返回一个含有<code>DataSet</code>实例的字典。</p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dat<span class="built_in">a_sets</span> = input_data.read_dat<span class="built_in">a_sets</span>(FLAGS.train_dir, FLAGS.fake_data)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong><code>fake_data</code>标记是用于单元测试的，读者可以不必理会。</p>
<table>
<thead>
<tr>
<th style="text-align:left">数据集</th>
<th style="text-align:left">目标</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>data_sets.train</code></td>
<td style="text-align:left">55000图像和标签，用于初级训练。</td>
</tr>
<tr>
<td style="text-align:left"><code>data_sets.validation</code></td>
<td style="text-align:left">5000图像和标签，用于迭代验证训练准确性。</td>
</tr>
<tr>
<td style="text-align:left"><code>data_sets.test</code></td>
<td style="text-align:left">10000图像和标签，用于最终测试训练的准确性。</td>
</tr>
</tbody>
</table>
<h3 id="输入和占位符">输入和占位符</h3><p><code>placeholder_inputs()</code>方法创建了两个<code>tf.placeholder</code>操作，用于定义输入的形状。形状参数中包含<code>batch_size</code>值，后续还会将实际的训练样本传入图中。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attr">images_placeholder</span> = tf.placeholder(tf.float32, <span class="attr">shape=(batch_size,</span></div><div class="line">                                                       mnist.IMAGE_PIXELS))</div><div class="line"><span class="attr">labels_placeholder</span> = tf.placeholder(tf.int32, <span class="attr">shape=(batch_size))</span></div></pre></td></tr></table></figure>
<p>在训练的循环代码的下方，传入的整个图像和标签数据集会被切片，以符合每一个操作所设置的<code>batch_size</code>值，占位符操作将会填补以符合这个<code>batch_size</code>值。然后使用<code>feed_dict</code>参数，将数据传入<code>sess.run()</code>函数。</p>
<h2 id="构建图">构建图</h2><p>在为数据创建占位符之后，就可以运行<code>mnist.py</code>文件，经过三阶段的模式函数操作：<code>inference()</code>， <code>loss()</code>，和<code>training()</code>。图表就构建完成了。</p>
<ul>
<li>1.<code>inference()</code>-尽可能地构建好图表，满足促使神经网络向前反馈并做出预测的要求。</li>
<li>2.<code>loss()</code>-往inference图表中添加生成损失（loss）所需要的操作（ops）。</li>
<li>3.<code>training()</code>-往损失图表中添加计算并应用梯度（gradients）所需的操作。</li>
</ul>
<p><img src="/img/17_03_03/002.png" alt=""></p>
<h3 id="推理(Inference)">推理(Inference)</h3><p><code>inference()</code>函数会尽可能地构建图表，做到返回包含了预测结果（output prediction）的Tensor。</p>
<p>它采用图像占位符作为输入，并在其上借助<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="external">ReLU</a>)激活函数构建一对完全连接层，以及一个有着十个节点、指明了输出logtis模型的线性层。</p>
<p>每个图层都在唯一的<a href="https://www.tensorflow.org/api_docs/python/tf/name_scope" target="_blank" rel="external"><code>tf.name_scope</code></a>下创建，创建于该作用域之下的所有元素都将带有其前缀。</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'hidden1'</span>):</div></pre></td></tr></table></figure>
<p>在定义的范围内，由这些层中的每一个使用的权重和偏差被生成为<a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="external"><code>tf.Variable</code></a>实例，具有它们期望的形状：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">weights = <span class="keyword">tf</span>.Variable(</div><div class="line">    <span class="keyword">tf</span>.truncated_normal([IMAGE_PIXELS, hidden1_units],</div><div class="line">                        stddev=<span class="number">1.0</span> / math.<span class="built_in">sqrt</span>(float(IMAGE_PIXELS))),</div><div class="line">    name=<span class="string">'weights'</span>)</div><div class="line">biases = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.zeros([hidden1_units]),</div><div class="line">                     name=<span class="string">'biases'</span>)</div></pre></td></tr></table></figure>
<p>例如，当在<code>hidden1</code>范围下创建这些时，赋予权重变量的唯一名称将是“<code>hidden1 / weights</code>”。</p>
<p>每个变量在构建时，都会执行初始化操作。</p>
<p>在大多数情况下，通过<a href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal" target="_blank" rel="external"><code>tf.truncated_normal</code></a>函数初始化权重变量，给赋予的shape则是一个二维tensor，其中第一个维度代表该层中权重变量所连接（connect from）的单元数量，第二个维度代表该层中权重变量所连接到的（connect to）单元数量。第一层，名字为<code>hidden1</code>，它的尺寸是<code>[IMAGE_PIXELS, hidden1_units]</code>，因为权重变量将图像输入连接到了<code>hidden1</code>层。<code>tf.truncated_normal</code>初始函数将根据所得到的均值和标准差，生成一个随机分布。</p>
<p>然后，通过<a href="https://www.tensorflow.org/api_docs/python/tf/zeros" target="_blank" rel="external"><code>tf.zeros</code></a>函数初始化偏差变量（biases），确保所有偏差的起始值都是0，而它们的形状则是其在该层中所接到的（connect to）单元数量。</p>
<p>图表的三个主要操作，分别是两个<code>tf.nn.relu</code>操作，它们中嵌入了隐藏层所需的<a href="https://www.tensorflow.org/api_docs/python/tf/matmul" target="_blank" rel="external"><code>tf.matmul</code></a>；以及logits模型所需的另外一个<code>tf.matmul</code>。三者依次生成，各自的<code>tf.Variable</code>实例则与输入占位符或下一层的输出tensor所连接。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden1 = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(images, weights) + biases)</div></pre></td></tr></table></figure>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden2 = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(hidden1, weights) + biases)</div></pre></td></tr></table></figure>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">logits</span> = tf.matmul(hidden2, weights) + biases</div></pre></td></tr></table></figure>
<p>最终，程序会返回包含了输出结果的<code>logits</code>Tensor。</p>
<h3 id="损失">损失</h3><p><code>loss()</code>函数通过添加所需的损失操作，进一步构建图表。</p>
<p>首先，来自<code>labels_placeholder</code>的值将转换为64位整数。然后，添加一个<a href="https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits" target="_blank" rel="external">tf.nn.sparse_softmax_cross_entropy_with_logits</a>操作，以从<code>labels_placeholder</code>自动生成1-hot标签，并且与<code>inference()</code>函数的输出logits与那些1-hot标签进行比较。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attr">labels</span> = tf.to_int64(labels)</div><div class="line"><span class="attr">cross_entropy</span> = tf.nn.sparse_softmax_cross_entropy_with_logits(</div><div class="line">    <span class="attr">labels=labels,</span> <span class="attr">logits=logits,</span> <span class="attr">name='xentropy')</span></div></pre></td></tr></table></figure>
<p>然后使用<a href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" target="_blank" rel="external"><code>tf.reduce_mean</code></a>来求在批量维度（第一维度）上的交叉熵的平均值，作为总损失。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">loss</span> = tf.reduce_mean(cross_entropy, name=<span class="string">'xentropy_mean'</span>)</div></pre></td></tr></table></figure>
<p>然后将包含损失值的张量返回。</p>
<blockquote>
<p><strong>注意：</strong>交叉熵是信息论中的一种理论，它用于描述神经网络的预测结果相对于实际所给定的真实结果的偏差程度。更多的信息，请参阅博文<a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="external">《可视化信息理论》</a>。</p>
</blockquote>
<h3 id="训练">训练</h3><p><code>training()</code>方法通过添加<a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="external">梯度下降</a>的操作来最小化损失。</p>
<p>首先，它通过<code>loss()</code>方法接受损失tensor，然后传递到<a href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar" target="_blank" rel="external"><code>tf.summary.scalar</code></a>，用于在与<code>SummaryWriter</code>（见下文）一起使用时生成事件文件中的摘要值的操作。在这里，它将在每次写出摘要时发出损失的快照值。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf<span class="selector-class">.summary</span><span class="selector-class">.scalar</span>(<span class="string">'loss'</span>, loss)</div></pre></td></tr></table></figure>
<p>接下来，我们实例化一个<a href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer" target="_blank" rel="external"><code>tf.train.GradientDescentOptimizer</code></a>，负责按照所要求的学习效率（learning rate）应用梯度下降法（gradients）。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(learning_rate)</div></pre></td></tr></table></figure>
<p>之后，我们生成一个单个的变量用于统计全局训练的次数，<a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize" target="_blank" rel="external"><code>tf.train.Optimizer.minimize</code></a>操作被同时用作在系统中更新可训练的权值，以及增加全局步长（global step）。按照惯例，这个操作被称为<code>train_op</code>，TensorFlow会话必须运行的，以便引入一个完整的训练步骤（见下文）。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">global_step</span> = tf.Variable(<span class="number">0</span>, name=<span class="string">'global_step'</span>, trainable=<span class="literal">False</span>)</div><div class="line"><span class="attr">train_op</span> = optimizer.minimize(loss, global_step=global_step)</div></pre></td></tr></table></figure>
<h2 id="训练模型">训练模型</h2><p>一旦图被构建，它就可以在由<code>fully_connected_feed.py</code>中的用户代码控制的循环中迭代地训练和求值。</p>
<h3 id="图">图</h3><p>在<code>run_training()</code>方法的一开始的部分，是一个python的<code>with</code>命令，这表示所有构建的操作将与默认全局<a href="https://www.tensorflow.org/api_docs/python/tf/Graph" target="_blank" rel="external"><code>tf.Graph</code></a>实例相关联。</p>
<figure class="highlight coq"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">with</span> tf.<span class="keyword">Graph</span>().as_default():</div></pre></td></tr></table></figure>
<p><code>tf.Graph</code>实例是一系列可以作为整体执行的操作。TensorFlow的大部分场景只需要依赖默认图表一个实例即可。</p>
<p>利用多个图表的更加复杂的使用场景也是可能的，但是超出了本教程的范围。</p>
<h3 id="会话">会话</h3><p>完成全部的构建准备、生成全部所需的操作之后，我们就可以创建一个<a href="https://www.tensorflow.org/api_docs/python/tf/Session" target="_blank" rel="external">tf.Session</a>，用于运行图表。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sess</span> = tf.Session()</div></pre></td></tr></table></figure>
<p>另外，也可以利用<code>with</code>代码块生成<code>Session</code>，限制作用域：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></div></pre></td></tr></table></figure>
<p><code>Session</code>函数中没有传入参数，表明该代码将会依附于（如果还没有创建会话，则会创建新的会话）默认的本地会话。</p>
<p>生成会话之后，所有<code>tf.Variable</code>实例都会立即通过调用各自初始化操作中的<a href="https://www.tensorflow.org/api_docs/python/tf/Session#run" target="_blank" rel="external"><code>tf.Session.run</code></a>函数进行初始化。</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">init</span> = tf.global_variables_initializer()</div><div class="line">sess.run(<span class="keyword">init</span>)</div></pre></td></tr></table></figure>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/Session#run" target="_blank" rel="external"><code>tf.Session.run</code></a>方法将会运行图表中与作为参数传入的操作相对应的完整子集。在初次调用时，<code>init</code>操作只包含了变量初始化程序<a href="https://www.tensorflow.org/api_docs/python/tf/group" target="_blank" rel="external"><code>tf.group</code></a>。图表的其他部分不会在这里，而是在下面的训练循环运行。</p>
<h3 id="训练循环">训练循环</h3><p>在通过会话来初始化变量后，就可以开始训练了。</p>
<p>训练的每一步都是通过用户代码控制，而能实现有效训练的最简单循环就是：</p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="keyword">step</span> <span class="keyword">in</span> xrange(FLAGS.max_steps):</div><div class="line">    sess.<span class="built_in">run</span>(train_op)</div></pre></td></tr></table></figure>
<p>但是，本教程中的例子要更为复杂一点，原因是我们必须把输入的数据根据每一步的情况进行切分，以匹配之前生成的占位符。</p>
<h3 id="向图表提供反馈">向图表提供反馈</h3><p>执行每一步时，我们的代码会生成一个反馈字典（feed dictionary），其中包含对应步骤中训练所要使用的样本，这些样本的key就是其所代表的占位符操作。</p>
<p><code>fill_feed_dict</code>函数会查询给定的<code>DataSet</code>，索要下一批次`batch_size的图像和标签，与占位符相匹配的Tensor则会包含下一批次的图像和标签。</p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">images_feed, labels_feed = data_set.next_batch(<span class="keyword">FLAGS</span>.batch_size,</div><div class="line">                                               <span class="keyword">FLAGS</span>.fake_data)</div></pre></td></tr></table></figure>
<p>然后，以占位符作为键，创建一个Python字典对象，值则是其代表的反馈Tensor。</p>
<figure class="highlight dts"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">feed_dict = &#123;</div><div class="line"><span class="symbol">    images_placeholder:</span> images_feed,</div><div class="line"><span class="symbol">    labels_placeholder:</span> labels_feed,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个字典随后作为<code>feed_dict</code>参数，传入<code>sess.run()</code>函数中，为这一步的训练提供输入样本。</p>
<h3 id="检查状态">检查状态</h3><p>在运行<code>sess.run</code>时，要在代码中明确其需要获取的两个值：<code>[train_op, loss]</code>。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="keyword">step</span> <span class="keyword">in</span> <span class="built_in">xrange</span>(FLAGS.max_steps):</div><div class="line">    feed_dict = fill_feed_dict(data_sets.train,</div><div class="line">                               images_placeholder,</div><div class="line">                               labels_placeholder)</div><div class="line">    <span class="symbol">_</span>, loss_value = sess.run([train_op, loss],</div><div class="line">                             feed_dict=feed_dict)</div></pre></td></tr></table></figure>
<p>因为要获取这两个值，<code>sess.run()</code>会返回一个有两个元素的元组。其中每一个<code>Tensor</code>对象，对应了返回的元组中的numpy数组，而这些数组中包含了当前这步训练中对应Tensor的值。由于<code>train_op</code>并不会产生输出，其在返回的元祖中的对应元素就是<code>None</code>，所以会被抛弃。但是，如果模型在训练中出现偏差，<code>loss</code> Tensor的值可能会变成NaN，所以我们要获取它的值，并记录下来。</p>
<p>假设训练一切正常，没有出现NaN，训练循环会每隔100个训练步骤，就打印一行简单的状态文本，告知用户当前的训练状态。</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">if</span> <span class="built_in">step</span> % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    <span class="built_in">print</span> <span class="string">'Step %d: loss = %.2f (%.3f sec)'</span> % (<span class="built_in">step</span>, loss_value, duration)</div></pre></td></tr></table></figure>
<h3 id="状态可视化">状态可视化</h3><p>为了发出<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard</a>所使用的事件文件（events file），所有的摘要（在这里只有一个）都要在图构建阶段合并至一个Tensor中。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">summary</span> = tf<span class="selector-class">.summary</span><span class="selector-class">.merge_all</span>()</div></pre></td></tr></table></figure>
<p>在创建好会话（session）之后，可以实例化一个<a href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter" target="_blank" rel="external"><code>tf.summary.FileWriter</code></a>，用于写入包含了图表本身和即时数据具体值的事件文件。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">summary_writer = tf<span class="selector-class">.summary</span><span class="selector-class">.FileWriter</span>(FLAGS<span class="selector-class">.train_dir</span>, sess.graph)</div></pre></td></tr></table></figure>
<p>最后，每次评估<code>summary</code>(摘要)并将输出传递给<code>add_summary()</code>函数时，事件文件将被新的摘要值更新。</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">summary_str = sess.<span class="built_in">run</span>(summary, feed_dict=feed_dict)</div><div class="line">summary_writer.add_summary(summary_str, <span class="built_in">step</span>)</div></pre></td></tr></table></figure>
<p>事件文件写入完毕之后，可以就训练文件夹打开一个TensorBoard，查看即时数据的情况。</p>
<p><img src="/img/17_03_03/003.png" alt=""></p>
<blockquote>
<p><strong>注意：</strong>了解更多如何构建并运行TensorBoard的信息，请查看相关教程<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">Tensorboard：训练过程可视化</a>。</p>
</blockquote>
<h3 id="保存检查点">保存检查点</h3><p>为了得到可以用来后续恢复模型以进一步训练或评估的检查点文件（checkpoint file），我们实例化一个<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="external"><code>tf.train.Saver</code></a>。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver = tf<span class="selector-class">.train</span><span class="selector-class">.Saver</span>()</div></pre></td></tr></table></figure>
<p>在训练循环中，将定期调用<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#save" target="_blank" rel="external"><code>tf.train.Saver.save</code></a>方法，使用所有可训练变量的当前值将检查点文件写入训练目录。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver.<span class="built_in">save</span>(sess, FLAGS.train_dir, global_step=<span class="keyword">step</span>)</div></pre></td></tr></table></figure>
<p>在将来的某个时间点，可以通过使用<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#restore" target="_blank" rel="external"><code>tf.train.Saver.restore</code></a>方法重新加载模型参数来恢复训练。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">saver</span><span class="selector-class">.restore</span>(<span class="selector-tag">sess</span>, <span class="selector-tag">FLAGS</span><span class="selector-class">.train_dir</span>)</div></pre></td></tr></table></figure>
<h2 id="评估模型">评估模型</h2><p>每隔一千个训练步骤，我们的代码会尝试使用训练数据集与测试数据集，对模型进行评估。<code>do_eval</code>函数会被调用三次，分别使用训练数据集、验证数据集合测试数据集。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">print</span> <span class="string">'Training Data Eval:'</span></div><div class="line"><span class="keyword">do</span>_<span class="built_in">eval</span>(sess,</div><div class="line">        <span class="built_in">eval</span>_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.train)</div><div class="line"><span class="built_in">print</span> <span class="string">'Validation Data Eval:'</span></div><div class="line"><span class="keyword">do</span>_<span class="built_in">eval</span>(sess,</div><div class="line">        <span class="built_in">eval</span>_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.validation)</div><div class="line"><span class="built_in">print</span> <span class="string">'Test Data Eval:'</span></div><div class="line"><span class="keyword">do</span>_<span class="built_in">eval</span>(sess,</div><div class="line">        <span class="built_in">eval</span>_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.test)</div></pre></td></tr></table></figure>
<blockquote>
<p>注意，更复杂的使用场景通常是，先隔绝<code>data_sets.test</code>测试数据集，只有在大量的超参数优化调整（hyperparameter tuning）之后才进行检查。但是，由于MNIST问题比较简单，我们在这里一次性评估所有的数据。</p>
</blockquote>
<h3 id="构建评估图(Eval_Graph)">构建评估图(Eval Graph)</h3><p>在打开默认图表（Graph）之前，我们应该先调用get_data(train=False)函数，抓取测试数据集。</p>
<p>在进入训练循环之前，评估操作应该通过<code>mnist.py</code>中的<code>evaluate()</code>函数来构建。<code>evaluate()</code>传入的<code>logist</code>和标签参数与<code>loss()</code>函数相同。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">eval_correct</span> = mnist.evaluation(logits, labels_placeholder)</div></pre></td></tr></table></figure>
<p><code>evaluation()</code>函数会生成<a href="https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k" target="_blank" rel="external"><code>tf.nn.in_top_k</code></a>操作，如果在K个最有可能的预测中可以发现真的标签，那么这个操作就会将模型输出标记为正确。在本文中，我们把K的值设置为1，也就是只有在预测是真的标签时，才判定它是正确的。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">eval_correct = tf<span class="selector-class">.nn</span><span class="selector-class">.in_top_k</span>(logits, labels, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<h3 id="评估输出">评估输出</h3><p>之后，我们可以创建一个循环，往其中添加<code>feed_dict</code>，并在调用<code>sess.run()</code>函数时传入<code>eval_correct</code>操作，目的就是用给定的数据集评估模型。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(steps_per_epoch):</div><div class="line">    feed_dict = fill_feed_dict(data_<span class="built_in">set</span>,</div><div class="line">                               images_placeholder,</div><div class="line">                               labels_placeholder)</div><div class="line">    <span class="literal">true</span>_count += sess.run(<span class="built_in">eval</span>_correct, feed_dict=feed_dict)</div></pre></td></tr></table></figure>
<p><code>true_count</code>变量会累加所有<code>in_top_k</code>操作判定为正确的预测之和。接下来，只需要将正确测试的总数，除以例子总数，就可以得出准确率了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">precision = <span class="literal">true</span>_count / num_examples</div><div class="line"><span class="built_in">print</span>(<span class="string">'  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f'</span> %</div><div class="line">      (num_examples, <span class="literal">true</span>_count, precision))</div></pre></td></tr></table></figure></span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/26/【Tensorflow r1.0 文档翻译】深入MNIST--专家级/" itemprop="url">
                【Tensorflow r1.0 文档翻译】深入MNIST--专家级
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-26T14:50:58+08:00" content="2017-02-26">
            2017-02-26
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/26/【Tensorflow r1.0 文档翻译】深入MNIST--专家级/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/26/【Tensorflow r1.0 文档翻译】深入MNIST--专家级/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>TensorFlow是一个用于进行大规模数值计算的强大库。其擅长的任务之一是实施和训练深层神经网络。在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。</p>
<p>这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看<a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">新手指南</a>。在开始之前，请确认<a href="https://www.tensorflow.org/install/index" target="_blank" rel="external">安装</a>了TensorFlow。</p>
<h2 id="关于本教程">关于本教程</h2><p>本教程的第一部分解释了<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py" target="_blank" rel="external">mnist_softmax.py</a>代码中发生了什么，这是Tensorflow模型的基本实现。第二部分显示了一些提高精度的方法。</p>
<p>您可以将本教程中的每个代码段复制并粘贴到Python环境中，当然你也可以选择只是读一下这部分代码。</p>
<p>我们将在本教程中完成：</p>
<ul>
<li>创建一个softmax回归函数，这是一个用于识别MNIST数字的模型，其原理是基于查看图像中的每个像素。</li>
<li>使用Tensorflow来训练模型以识别数字，方法是“查看”数千个示例（并运行我们的第一个Tensorflow会话）。</li>
<li>使用我们的测试数据检查模型的精度。</li>
<li>构建，训练和测试多层卷积神经网络以提高结果。</li>
</ul>
<h2 id="准备工作">准备工作</h2><p>在我们创建模型之前，我们首先加载MNIST数据集，并启动TensorFlow会话。</p>
<h3 id="加载MNIST数据">加载MNIST数据</h3><p>如果您要复制粘贴本教程中的代码，请从这两行代码开始，这两行代码将自动下载并读入数据：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from tensorflow<span class="selector-class">.examples</span><span class="selector-class">.tutorials</span><span class="selector-class">.mnist</span> import input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=True)</div></pre></td></tr></table></figure>
<p>这里的<code>mnist</code>是一个轻量级类，将训练集，验证集和测试集存储为NumPy数组。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。</p>
<h3 id="启动TensorFlow_InteractiveSession">启动TensorFlow InteractiveSession</h3><p>Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。</p>
<p>这里，我们使用更加方便的<code>InteractiveSession</code>类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些<a href="https://www.tensorflow.org/get_started/get_started#the_computational_graph" target="_blank" rel="external">计算图</a>，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用<code>InteractiveSession</code>，那么你需要在启动session之前构建整个计算图，然后启<a href="https://www.tensorflow.org/get_started/get_started#the_computational_graph" target="_blank" rel="external">动该计算图</a>。</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="title">sess</span> = tf.<span class="type">InteractiveSession</span>()</div></pre></td></tr></table></figure>
<h3 id="计算图">计算图</h3><p>为了在Python中执行高效的数值计算，我们通常引入类似<strong><a href="http://www.numpy.org/" target="_blank" rel="external">NumPy</a></strong>这种库来执行开销昂贵的操作。例如在Python之外其他高效的语言来执行矩阵乘法这类操作。不幸的是，每次操作之后切换回Python的动作依然是一个巨大的开销。这种开销特别的差，如果你想要以一种分布式的方式运行在GPU上的话，这里传输数据将会是一个巨大的开销。</p>
<p>TensorFlow也会在Python外部执行大量的运算，但它做了进一步的处理来规避了这种开销。取代独立于Python运行单一的代价昂贵的操作的模式，TensorFlow的方式是通过在Python中描述一个可交互的操作图，然后完全在Python之外进行运行。<strong>Theano</strong>或者<strong>Torch</strong>也有与此类似的实现。</p>
<p>在这里Python代码的作用是用来在外部定义一个操作图，然后决定具体哪一部分的运算图要被运行。详细内容，见<a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门/">TensorFlow入门</a>中的<a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门#用于计算的Graph（图）">用于计算的Graph（图）</a>部分。</p>
<h2 id="构建一个Softmax回归模型">构建一个Softmax回归模型</h2><p>这一节，我们通过一个单一的线性层来构建一个softmax回归模型。在下一节中，我们将把这个softmax回归扩展为一个多层卷积网络。</p>
<h3 id="占位符（Placeholders）">占位符（Placeholders）</h3><p>我们通过创建输入的图像创建的节点和输出的类别创建的分类来构建一个计算图。</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, <span class="built_in">shape</span>=[<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">y_ = tf.placeholder(tf.float32, <span class="built_in">shape</span>=[<span class="keyword">None</span>, <span class="number">10</span>])</div></pre></td></tr></table></figure>
<p>这里<code>x</code>和<code>y_</code>不是具体的值。相反，他们都是一个<code>placeholder</code>（占位符）–当我们让TensorFlow开始执行计算时才被输入具体值。</p>
<p>输入图像的<code>x</code>包含一个2维的浮点数张量。这里我们赋予它一个<code>shape</code>（形状）为<code>[None, 784]</code>，其中<code>784</code>是由28乘28像素的图片单行展开后的维度数，<code>None</code>表示第一个维度大小不定，可以是任意尺寸，用以指代batch的大小。目标输出类别<code>y_</code>也包含一个2维的tensor，它每行都是一个10维的one-hot向量，用于表示相应的MNIST图像属于哪个数字类（0到9）。</p>
<p>虽然<code>placeholder</code>的<code>shape</code>参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。</p>
<h3 id="变量（Variables）">变量（Variables）</h3><p>我们现在为我们的模型定义了权值<code>W</code>和偏置量<code>b</code>。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：<code>Variable</code>。一个<code>Variable</code>代表TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用<code>Variable</code>来表示。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W</span> = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</div><div class="line"><span class="attr">b</span> = tf.Variable(tf.zeros([<span class="number">10</span>]))</div></pre></td></tr></table></figure>
<p>我们在调用<code>tf.Variable</code>的时候传入初始值。在这个例子中，我们把<code>W</code>和<code>b</code>初始化全为0的tensor。<code>W</code>是一个$784×10$的矩阵（因为我们有784个输入特征以及10个输出值），<code>b</code>是一个10维向量（因为我们有10种分类）。</p>
<p>在<code>Variable</code>可以在session中被使用之前，他们必须被session初始化。此步骤使用已经指定的初始值（在这里tensor全部以0填充），并将它们分配给每个$Variable$。下面的代码可以一次初始化全部的<code>Variables</code>：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">tf</span><span class="selector-class">.global_variables_initializer</span>())</div></pre></td></tr></table></figure>
<h3 id="类别预测与损失函数">类别预测与损失函数</h3><p>现在我们可以实现我们自己的回归模型了。只需要一行代码！我们把向量化后的图片输入<code>x</code>和权重矩阵<code>W</code>相乘，加上偏置<code>b</code>，然后计算每个分类的softmax概率值。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">y</span> = tf.matmul(x,W) + b</div></pre></td></tr></table></figure>
<p>我们可以很容易地指定一个损失函数。损失表示模型的预测效果在单个示例的糟糕程度；在我们的训练过程中，我们会尽量去最小化这个值。在这里，我们的损失函数就是介于目标值和应用于模型预测的softmax激励函数之间的交叉熵。正如我们在新手教学中用到的稳定的方程一样：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">cross_entropy</span> = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(<span class="attr">labels=y_,</span> <span class="attr">logits=y))</span></div></pre></td></tr></table></figure>
<p>请注意，<code>tf.nn.softmax_cross_entropy_with_logits</code>内部将softmax应用到非规范化的模型预测中，并且将所有的结果求和，通过<code>tf.reduce_mean</code>来取这些和的平均值。</p>
<h2 id="训练模型">训练模型</h2><p>现在我们已经定义好了我们的模型和用于训练的损失函数，那么用TensorFlow进行训练就很简单了。由于TensorFlow知道整个计算图，所以它可以使用自动微分来找出关于每个变量的损失梯度。TensorFlow有多种<a href="https://www.tensorflow.org/api_guides/python/train#optimizers" target="_blank" rel="external">内置的优化算法</a>。对于这个例子，我们将使用最大梯度下降，步长为0.5，来下降交叉熵。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(<span class="number">0.5</span>).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<p>TensorFlow在这一行中实际上是在计算图中添加新的操作。这些操作包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</p>
<p>返回的<code>train_step</code>操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行<code>train_step</code>来完成。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="symbol">_</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</div><div class="line">  <span class="built_in">batch</span> = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">  train_step.run(feed_dict=&#123;x: <span class="built_in">batch</span>[<span class="number">0</span>], y_: <span class="built_in">batch</span>[<span class="number">1</span>]&#125;)</div></pre></td></tr></table></figure>
<p>每次训练迭代我们都会加入100个训练样本。然后，然后执行一次<code>train_step</code>操作，并通过<code>feed_dict</code>将<code>placeholder</code>tensor<code>x</code>和<code>y_</code>，用训练训练数据替代。请注意，您可以使用<code>feed_dict</code>替换计算图形中的任何tensor。–它不仅仅局限于<code>placeholder</code>。</p>
<h3 id="评估模型">评估模型</h3><p>那么我们的模型表现如何呢？</p>
<p>首先，来让我们找出那些预测正确的标签。<code>tf.argmax</code>是一个很有用的方法，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。例如，<code>tf.argmax(y，1)</code>是我们的模型认为每个输入最可能的标签，而<code>tf.argmax(y_，1)</code>是正确的标签。我们可以用<code>tf.equal</code>来检查我们我预测值与真实值是否相符。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(<span class="keyword">y</span>,<span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_,<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code>会变成<code>[1,0,1,1]</code>，取平均值后得到<code>0.75</code>.</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</div></pre></td></tr></table></figure>
<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>
<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(accuracy.eval(feed_dict=&#123;<span class="attribute">x</span>: mnist<span class="variable">.test</span><span class="variable">.images</span>, y_: mnist<span class="variable">.test</span><span class="variable">.labels</span>&#125;))</div></pre></td></tr></table></figure>
<h2 id="构建多层卷积网络">构建多层卷积网络</h2><p>在MNIST数据集上获得92%的准确率是相当差的。甚至差到令人感到尴尬的地步。在本节中，我们将解决这个问题。我们将从一个非常简单的模型跳转到一个中等复杂的模型：一个小型的卷积神经网络。这将会使我们得到一个大概在99.2%的准确率。–虽然不是最好的结果，但还算是令人满意的一个结果。</p>
<h3 id="权值初始化">权值初始化</h3><p>要创建这个模型，我们需要创建很多权值和偏置量。通常情况下，应该使用少量噪音数据来初始化权值以用于打破对称性，并且防止0梯度产生。由于我们使用的是<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="external">ReLU</a>)神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个用于初始化的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div></pre></td></tr></table></figure>
<h3 id="卷积和池化">卷积和池化</h3><p>TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做最大池（max pooling）。为了使代码更简洁，我们把这部分抽象成一个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div></pre></td></tr></table></figure>
<h3 id="第一层卷积">第一层卷积</h3><p>现在，我们可以实现我们的第一层了。它由一个卷积接一个最大池组成。卷积在每个5x5的patch中算出32个特征。卷积的权重tensor形状是<code>[5, 5, 1, 32]</code>。前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</div><div class="line">b_conv1 = bias_variable([<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>为了用这一层，我们把<code>x</code>变成一个4维tensor，其第<code>2</code>、第<code>3</code>维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</div></pre></td></tr></table></figure>
<p>然后我们将<code>x_image</code>与权值tensor进行卷积，加上偏置量，然后应用ReLU激励函数，最后最大池化。<code>max_pool_2x2</code>方法可将图片大小缩小为14x14。</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div></pre></td></tr></table></figure>
<h3 id="第二层卷积">第二层卷积</h3><p>为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W_conv2</span> = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line"><span class="attr">b_conv2</span> = bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line"><span class="attr">h_conv2</span> = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line"><span class="attr">h_pool2</span> = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<h3 id="密集连接层">密集连接层</h3><p>现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的tensor reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div></pre></td></tr></table></figure>
<h3 id="Dropout">Dropout</h3><p>为了减少过拟合，我们在输出层之前加入<a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="external">dropout</a>。我们用一个<code>placeholder</code>来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的<code>tf.nn.dropout</code>操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。</p>
<blockquote>
<p>对于这个小型卷积网络，性能实际上几乎相同，没有压差。Dropout往往是非常有效的减少过度拟合的方式，但当训练非常大的神经网络时，它是最有用的。</p>
</blockquote>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">keep_prob</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">h_fc1_drop</span> = tf.nn.dropout(h_fc1, keep_prob)</div></pre></td></tr></table></figure>
<h3 id="读出层">读出层</h3><p>最后，我们添加一个softmax层，就像前面的单层softmax 回归一样。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W_fc2</span> = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line"><span class="attr">b_fc2</span> = bias_variable([<span class="number">10</span>])</div><div class="line"></div><div class="line"><span class="attr">y_conv</span> = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</div></pre></td></tr></table></figure>
<h3 id="训练和评估模型">训练和评估模型</h3><p>这个模型的效果如何呢？为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码。</p>
<p>不过有以下几点不同：</p>
<ul>
<li>我们将用更复杂的ADAM优化器来替换最陡的梯度下降优化器。</li>
<li>我们将在<code>feed_dict</code>中包含附加参数<code>keep_prob</code>来控制丢失率。</li>
<li>我们将在训练过程中的每执行100次迭代时，添加一次日志记录。</li>
</ul>
<p>随时可以继续运行此代码，但它会进行20,000次训练迭代，可能需要一段时间（可能长达半小时），具体时间取决于您的处理器。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">cross_entropy = <span class="keyword">tf</span>.reduce_mean(</div><div class="line">    <span class="keyword">tf</span>.<span class="keyword">nn</span>.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</div><div class="line">train_step = <span class="keyword">tf</span>.train.AdamOptimizer(<span class="number">1</span><span class="keyword">e</span>-<span class="number">4</span>).minimize(cross_entropy)</div><div class="line">correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(y_conv,<span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_,<span class="number">1</span>))</div><div class="line">accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</div><div class="line">sess.run(<span class="keyword">tf</span>.global_variables_initializer())</div><div class="line"><span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="number">20000</span>):</div><div class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    train_accuracy = accuracy.<span class="built_in">eval</span>(feed_dict=&#123;</div><div class="line">        <span class="keyword">x</span>:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_pro<span class="variable">b:</span> <span class="number">1.0</span>&#125;)</div><div class="line">    <span class="keyword">print</span>(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy))</div><div class="line">  train_step.run(feed_dict=&#123;<span class="keyword">x</span>: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_pro<span class="variable">b:</span> <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line"><span class="keyword">print</span>(<span class="string">"test accuracy %g"</span>%accuracy.<span class="built_in">eval</span>(feed_dict=&#123;</div><div class="line">    <span class="keyword">x</span>: mnist.test.images, y_: mnist.test.labels, keep_pro<span class="variable">b:</span> <span class="number">1.0</span>&#125;))</div></pre></td></tr></table></figure>
<p>以上代码，在最终测试集上的准确率大概是99.2%。</p>
<p>目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/22/【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别/" itemprop="url">
                【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-22T19:50:58+08:00" content="2017-02-22">
            2017-02-22
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/22/【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/22/【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>本教程面向那些不熟悉<strong>机器学习</strong>和<strong>TensorFlow</strong>的读者。如果你已经知道MNIST是什么，softmax（多项Logistic）回归是什么，你可能更喜欢这个<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">更快节奏的教程</a>。在开始教程之前，请确认<a href="https://www.tensorflow.org/install/index" target="_blank" rel="external">安装TensorFlow</a>。</p>
<p>当一个人开始学习如何编程时，有一个传统，就是编写的第一个程序是能够打印”Hello World.”的程序。正如编程中的”Hello World”一样，机器学习中有MNIST。</p>
<p>MNIST是一个简单的计算机视觉数据集。它由像以下这样的手写数字的图像组成：</p>
<p><img src="/img/17_02_22/001.png" alt=""></p>
<p>它还包括每个图像的标签，用于标识是哪个数字。例如，上述图像的标签是<code>5</code>,<code>0</code>,<code>4</code>和<code>1</code>。</p>
<p>在本教程中，我们将训练一个模型，用来查看图像并预测它们是什么数字。我们的目标不是训练一个真正精准的，拥有高性能的模型，而是浅尝辄止的来体验一下TensorFlow的使用。 - 尽管我们稍后会给出实现这种效果的代码。因此，我们将从一个非常简单的，称为<strong>Softmax回归</strong>的模型开始。</p>
<p>这个教程的实际代码非常短，其中真正有趣的东西只有三行代码。然而，了解背后的想法是非常重要的：TensorFlow如何工作和核心机器学习概念。因此，我们将非常仔细地完成这部分代码。</p>
<h2 id="关于本教程">关于本教程</h2><p>本教程是对<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py" target="_blank" rel="external">mnist_softmax.py</a>中的代码进行逐行解释。</p>
<p>您可以通过以下几种不同的方式使用本教程：</p>
<ul>
<li>在阅读每行的解释时，将每个代码段逐行复制并粘贴到Python环境中。</li>
<li>在阅读教程期间，运行整个mnist_softmax.py，并使用本教程来了解您不清楚的代码行。</li>
</ul>
<p>我们将在本教程中完成：</p>
<ul>
<li>了解MNIST数据和softmax回归。</li>
<li>创建一个函数，它是一个用于识别数字的模型，其识别原理是基于查看图像中的每个像素的值来实现的。</li>
<li>使用TensorFlow来训练模型以识别数字，其训练方式是“查看”数千个示例（运行我们的第一个TensorFlow会话来执行此逻辑）。</li>
<li>使用我们的测试数据检查模型的精度。</li>
</ul>
<h2 id="MNIST数据集">MNIST数据集</h2><p>MNIST数据集托管在<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">Yann LeCun的站点</a>。如果您要复制粘贴本教程中的代码，请从这两行代码开始，这两行代码将自动下载并读入数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from tensorflow.examples.tutorials.mnist import input_data</div><div class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</div></pre></td></tr></table></figure>
<p>MNIST数据被分为三部分：55,000个训练数据（<code>mnist.train</code>），10,000个测试数据（<code>mnist.test</code>）和5,000个验证数据（<code>mnist.validation</code>）。这种切分是非常重要的：它能通过一部分我们并没有实际用来训练学习的数据，来确保我们的算法有很好的通用性。</p>
<p>如前所述，每个MNIST数据点有两个部分：手写数字的图像和相应的标签。我们称为图像”x”和标签”y”。训练集和测试集都包含图像及其相应的标签;例如训练图像是<code>mnist.train.images</code>，训练标签是<code>mnist.train.labels</code>。</p>
<p>每张图像的尺寸是28×28像素。我们可以把它解释为一个大的数组：</p>
<p><img src="/img/17_02_22/002.png" alt=""></p>
<p>我们可以将这个数组变成一个长度为28x28 = 784的向量。如何平铺数组其实并不重要，重要的是要保证图像和数组之间的一致性。从这个角度来看，MNIST图像只是784维向量空间中的一堆点，具有<a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" target="_blank" rel="external">非常丰富的结构</a>（警告：计算密集的可视化）。</p>
<p>展平数据丢弃了关于图像的2D结构的信息。这样做是不是并不够好？没错，最好的计算机视觉方法确实可以利用这种2D结构信息，我们将在后面的教程进行介绍。但是我们在这里所使用的一种简单方法：<strong>softmax回归</strong>（下面会给出定义），不会利用到这种信息。</p>
<p><code>mnist.train.images</code>是形状为<code>[55000,784]</code>的张量（n维数组）。第一个维度是在列表中图像的索引，第二个维度是每个图像中的每个像素点的索引。对于特定图像中的特定像素，张量中的每个条目是介于0和1之间的像素强度。</p>
<p><img src="/img/17_02_22/003.png" alt=""></p>
<p>MNIST中的每个图像都有相应的标签，标签用介于0到9之间的数字表示图像中绘制的数字。</p>
<p>为了达到本教程的目的，我们需要要将我们的标签作为“one-hot 向量”。one-hot向量是指在大多数维度上数值为0，仅在其中一个维度上数值为1的向量。在这种情况下，第n个数字将被表示为在第n维中为1的向量。例如，3将表示为$[0,0,0,1,0,0,0,0,0,0]$。因此，<code>mnist.train.labels</code>是一个形状为<code>[55000, 10]</code>的数字矩阵。</p>
<p><img src="/img/17_02_22/004.png" alt=""></p>
<p>现在，我们可以开始构建我们的模型啦！</p>
<h2 id="Softmax回归">Softmax回归</h2><p>我们知道MNIST中的每个图像都是一个在0和9之间的手写数字。因此，对于给定的图像，只有10种可能的结果。我们想要能够看到一个图像，并给出它的每个数字的概率。例如，用我们的模型来查看一个9的图片，80％的可能性确认是9，但有5％的可能是8（因为8和9顶部都有一个圈），剩余的可能性分布在其他数值上。</p>
<p>这是一个<strong>softmax回归</strong>的典型案例。如果你想给一个对象赋予其表示不同数字的概率，可以使用softmax，因为softmax可以得出一组介于0到1之间的值，并且这组值加起来结果为1。即使在以后，当我们训练其他更复杂的模型时，最后一步也是一层softmax。</p>
<p>softmax回归有两个步骤：首先我们将图片中属于某个特定数字的证据（evidence）相加，然后将该证据转换为概率。</p>
<p>为了计算给定图像在特定类中的证据，我们对像素强度进行加权求和。如果像素点有很高的强度表示和对应的标签数字不匹配，那么这一点的权值是负数，相反，权值是正数。</p>
<p>下面的图片显示了一个模型学习到的图片上每个像素对于特定数字类的权值。红色表示负权重，蓝色表示正权重。</p>
<p><img src="/img/17_02_22/005.png" alt=""></p>
<p>我们还需要增加一个偏置量（bias），因为输入往往会带有一些无关的干扰量。因此对于给定的输入图片<strong>x</strong>它代表的是数字<strong>i</strong>的证据可以表示为：</p>
<p>$$<br>\text{evidence}_i = \sum_j W_{i,~ j} x_j + b_i<br>$$</p>
<p>其中，$W_i$表示权值，$b_i$代表$i$类别的偏置量，$j$代表给定图片$x$的像素索引，用于像素求和。然后用softmax函数可以把这些证据转换成概率<strong>y</strong>：</p>
<p>$$<br>y = \text{softmax}(\text{evidence})<br>$$</p>
<p>这里softmax用作“激活”或“链接”函数，将我们的线性函数的输出变形为我们想要的形式 - 在这里，也就是10种数字的概率分布。你可以把它看作是将证据转换为每种分类的概率。它的定义是：</p>
<p>$$<br>\text{softmax}(x) = \text{normalize}(\exp(x))<br>$$</p>
<p>如果你把这个方程展开，你将得到：</p>
<p>$$<br>\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}<br>$$</p>
<p>但通常我们把softmax定义为第一种形式：对其输入求幂，然后将其归一化处理。这里幂运算表示，更大的证据对应更大的假设模型（hypothesis）里面的乘数权重值。反之，拥有更少的证据意味着在假设模型里面拥有更小的乘数系数。假设模型里的权值不可以是0值或者负值。Softmax然后会正则化这些权重值，使它们的总和等于1，以此构造一个有效的概率分布。（更多的关于Softmax函数的信息，可以参考Michael Nieslen的书里面的这个<a href="http://neuralnetworksanddeeplearning.com/chap3.html#softmax" target="_blank" rel="external">部分</a>，其中有关于softmax的可交互式的可视化解释。）</p>
<p>softmax回归可以表示为下面这张图，不过真实情况下会有更多的$x$值。我们通过计算出$x$的权值之和加上一个偏置量，然后代入到一个softmax中，来计算出每个输出值。</p>
<p><img src="/img/17_02_22/006.png" alt=""></p>
<p>如果我们把它写成方程的形式，我们将得到：</p>
<p><img src="/img/17_02_22/007.png" alt=""></p>
<p>我们可以“向量化”这个过程，把它变成矩阵乘法和向量加法。这有助于提升计算效率。 （这也是一个有用的思考方式。）</p>
<p><img src="/img/17_02_22/008.png" alt=""></p>
<p>更紧凑的表达形式如下：</p>
<p>$$<br>y = \text{softmax}(Wx + b)<br>$$</p>
<p>现在让我们把它变成TensorFlow可以使用的形式。</p>
<h2 id="回归的实现">回归的实现</h2><p>为了在Python中进行高效的数值计算，我们通常使用像<strong><a href="http://www.numpy.org/" target="_blank" rel="external">NumPy</a></strong>这样的库，它们会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果要在GPU上以分布式方式运行计算，那么这种开销尤其糟糕，其中传输数据的成本很高。</p>
<p>TensorFlow也在Python之外做了很大量的计算工作，但它做了进一步的完善以改善前面说的那种切换。TensorFlow不是独立于Python运行一个昂贵的操作，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行。（这样类似的运行方式，可以在不少的机器学习库中看到。）</p>
<p>要使用TensorFlow，首先我们需要导入它。</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<p>我们通过操作符号变量来描述这些交互的操作单元。让我们用下面的方式创建一个：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">x</span> = tf.placeholder(tf.float32, [None, <span class="number">784</span>])</div></pre></td></tr></table></figure>
<p><code>x</code>不是一个特定的值。它是一个占位符(<code>placeholder</code>)，当我们要求TensorFlow运行一个计算时，我们将输入一个值。我们希望能够输入任意数量的MNIST图像，其中每个图像都被展开为784维向量。我们将其表示为float类型的2-D张量，形状为<code>[None, 784]</code>。（这里的<code>None</code>表示维度可以是任何长度。）</p>
<p>我们的模型还需要权重和偏差。当然我们可以把它们当做是另外的输入（使用占位符），但TensorFlow有一个更好的方法来表示它们：<code>Variable</code>。<code>Variable</code>代表一个可修改的张量，它存在于TensorFlow中用于描述交互性操作的图中。在计算过程中，它们可以被拿来使用甚至可以修改。对于机器学习应用，一般都会有模型参数，可以用Variable表示。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W</span> = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</div><div class="line"><span class="attr">b</span> = tf.Variable(tf.zeros([<span class="number">10</span>]))</div></pre></td></tr></table></figure>
<p>我们通过给与<code>tf.Variable</code>初始值来创建<code>Variable</code>:在这种情况下，我们将<code>W</code>和<code>b</code>初始化为全部为0的张量。因为我们要通过学习得到<code>W</code>和<code>b</code>，因此它们的初始值具体是什么并不重要。</p>
<p>注意，<code>W</code>的形状为<code>[784,10]</code>，因为我们想要用784维的图片向量乘以它以得到一个10维的证据值向量，其中每一位对应着不同数字类别。<code>b</code>的形状是<code>[10]</code>，所以我们可以直接把它加到输出上面。</p>
<p>现在，我们可以实现我们的模型啦。只需要一行代码！</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax</span>(tf.matmul(x, W) + b)</div></pre></td></tr></table></figure>
<p>首先，我们通过表达式<code>tf.matmul(x, W)</code>将<code>x</code>和<code>W</code>相乘。这对应于前面方程中的$Wx$，<code>x</code>是一个拥有多个输入的2D张量。紧接着，我们加上<code>b</code>，最后，代入到<code>tf.nn.softmax</code>中。</p>
<p>就是这样，在几行用来设置变量的代码之后，我们只需要一行代码就可以定义好我们的模型。这不仅仅是因为TensorFlow被设计为使<strong>softmax回归</strong>变得特别简单，它也用这种非常灵活的方式来描述其他各种数值计算，从机器学习模型对物理学模拟仿真模型。一旦被定义好之后，我们的模型就可以在不同的设备上运行：计算机的CPU，GPU，甚至是手机！</p>
<h2 id="训练">训练</h2><p>为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好的。实际上，在机器学习中，我们通常定义指标来表示一个模型是坏的，这个指标称为成本（cost）或损失（loss），然后尽量最小化这个指标。</p>
<p>一个非常常见的，非常好的用来衡量模型损失的函数称为“<strong>交叉熵(cross-entropy)</strong>”。交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：</p>
<p>$$<br>H_{y’}(y) = -\sum_i y’_i \log(y_i)<br>$$</p>
<p><strong>y</strong>是我们预测的概率分布,<strong>y’</strong>是实际的分布（我们输入的one-hot vector)。比较粗糙的理解是，交叉熵是用来衡量相对于真实值我们所给出的预测的低效性。有关交叉熵的更详细的讨论超出了本教程的范畴，但<a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="external">理解它的原理</a>很有必要。</p>
<p>为了计算交叉熵，我们首先需要添加一个新的占位符用于输入正确值：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">y_</span> = tf.placeholder(tf.float32, [None, <span class="number">10</span>])</div></pre></td></tr></table></figure>
<p>然后，我们可以实现交叉熵方法:$-\sum y’\log(y)$</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cross_entropy = <span class="keyword">tf</span>.reduce_mean(-<span class="keyword">tf</span>.reduce_sum(y_ * <span class="keyword">tf</span>.<span class="built_in">log</span>(<span class="keyword">y</span>), reduction_indices=[<span class="number">1</span>]))</div></pre></td></tr></table></figure>
<p>首先，<code>tf.log</code>计算了每个<code>y</code>的对数。接下来，我们将<code>y_</code>与相应的<code>tf.log(y)</code>的元素做乘法运算。然后，由于参数<code>reduction_indices=[1]</code>，<code>tf.reduce_sum</code>将<code>y</code>中的第二维中的元素相加求和。最后，通过<code>tf.reduce_mean</code>计算批次中所有示例的平均值。</p>
<p>注意，在源码中，我们不使用这些信息，因为它在数值上并不稳定。取而代之的是，我们将<code>tf.nn.softmax_cross_entropy_with_logits</code>用于非规范化的逻辑上（例如，我们对<code>tf.matmul(x, W) + b</code>使用<code>softmax_cross_entropy_with_logits</code>），因为这样在数值上更稳定方法，它在内部执行了softmax的计算。在你的代码中考虑使用<code>tf.nn.softmax_cross_entropy_with_logits</code>来代替之前的逻辑。</p>
<p>现在，我们知道了我们想要我们的模型做什么，使用TensorFlow来训练它也非常简单。因为TensorFlow知道用于计算的整个图（graph），它会自动地使用<strong><a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="external">反向传播算法</a></strong>来有效地确定你的变量是如何影响你想要最小化的那个成本值的。然后它可以应用您选择的优化算法修改变量和减少损失。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(<span class="number">0.5</span>).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<p>在这里，我们通过使用学习率为0.5的<a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="external">梯度下降算法</a>令TensorFlow最小化<code>cross_entropy</code>（交叉熵）。梯度下降是一个简单的程序，它的原理是每次向着减少损失的方向移动一小步，来最小化代价函数。但TensorFlow也提供了<a href="https://www.tensorflow.org/api_guides/python/train#optimizers" target="_blank" rel="external">很多其他的优化算法</a>，只需要简单的调整一行代码就可以随意切换。</p>
<p>TensorFlow在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元，用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。</p>
<p>我们现在可以在<code>InteractiveSession</code>中启动我们的模型：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sess</span> = tf.InteractiveSession()</div></pre></td></tr></table></figure>
<p>我们首先要创建一个操作来初始化我们创建的变量：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">tf</span><span class="selector-class">.global_variables_initializer</span>()<span class="selector-class">.run</span>()</div></pre></td></tr></table></figure>
<p>让我们开始执行训练 - 我们将运行1000次训练步骤！</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  batch_xs, batch_ys = mnist<span class="selector-class">.train</span><span class="selector-class">.next_batch</span>(<span class="number">100</span>)</div><div class="line">  sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</div></pre></td></tr></table></figure>
<p>每循环一次，我们将从我们的训练集中得到一批100个随机数据点。然后我们用这些数据点作为参数替换之前的占位符来运行<code>train_step</code>。</p>
<p>使用小批随机数据称为<strong>随机训练(stochastic training)</strong> - 在这里更确切的说是随机梯度下降训练。理想情况下，我们希望将所有数据用于训练的每个步骤，因为这能给我们更好的训练结果，但很明显这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。</p>
<h2 id="评估我们的模型">评估我们的模型</h2><p>那么我们的模型表现如何呢？</p>
<p>首先，来让我们找出那些预测正确的标签。<code>tf.argmax</code>是一个很有用的方法，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。例如，<code>tf.argmax(y，1)</code>是我们的模型认为每个输入最可能的标签，而<code>tf.argmax(y_，1)</code>是正确的标签。我们可以用<code>tf.equal</code>来检查我们我预测值与真实值是否相符。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(<span class="keyword">y</span>,<span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_,<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code>会变成<code>[1,0,1,1]</code>，取平均值后得到<code>0.75</code>.</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</div></pre></td></tr></table></figure>
<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>
<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(sess.run(accuracy, feed_dict=&#123;<span class="attribute">x</span>: mnist<span class="variable">.test</span><span class="variable">.images</span>, y_: mnist<span class="variable">.test</span><span class="variable">.labels</span>&#125;))</div></pre></td></tr></table></figure>
<p>结果大概维持在92%左右。</p>
<p>这种结果很好吗？其实并不是很好。其实，它相当差。这是因为我们使用的是一个非常简单的模型。我们可以通过做一些简单的修改，可以将正确率提高到97%。事实上，最优秀的模型可以达到超过99.7%的准确率！（想了解更多信息，可以看看这个关于各种模型的<a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" target="_blank" rel="external">性能对比列表</a>。)</p>
<p>比结果更重要的是，我们从这个模型中学习到的设计思想。不过，如果你仍然对这里的结果有点失望，可以查看<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">下一个教程</a>，在那里你可以学习如何用FensorFlow构建更加复杂的模型以获得更好的性能！</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/20/【Tensorflow r1.0 文档翻译】TensorFlow入门/" itemprop="url">
                【Tensorflow r1.0 文档翻译】TensorFlow入门
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-20T21:42:58+08:00" content="2017-02-20">
            2017-02-20
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/20/【Tensorflow r1.0 文档翻译】TensorFlow入门/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/20/【Tensorflow r1.0 文档翻译】TensorFlow入门/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="TensorFlow入门">TensorFlow入门</h2><p>这是一个TensorFlow的入门指南。在你使用这份指南之前，请先<a href="https://www.tensorflow.org/install/" target="_blank" rel="external">安装TensorFlow</a>。在充分的使用本指南之前，您应该了解以下内容：</p>
<ul>
<li>如何使用Python进行编程。</li>
<li>至少对矩阵有一些了解</li>
<li>最好是对<strong>机器学习</strong>有一点了解。但即使你对<strong>机器学习</strong>有一点了解、或者甚至完全不了解，那么你很有必要读一读这一篇指南了。</li>
</ul>
<p>TensorFlow提供了多种API。即使是最低版本的TensorFlow 核心 API，也为您提供了完整的编程控制。如果您是机器学习研究人员，或需要对模型进行精细控制的人，那么我们建议你使用TensorFlow 核心代码，否则我们建议您使用TensorFlow Core API。这些更高级的API通常比TensorFlow 核心代码更容易学习和使用。此外，较高级别的API使重复性任务更容易上手，并且在不同用户之间更一致。高级API（如<strong>tf.contrib.learn</strong>）可帮助您管理数据集、估计量、训练和推断。注意，在一些高级TensorFlow API 中，方法名称包含<code>contrib</code>的API表示仍在开发中。一些<code>contrib</code>方法可能会在随后的TensorFlow版本中发生改变或过时。</p>
<p>本指南从TensorFlow 核心教程开始。稍后，我们将演示如何在<code>tf.contrib.learn</code>中实现相同的模型。了解TensorFlow核心原则将会给你提供一个很棒的心理模型，这个模型是用于说明当您使用更紧凑的更高级别的API时，内部是如何工作的。</p>
<h2 id="Tensors（张量）">Tensors（张量）</h2><p>TensorFlow中的数据的中心单元是<strong>Tensors(张量)</strong>。tensor是由一组原始数据组成，这些原始数据是由一组任意数量维度的数组形成。一个tensor的<strong>rank</strong>是表示它尺寸的一个数值。下面是几个tensor的例子：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">3</span> # a rank <span class="number">0</span> tensor; this is a scalar with shape []</div><div class="line">[<span class="number">1.</span> ,<span class="number">2.</span>, <span class="number">3.</span>] # a rank <span class="number">1</span> tensor; this is a <span class="type">vector</span> with shape [<span class="number">3</span>]</div><div class="line">[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]] # a rank <span class="number">2</span> tensor; a matrix with shape [<span class="number">2</span>, <span class="number">3</span>]</div><div class="line">[[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]], [[<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]]] # a rank <span class="number">3</span> tensor with shape [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure>
<h2 id="TensorFlow核心教程">TensorFlow核心教程</h2><h3 id="导入TensorFlow">导入TensorFlow</h3><p>TensorFlow程序的标准导入语句如下：</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<p>这样做可以使得Python能够正常访问TensorFlow的所有类、方法和符号。我们的大多数文档都假设你已经这样做了。</p>
<h3 id="用于计算的Graph（图）">用于计算的Graph（图）</h3><p>也许你会认为TensorFlow Core的程序包含下面两部分组成：</p>
<ul>
<li>1.构建<strong>computational graph（用于计算的图）</strong></li>
<li>2.运行<strong>computational graph（用于计算的图）</strong></li>
</ul>
<p>一个<strong>computational graph（用于计算的图）</strong>是一系列排列在graph的节点上的TensorFlow操作单元。让我们来构建一个简单的<strong>computational graph</strong>。每个节点接受0个或多个tensor作为输入，并且产生一个tensor作为输出。<strong>常量类型</strong>是节点的一种类型。正如所有的TensorFlow常量一样，它是不接收输入的，并且输出一个它内部存储的值。我们可以按照下面的方式来创建两个浮点Tensor节点<code>node1</code>和<code>node2</code>：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">node1 = <span class="keyword">tf</span>.constant(<span class="number">3.0</span>, <span class="keyword">tf</span>.float32)</div><div class="line">node2 = <span class="keyword">tf</span>.constant(<span class="number">4.0</span>) # also <span class="keyword">tf</span>.float32 implicitly</div><div class="line"><span class="keyword">print</span>(node1, node2)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Tensor(<span class="string">"Const:0"</span>, shape=(), dtype=float32) Tensor(<span class="string">"Const_1:0"</span>, shape=(), dtype=float32)</div></pre></td></tr></table></figure>
<p>你会注意到，打印节点并不会如你所想的输出<code>3.0</code>和<code>4.0</code>。相反，这些节点会在计算时分别产生<code>3.0</code>和<code>4.0</code>。为了实际评估这些节点，我们必须以一个 <strong>session（会话）</strong> 来运行 <strong>computational graph</strong>。<strong>session（会话）</strong> 封装了TensorFlow运行时的控件和状态。</p>
<p>下面的代码创建了一个<code>Session</code>对象，并且执行了它的<code>run</code>方法来运行包含了<code>node1</code>和<code>node2</code>的<strong>computational graph</strong>的计算结果。通过在<strong>session</strong>中运行<strong>computational graph</strong>的代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(sess.run([node1, node2])</span></span>)</div></pre></td></tr></table></figure>
<p>我们看到了我们期望看到的<code>3.0</code>和<code>4.0</code>的输出:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">3.0</span>, <span class="number">4.0</span>]</div></pre></td></tr></table></figure>
<p>我们可以通过将<code>Tensor</code>节点与操作节点（操作也是一种节点）组合起来的方式来构建更复杂的计算。例如，我们可以将两个常量节点执行加法操作，并且产生一个新的graph，代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">node3 = tf.add(node1, node2)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"node3: "</span>, node3)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"sess.run(node3): "</span>,sess.run(node3)</span></span>)</div></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node3:  Tensor(<span class="string">"Add_2:0"</span>, shape=(), dtype=float32)</div><div class="line">sess.run(node3):  7.0</div></pre></td></tr></table></figure>
<p>TensorFlow提供了一个叫做<strong>TensorBoard</strong>的很实用的程序，它可以将computational graph可视化的展示出来。下面是通过TensorBoard来可视化一个graph的效果：</p>
<p><img src="/img/17_02_20/001.png" alt=""></p>
<p>由于我们用到的是常量，因此这个图看起来并不是特别有趣，因为它总是产生一个恒定的结果。graph可以被参数化，并且通过<strong>placeholders（占位符）</strong>来接受外部的输入。<strong>placeholders（占位符）</strong>表示对稍后所提供的值的一个承诺。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attr">a</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">b</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">adder_node</span> = a + b  # + 是tf.add(a, b)的缩写形式</div></pre></td></tr></table></figure>
<p>上面三行的表达形式看起来有点像一个方法，或lambda表达式：其中我们定义两个输入参数（<code>a</code>和<code>b</code>），然后对它们执行一个操作。我们可以通过多个输入来计算这个graph的执行结果，其中我们的输入是通过<code>feed_dict</code>参数来指定对这些<strong>placeholders（占位符）</strong>提供具体值的<code>Tensors</code>的输入的：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">adder_node</span>, &#123;<span class="attribute">a</span>: <span class="number">3</span>, b:<span class="number">4.5</span>&#125;))</div><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">adder_node</span>, &#123;<span class="attribute">a</span>: [<span class="number">1</span>,<span class="number">3</span>], b: [<span class="number">2</span>, <span class="number">4</span>]&#125;))</div></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">7.5</span></div><div class="line">[ <span class="number">3.</span>  <span class="number">7.</span>]</div></pre></td></tr></table></figure>
<p>在TensorBoard中，graph看起来是这个样子：</p>
<p><img src="/img/17_02_20/002.png" alt=""></p>
<p>我们可以通过添加其他操作，来让我们的<strong>computational graph</strong>看起来更复杂。例如：</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">add_and_triple </span>= <span class="keyword">adder_node </span>* <span class="number">3</span>.</div><div class="line"><span class="symbol">print</span>(sess.run(<span class="keyword">add_and_triple, </span>&#123;a: <span class="number">3</span>, <span class="keyword">b:4.5&#125;))</span></div></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">22<span class="selector-class">.5</span></div></pre></td></tr></table></figure>
<p>上面的computational graph在TensorBoard中看起来是这样的：</p>
<p><img src="/img/17_02_20/003.png" alt=""></p>
<p>在机器学习中，我们通常需要一个可以接受任意输入的模型，例如上面的模型。为了使模型可训练，我们需要能够修改<code>graph</code>以获得具有相同输入的新输出。<strong>Variables（变量）</strong>允许我们向graph中添加可训练的参数。它们由一个类型和初始值组成：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W</span> = tf.Variable([.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="attr">b</span> = tf.Variable([-.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="attr">x</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">linear_model</span> = W * x + b</div></pre></td></tr></table></figure>
<p>当调用<code>tf.constant</code>时，常量被初始化，它们的值永远不会改变。相比之下，变量<code>tf.Variable</code>在调用时不会被初始化。要初始化TensorFlow程序中的所有变量，必须显式调用特殊的初始化操作，如下所示：</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">init</span> = tf.global_variables_initializer()</div><div class="line">sess.run(<span class="keyword">init</span>)</div></pre></td></tr></table></figure>
<p><code>init</code>是TensorFlow的sub-graph的一个重要的操作，它用于初始化所有的全局变量。在这里直到我们调用<code>sess.run</code>之前，变量是未初始化的。</p>
<p>由于<code>x</code>是一个占位符，因此我们可以同时计算<code>linear_model</code>几个值，如下所示：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">linear_model</span>, &#123;<span class="attribute">x</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]&#125;))</div></pre></td></tr></table></figure>
<p>产生如下输出：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[ <span class="number">0.</span>          <span class="number">0.30000001</span>  <span class="number">0.60000002</span>  <span class="number">0.90000004</span>]</div></pre></td></tr></table></figure>
<p>我们创建了一个模型，但目前我们还不知道它是好是坏。为了评估训练数据的模型，我们需要一个<strong>loss function（损失函数）</strong>，我们可以用一个<code>y</code>占位符来提供所需的值。</p>
<p>损失函数会计算出当前训练出的模型和所提供的数据之间的距离。我们将使用用于线性回归的标准损失模型，其原理是将当前模型和提供的数据之间的增量的平方求和。<code>linear_model - y</code>创建一个向量，其中每个元素是相应的样本的误差增量。我们称之为<code>tf.square</code>平方误差。然后，我们使用<code>tf.reduce_sum</code>将所有平方误差求和，以创建一个单一的标量，用于提取出表示所有样本的总误差值：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">y</span> = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32)</div><div class="line">squared_deltas = <span class="keyword">tf</span>.square(linear_model - <span class="keyword">y</span>)</div><div class="line">loss = <span class="keyword">tf</span>.reduce_sum(squared_deltas)</div><div class="line"><span class="keyword">print</span>(sess.run(loss, &#123;<span class="keyword">x</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], <span class="keyword">y</span>:[<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">3</span>]&#125;))</div></pre></td></tr></table></figure>
<p>输出损失值：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">23<span class="selector-class">.66</span></div></pre></td></tr></table></figure>
<p>我们可以通过手动的将<code>W</code>和<code>b</code>的值重新赋值为<code>-1</code>和<code>1</code>的方式来提高我们的算法的效果。变量可以初始化后将数据提供给<code>tf.Variable</code>对象，也可以使用像<code>tf.assign</code>这样的操作来更改。例如，<code>W=-1</code>和<code>b=1</code>是我们的模型中的最佳参数。因此我们可以更改<code>W</code>和<code>b</code>的值：</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fixW = tf.assign(W, <span class="string">[-1.]</span>)</div><div class="line">fixb = tf.assign(b, <span class="string">[1.]</span>)</div><div class="line">sess.run(<span class="string">[fixW, fixb]</span>)</div><div class="line">print(sess.run(loss, &#123;x:<span class="string">[1,2,3,4]</span>, y:<span class="string">[0,-1,-2,-3]</span>&#125;))</div></pre></td></tr></table></figure>
<p>最终输出结果的损失是0：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0<span class="selector-class">.0</span></div></pre></td></tr></table></figure>
<p>我们猜到了“最完美”的参数<code>W</code>和<code>b</code>的值，但机器学习的目标是<strong>自动</strong>找到正确的模型参数。我们将在下一节中说明如何完成这一任务。</p>
<h2 id="tf-train_API">tf.train API</h2><p>机器学习的完整讨论超出了本教程的范围。然而，TensorFlow提供了缓慢地改变每个变量以便<strong>最小化损失函数</strong>的<strong>optimizers（优化器）</strong>。其中最简单的优化器是<strong>gradient descent（梯度下降）</strong>。其原理是根据相对于该变量的损失导数的大小修改每个变量的值。一般来说，人工计算导数是繁琐的并且容易出错。因此，TensorFlow可以使用<code>tf.gradients</code>函数，来自动的产生当前所给模型描述的导数。为了简化操作，优化器通常会自动地为您执行此操作。例如：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">optimizer</span> = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line"><span class="attr">train</span> = optimizer.minimize(loss)</div></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">init</span>) # 将值重置为不正确的默认值</div><div class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(1000):</div><div class="line">  <span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">train</span>, &#123;<span class="attribute">x</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">3</span>]&#125;)</div><div class="line"></div><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-attr">[W, b]</span>))</div></pre></td></tr></table></figure>
<p>最终训练得出的模型参数：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[array([<span class="number">-0.9999969</span>], dtype=float32), array([ <span class="number">0.99999082</span>],</div><div class="line"> dtype=float32)]</div></pre></td></tr></table></figure>
<p>现在我们已经完成了实际的机器学习！完成这个简单的线性回归不需要太多的TensorFlow核心代码，但是更复杂的学习模型和方法通常需要更多的代码。因此，TensorFlow为通用模式、结构和功能提供了一套更高级别的抽象实现。我们将在下一节中学习如何使用这些抽象实现。</p>
<h3 id="完整的程序">完整的程序</h3><p>完成的可训练线性回归模型程序如下所示：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">import</span> numpy as np</div><div class="line"><span class="built_in">import</span> tensorflow as tf</div><div class="line"></div><div class="line"><span class="comment"># Model parameters</span></div><div class="line"><span class="attr">W</span> = tf.Variable([.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="attr">b</span> = tf.Variable([-.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="comment"># Model input and output</span></div><div class="line"><span class="attr">x</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">linear_model</span> = W * x + b</div><div class="line"><span class="attr">y</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="comment"># loss</span></div><div class="line"><span class="attr">loss</span> = tf.reduce_sum(tf.square(linear_model - y)) <span class="comment"># sum of the squares</span></div><div class="line"><span class="comment"># optimizer</span></div><div class="line"><span class="attr">optimizer</span> = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line"><span class="attr">train</span> = optimizer.minimize(loss)</div><div class="line"><span class="comment"># training data</span></div><div class="line"><span class="attr">x_train</span> = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div><div class="line"><span class="attr">y_train</span> = [<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">3</span>]</div><div class="line"><span class="comment"># training loop</span></div><div class="line"><span class="attr">init</span> = tf.global_variables_initializer()</div><div class="line"><span class="attr">sess</span> = tf.Session()</div><div class="line">sess.run(init) <span class="comment"># reset values to wrong</span></div><div class="line">for i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  sess.run(train, &#123;x:x_train, y:y_train&#125;)</div><div class="line"></div><div class="line"><span class="comment"># evaluate training accuracy</span></div><div class="line">curr_W, curr_b, <span class="attr">curr_loss</span>  = sess.run([W, b, loss], &#123;x:x_train, y:y_train&#125;)</div><div class="line">print(<span class="string">"W: %s b: %s loss: %s"</span>%(curr_W, curr_b, curr_loss))</div></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">W</span>: <span class="selector-attr">[-0.9999969]</span> <span class="selector-tag">b</span>: <span class="selector-attr">[ 0.99999082]</span> <span class="selector-tag">loss</span>: 5<span class="selector-class">.69997e-11</span></div></pre></td></tr></table></figure>
<p>这个更复杂的程序也可以在TensorBoard中可视化：</p>
<p><img src="/img/17_02_20/004.png" alt=""></p>
<h2 id="tf-contrib-learn">tf.contrib.learn</h2><p><code>tf.contrib.learn</code>是一个高级别的TensorFlow库，它简化了机器学习的机制，包括：</p>
<ul>
<li>运行训练循环</li>
<li>运行评估循环</li>
<li>管理数据集</li>
<li>管理数据导入</li>
</ul>
<p><code>tf.contrib.learn</code>定义了许多常见的模型。</p>
<h3 id="基本用法">基本用法</h3><p>请注意，线性回归在使用<code>tf.contrib.learn</code>的情况下变得更简单了：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">import</span> tensorflow as tf</div><div class="line"><span class="comment"># NumPy is often used to load, manipulate and preprocess data.</span></div><div class="line"><span class="built_in">import</span> numpy as np</div><div class="line"></div><div class="line"><span class="comment"># Declare list of features. We only have one real-valued feature. There are many</span></div><div class="line"><span class="comment"># other types of columns that are more complicated and useful.</span></div><div class="line"><span class="attr">features</span> = [tf.contrib.layers.real_valued_column(<span class="string">"x"</span>, <span class="attr">dimension=1)]</span></div><div class="line"></div><div class="line"><span class="comment"># An estimator is the front end to invoke training (fitting) and evaluation</span></div><div class="line"><span class="comment"># (inference). There are many predefined types like linear regression,</span></div><div class="line"><span class="comment"># logistic regression, linear classification, logistic classification, and</span></div><div class="line"><span class="comment"># many neural network classifiers and regressors. The following code</span></div><div class="line"><span class="comment"># provides an estimator that does linear regression.</span></div><div class="line"><span class="attr">estimator</span> = tf.contrib.learn.LinearRegressor(<span class="attr">feature_columns=features)</span></div><div class="line"></div><div class="line"><span class="comment"># TensorFlow provides many helper methods to read and set up data sets.</span></div><div class="line"><span class="comment"># Here we use `numpy_input_fn`. We have to tell the function how many batches</span></div><div class="line"><span class="comment"># of data (num_epochs) we want and how big each batch should be.</span></div><div class="line"><span class="attr">x</span> = np.array([<span class="number">1</span>., <span class="number">2</span>., <span class="number">3</span>., <span class="number">4</span>.])</div><div class="line"><span class="attr">y</span> = np.array([<span class="number">0</span>., -<span class="number">1</span>., -<span class="number">2</span>., -<span class="number">3</span>.])</div><div class="line"><span class="attr">input_fn</span> = tf.contrib.learn.io.numpy_input_fn(&#123;<span class="string">"x"</span>:x&#125;, y, <span class="attr">batch_size=4,</span></div><div class="line">                                              <span class="attr">num_epochs=1000)</span></div><div class="line"></div><div class="line"><span class="comment"># We can invoke 1000 training steps by invoking the `fit` method and passing the</span></div><div class="line"><span class="comment"># training data set.</span></div><div class="line">estimator.fit(<span class="attr">input_fn=input_fn,</span> <span class="attr">steps=1000)</span></div><div class="line"></div><div class="line"><span class="comment"># Here we evaluate how well our model did. In a real example, we would want</span></div><div class="line"><span class="comment"># to use a separate validation and testing data set to avoid overfitting.</span></div><div class="line">estimator.evaluate(<span class="attr">input_fn=input_fn)</span></div></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'global_step'</span>: <span class="number">1000</span>, <span class="string">'loss'</span>: <span class="number">1.9650059</span>e-<span class="number">11</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="定制模型">定制模型</h3><p><code>tf.contrib.learn</code>不会将你锁定在预定义模型中。假设我们想创建一个未内置到TensorFlow中的自定义模型。我们仍然可以保留<code>tf.contrib.learn</code>的数据集、馈送、训练等的高级抽象。为了说明，我们将演示如何使用我们的较低级别TensorFlow API的知识来实现​​我们自己的等效模型到<code>LinearRegressor</code>。</p>
<p>要定义与<code>tf.contrib.learn</code>一起使用的自定义模型，我们需要使用<code>tf.contrib.learn.Estimator</code>。 <code>tf.contrib.learn.LinearRegressor</code>实际上是<code>tf.contrib.learn.Estimator</code>的子类。替代子类<code>Estimator</code>，我们只是提供<code>Estimator</code>一个<code>model_fn</code>函数，用于告诉<code>tf.contrib.learn</code>如何评估预测、训练步骤和损失。代码如下：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">import</span> numpy as np</div><div class="line"><span class="built_in">import</span> tensorflow as tf</div><div class="line"><span class="comment"># Declare list of features, we only have one real-valued feature</span></div><div class="line">def model(features, labels, mode):</div><div class="line">  <span class="comment"># Build a linear model and predict values</span></div><div class="line">  <span class="attr">W</span> = tf.get_variable(<span class="string">"W"</span>, [<span class="number">1</span>], <span class="attr">dtype=tf.float64)</span></div><div class="line">  <span class="attr">b</span> = tf.get_variable(<span class="string">"b"</span>, [<span class="number">1</span>], <span class="attr">dtype=tf.float64)</span></div><div class="line">  <span class="attr">y</span> = W*features['x'] + b</div><div class="line">  <span class="comment"># Loss sub-graph</span></div><div class="line">  <span class="attr">loss</span> = tf.reduce_sum(tf.square(y - labels))</div><div class="line">  <span class="comment"># Training sub-graph</span></div><div class="line">  <span class="attr">global_step</span> = tf.train.get_global_step()</div><div class="line">  <span class="attr">optimizer</span> = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line">  <span class="attr">train</span> = tf.group(optimizer.minimize(loss),</div><div class="line">                   tf.assign_add(global_step, <span class="number">1</span>))</div><div class="line">  <span class="comment"># ModelFnOps connects subgraphs we built to the</span></div><div class="line">  <span class="comment"># appropriate functionality.</span></div><div class="line">  return tf.contrib.learn.ModelFnOps(</div><div class="line">      <span class="attr">mode=mode,</span> <span class="attr">predictions=y,</span></div><div class="line">      <span class="attr">loss=</span> loss,</div><div class="line">      <span class="attr">train_op=train)</span></div><div class="line"></div><div class="line"><span class="attr">estimator</span> = tf.contrib.learn.Estimator(<span class="attr">model_fn=model)</span></div><div class="line"><span class="comment"># define our data set</span></div><div class="line"><span class="attr">x=np.array([1.,</span> <span class="number">2</span>., <span class="number">3</span>., <span class="number">4</span>.])</div><div class="line"><span class="attr">y=np.array([0.,</span> -<span class="number">1</span>., -<span class="number">2</span>., -<span class="number">3</span>.])</div><div class="line"><span class="attr">input_fn</span> = tf.contrib.learn.io.numpy_input_fn(&#123;<span class="string">"x"</span>: x&#125;, y, <span class="number">4</span>, <span class="attr">num_epochs=1000)</span></div><div class="line"></div><div class="line"><span class="comment"># train</span></div><div class="line">estimator.fit(<span class="attr">input_fn=input_fn,</span> <span class="attr">steps=1000)</span></div><div class="line"><span class="comment"># evaluate our model</span></div><div class="line">print(estimator.evaluate(<span class="attr">input_fn=input_fn,</span> <span class="attr">steps=10))</span></div></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'loss'</span>: <span class="number">5.9819476</span>e-<span class="number">11</span>, <span class="string">'global_step'</span>: <span class="number">1000</span>&#125;</div></pre></td></tr></table></figure>
<p>注意，自定义<code>model()</code>函数的内容与低版本API的手册中的模型训练循环非常相似。</p>
<h2 id="下一步">下一步</h2><p>现在你了解到了TensorFlow的基本运作的知识。我们还有几个教程，您可以查看以了解更多。如果你是机器学习的初学者，请参阅<a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">【深度学习的HelloWorld – MNIST手写数字识别】</a>，否则请参阅<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">【深入MNIST – 专家级】</a>。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/20/【Tensorflow r1.0 文档翻译】入门教程/" itemprop="url">
                【Tensorflow r1.0 文档翻译】入门教程
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-20T21:19:58+08:00" content="2017-02-20">
            2017-02-20
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/20/【Tensorflow r1.0 文档翻译】入门教程/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/20/【Tensorflow r1.0 文档翻译】入门教程/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><ul>
<li><a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门/">【TensorFlow入门】</a></li>
<li><a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">【机器学习的HelloWorld – MNIST手写数字识别】</a></li>
<li><a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">【深入MNIST – 专家级】</a></li>
<li><a href="/2017/03/03/【Tensorflow%20r1.0%20文档翻译】Tensorflow原理导论/">【TensorFlow原理导论】</a></li>
<li><a href="">【tf.contrib.learn快速入门】</a></li>
<li><a href="">【通过tf.contrib.learn来构建输入方法】</a></li>
<li><a href="">【TensorBoard:可视化学习】</a></li>
<li><a href="">【TensorBoard:嵌入可视化】</a></li>
<li><a href="">【TensorBoard:图的可视化】</a></li>
<li><p><a href="">【使用tf.contrib.learn记录和监视的基本知识】</a></p>
</li>
<li><p><a href="">【TensorFlow 版本】</a></p>
</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/09/斯坦福机器学习课程 第七周 (2)核函数/" itemprop="url">
                斯坦福机器学习课程 第七周 (2)核函数
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-09T21:07:58+08:00" content="2017-02-09">
            2017-02-09
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/09/斯坦福机器学习课程 第七周 (2)核函数/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/09/斯坦福机器学习课程 第七周 (2)核函数/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="核函数_I">核函数 I</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/YOMHn/kernels-i" target="_blank" rel="external">视频地址</a></p>
<p>在本节，我将对支持向量机算法做一些改变，以构造复杂的非线性分类器。我们用<strong>“kernels(核函数)”</strong>来达到此目的。</p>
<p>我们来看看<strong>核函数</strong>是什么，以及如何使用。</p>
<p>如果你有一个像这个样的训练集：</p>
<p><img src="/img/17_02_09/001.png" alt=""></p>
<p>然后你希望拟合一个非线性的判别边界来区别正负样本，那么你的判别边界可能是这样的：</p>
<p><img src="/img/17_02_09/002.png" alt=""></p>
<p>当我们这么做的时候，其实这个决策边界是由类似于下面这种多项式构成的：</p>
<ul>
<li><p>如果$<br>\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1x_2+\theta_4x_1^2 + \theta_5x_2^2 + … \ge 0<br>$，则预测$h_{\theta}(x)=1$；</p>
</li>
<li><p>如果$<br>\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1x_2+\theta_4x_1^2 + \theta_5x_2^2 + … \lt 0<br>$，则预测$h_{\theta}(x)=0$；</p>
</li>
</ul>
<p>如果我们把假设函数改写成下面这种形式：</p>
<p>$$<br>\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\theta_4f_4+\theta_5f_5+…<br>$$</p>
<p>因此有：</p>
<p>$$<br>f_1=x_1<br>$$</p>
<p>$$<br>f_2=x_2<br>$$</p>
<p>$$<br>f_3=x_1x_2<br>$$</p>
<p>$$<br>f_4=x_1^2<br>$$</p>
<p>$$<br>f_5=x_2^2<br>$$</p>
<p>$$<br>…<br>$$</p>
<p>以此类推，可以依次加入这些高阶项，但我们其实并不知道这些高阶项是不是我们真正需要的。我们之前谈到计算机视觉的时候，提到过在这里的输入是一个有很多像素的图像，我们看到如果用高阶项作为特征变量，运算量将是非常大的，因为有太多的高阶项需要被计算。</p>
<p>因此，我们是否有不同的选择，或者是更好的选择来构造特征变量，以用来嵌入到假设函数中呢？</p>
<h3 id="用核函数构造新特征">用核函数构造新特征</h3><p>事实上，这里有一个可以构造新特征$f_1$、$f_2$、$f_3$的方法。</p>
<p>首先我们定义三个特征变量(但是对于实际问题而言，我们可以定义非常多的特征变量）：</p>
<p><img src="/img/17_02_09/003.png" width="300" height="200" align="center"></p>
<p>将这三个点标记为$l^{(1)}$、$l^{(2)}$、$l^{(3)}$，接下来我要做的是定义新的特征变量：</p>
<p>$$<br>f_1=similarity(x,l^{(1)})<br>$$</p>
<p>这里$similarite(x,l^{(1)})$是一种相似度的度量，度量样本$x$与第一个标记$l^{(1)}$的相似度。</p>
<p>这个度量相似度的公司是这样的：</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=similarity(x,l^{(1)})<br>\\<br>&amp;=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<blockquote>
<p>$exp$是自然常数$e$为底的指数函数。</p>
</blockquote>
<p>不知道你之前是否看了上一个选修课程的视频，$||w||$是表示向量$w$的长度。因此这里的$||x-l^{(1)}||$的意思就是就是向量的欧式距离。</p>
<p>因此，我们可以依次写出$f_1$、$f_2$、$f_3$:</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=similarity(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_2&amp;=similarity(x,l^{(2)})<br>=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_3&amp;=similarity(x,l^{(3)})<br>=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>这里的$similarite(x,l)$函数，就被称为<strong>核函数(Kernels)</strong>。在这里，我们的例子中所说的<strong>核函数</strong>，实际上是<strong>高斯核函数</strong>，在后面我们还会见到不同的<strong>核函数</strong>。</p>
<p>核函数我们通常不写作$similarity(x,l^{(i)})$，而是写作：</p>
<p>$$<br>k(x,l^{(i)})<br>$$</p>
<h3 id="核函数可以做什么？">核函数可以做什么？</h3><p>我们来看看核函数到底可以做什么？</p>
<p>首先让我们来看看第一个标记：</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=similarity(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>=exp(-\frac{\sum_{j=1}^{n}(x_j-l_j^{(1)})^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$l^{(1)}$是我之前在图中选取的几个点之中的一个，上面是$x$和$l^{(1)}$之间的核函数。</p>
<p>其中$||x-l^{(1)}||^2$这一项可以表示成各个$x$向量到$l$向量的距离求和的形式：$\sum_{j=1}^{n}(x_j-l_j^{(1)})^2$。（这里我们依然忽略了截距的影响，即令$x_0=1$）。</p>
<p>假设，如果$x\approx l^{(1)}$，即$x$与其中一个标记点非常接近，那么这个欧氏距离$||x-l^{(1)}||$就会接近0，因此：</p>
<p>$$<br>f_1<br>\approx<br>exp(-\frac{0^2}{2σ^2})<br>\approx1<br>$$</p>
<p>相反的，如果$x$离$l^{(1)}$很远，那么会有：</p>
<p>$$<br>f_1<br>\approx<br>exp(-\frac{(large\ number)^2}{2σ^2})<br>\approx0<br>$$</p>
<p><strong>这些特征变量的作用是度量$x$到标记$l^{(1)}$的相似度的，并且如果$x$离$l$非常接近，那么特征变量$f$就接近1；如果$x$离标记$l^{(1)}$非常远，那么特征变量$f$就接近于0。</strong></p>
<p>之前我绘制的三个标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$每一个标记点会定义一个新的特征变量：</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=k(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_2&amp;=k(x,l^{(2)})<br>=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_3&amp;=k(x,l^{(3)})<br>=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p><strong>也就是说，给出一个训练样本$x$，我们就能基于我们之前给出的标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$来计算出三个新的特征变量$f_1$、$f_2$、$f_3$。</strong></p>
<h3 id="深入理解核函数">深入理解核函数</h3><p>接下来让我们通过画一些图来更好地理解<strong>核函数</strong>是什么样的。</p>
<h4 id="$x$对$f$的值的影响">$x$对$f$的值的影响</h4><p>看下面这个例子，假设我们有两个特征$x_1$和$x_2$，假设我们第一个标记点是$l^{(1)}$：</p>
<p>$$<br>l^{(1)}=<br>\begin{bmatrix}<br>   3 \\<br>    5<br> \end{bmatrix}<br>$$</p>
<p>假设:</p>
<p>$$<br>σ^2=1<br>$$</p>
<p>如果我画出:</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=k(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>结果就是这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">3D曲面图</th>
<th style="text-align:center">等高线图</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/004.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/005.png" width="300" height="200" align="center"></td>
</tr>
</tbody>
</table>
<p>其中左侧的图纵轴是$f_1$，水平方向的两个轴分别是$x_1$和$x_2$。右侧的图是左侧图的等高线图。</p>
<p>你会发现，当$x=(3,5)$的时候，$f_1=1$，因为它在最大值的位置上。所以如果$x$往旁边移动，离这个点越远，那么从图中可以看到$f_1$的值就越接近0。</p>
<h4 id="$σ^2$对$f$的值的影响">$σ^2$对$f$的值的影响</h4><p>在这里，要提到的另一点就是$σ^2$对结果的影响。$σ^2$是<strong>高斯核函数</strong>的参数，改变它会得到略微不同的结果。</p>
<p>可以对比$σ^2=1$、$σ^2=0.5$、$σ^2=3$的情况：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$σ^2=1$</th>
<th style="text-align:center">$σ^2=0.5$</th>
<th style="text-align:center">$σ^2=3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/004.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/006.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/008.png" width="300" height="200" align="center"></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/005.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/007.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/009.png" width="300" height="200" align="center"></td>
</tr>
</tbody>
</table>
<p>你会发现，函数的形状还是相似的，只是$σ^2=0.5$相较于$σ^2=1$凸起的宽度变窄了，等值线图也收缩了一些；$σ^2=3$相较于$σ^2=1$凸起的宽度变宽了，等值线也扩张了一些。</p>
<p>所以，如果我们将$σ^2$设为0.5时，特征变量$f_1$下降到0的速度也会相应变快；如果我们将$σ^2$设为3时，特征变量$f_1$下降到0的速度也会相应变慢。</p>
<h3 id="获取预测函数">获取预测函数</h3><p>讲完了特征变量的定义，我们来看看我们能得到什么样的预测函数。</p>
<p>给定一个训练样本$x$，我们要计算出三个特征变量$f_1$ $f_2$ $f_3$</p>
<p><img src="/img/17_02_09/010.png" alt=""></p>
<p>并且如果$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3 \ge 0$，则预测函数的预测值为1，即$y=1$。</p>
<p>对于这个特定的例子而言，假设我们已经找到了一个学习算法，并且假设我已经得到了这些参数的值，比如如果：</p>
<p>$$<br>\theta_0=-0.5 \\<br>\theta_1=1 \\<br>\theta_2=1 \\<br>\theta_3=0<br>$$</p>
<p>如果我们现在有一个训练样本$x$：</p>
<p><img src="/img/17_02_09/011.png" alt=""></p>
<p>我想知道预测函数会给出什么结果。</p>
<p>看看这个公式：</p>
<p>$$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3 \ge 0$$</p>
<p>因为我的训练样本$x$接近于$l^{(1)}$，那么$f_1$就接近于1：</p>
<p>$$<br>f_1\approx1<br>$$</p>
<p>又因为训练样本$x$离$l^{(2)}$ $l^{(3)}$都很远，所以$f_2$就接近于0，$f_3$也接近于0：</p>
<p>$$<br>f_2\approx0 \\<br>f_3\approx0<br>$$</p>
<p>所以带入上面的公式可以得到：</p>
<p>$$<br>\begin{align*}<br>\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3<br>&amp;=\theta_0 + \theta_1·1 + \theta_2·0 +  \theta_3·0<br>\\<br>&amp;=-0.5+1<br>\\<br>&amp;=0.5\\<br>\ge0<br>\end{align*}<br>$$</p>
<p>因此对于这一点，我们预测的结果是$y=1$。</p>
<hr>
<p>现在我们选择另一个不同的点，假设我选择了另一个点：</p>
<p><img src="/img/17_02_09/012.png" alt=""></p>
<p>如果将这个训练样本$x$带入之前相同的计算，你发现$f_1$ $f_2$ $f_3$都接近于0。</p>
<p>因此，我们得到$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3=-0.5$，因为$θ_0=-0.5$，并且$f_1$ $f_2$ $f_3$都为0，因此最后结果是-0.5，小于0。因此这个点，我们预测的y值是0。</p>
<hr>
<p>类似的，如果你自己来对大量的点进行这样相应的处理，你应该可以确定如果你有一个非常接近于$l^{(2)}$的训练样本，那么通过这个点预测的y值也是1。</p>
<p>实际上，你最后得到的结果是：<strong>对于接近$l^{(1)}$和$l^{(2)}$的点，我们的预测值是1，对于远离$l^{(1)}$和$l^{(2)}$的点，我们最后预测的结果是等于0的</strong>。</p>
<p>我们最后会得到这个预测函数的判别边界看起来是类似这样的结果：</p>
<p><img src="/img/17_02_09/013.png" width="300" height="200" align="center"></p>
<p>在这个红色的判别边界里面，预测的y值等于1；在这外面预测的y值等于0。</p>
<p>因此这就是一个我们如何通过标记点，以及核函数，来训练出非常复杂的非线性判别边界的方法。</p>
<p>这就是核函数这部分的概念，以及我们如何在支持向量机中使用它们。我们通过标记点和相似性函数来定义新的特征变量从而训练复杂的非线性分类器。</p>
<p>目前还有一些问题我们并没有做出回答，其中一个是<strong>我们如何得到这些标记点</strong>；另一个是<strong>其他的相似度方程（核函数）是什么样的</strong>，如果有其他的话，我们能够用其他的相似度方程来代替我们所讲的这个高斯核函数吗？在下一个视频中我们会回答这些问题，然后把所有东西都整合到一起来看看支持向量机如何通过核函数的定义 有效地学习复杂非线性函数。</p>
<h2 id="核函数_II">核函数 II</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/hxdcH/kernels-ii" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上一节视频里，我们讨论了<strong>核函数</strong>这个想法以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。例如，<strong>怎么处理支持向量机中的偏差方差折中</strong>。</p>
</blockquote>
<h3 id="如何选取标记点(landmark)">如何选取标记点(landmark)</h3><p>在上一节课中，我谈到过选择标记点，例如$l^{(1)}$ $l^{(2)}$ $l^{(3)}$ 这些点使我们能够定义<strong>相似度函数</strong>，也称之为<strong>核函数</strong>。在这个例子里，我们的相似度函数为<strong>高斯核函数</strong>。</p>
<p><img src="/img/17_02_09/014.png" alt=""></p>
<p>但是，我们从哪里得到这些标记点呢？我们从哪里得到$l^{(1)}$ $l^{(2)}$ $l^{(3)}$？ 而且在一些复杂的学习问题中，也许我们需要更多的标记点，而不是我们手选的这三个。</p>
<p>因此 在实际应用时 怎么选取标记点 是机器学习中必须解决的问题 这是我们的数据集 有一些正样本和一些负样本 我们的想法是 我们将选取样本点 我们拥有的 每一个样本点 我们只需要直接使用它们 我们直接 将训练样本 作为标记点 </p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/06/斯坦福机器学习课程 第七周 (1)大间距分类/" itemprop="url">
                斯坦福机器学习课程 第七周 (1)大间距分类
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-06T00:42:58+08:00" content="2017-02-06">
            2017-02-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/06/斯坦福机器学习课程 第七周 (1)大间距分类/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/06/斯坦福机器学习课程 第七周 (1)大间距分类/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="优化目标">优化目标</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/sHfVT/optimization-objective" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>到目前为止，你已经见过一系列不同的学习算法。在监督学习中许多学习算法的性能都非常类似，因此重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是应用这些算法时所创建的大量数据。</p>
<p>在应用这些算法时，表现情况通常依赖于你的水平。比如你为学习算法所设计的 特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法，广泛的应用于工业界和学术界，它被称为<strong>支持向量机(Support Vector Machine)</strong>。</p>
<p>与逻辑回归和神经网络相比，<strong>支持向量机</strong>或者简称<strong>SVM</strong>在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p>
<p>因此，在接下来的视频中我会探讨这一算法，在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于<strong>支持向量机</strong>，鉴于该算法的强大和受欢迎度，在本课中我会花许多时间来讲解它，它也是我们所介绍的最后一个监督学习算法。</p>
</blockquote>
<h3 id="支持向量机引入">支持向量机引入</h3><p>为了描述<strong>支持向量机</strong>，我将会从逻辑回归开始，展示我们如何一点一点修改，来得到本质上的支持向量机。</p>
<p>在逻辑回归中，我们已经熟悉了它的假设函数形式：</p>
<p>$$<br>h_{\theta}(x)=\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}<br>$$</p>
<p>和S型激励函数：</p>
<p><img src="/img/17_02_06/001.png" width="300" height="200" align="center"></p>
<p>现在让我们一起来考虑下，我们想要逻辑回归做什么？</p>
<ul>
<li><p>如果有一个样本为$y=1$，那么我们希望假设函数$h(x)≈1$，即$\theta^{T}&gt;&gt;0$。你不难发现，此时逻辑回归的输出将趋近于1。</p>
</li>
<li><p>如果有另一个样本为$y=0$，那么我们希望假设函数$h(x)≈0$，即$\theta^{T}&lt;&lt;0$。此时逻辑回归的输出将趋近于0。</p>
</li>
</ul>
<hr>
<p>如果你进一步观察逻辑回归的代价函数，你会发现每个样本(x, y)都会为总代价函数增加这样的一项：</p>
<p>$$<br>-(ylogh_{\theta}(x) + (1-y)log(1-h_{\theta}(x)))<br>= -ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。</p>
<p>现在一起来考虑<strong>y=1</strong>和<strong>y=0</strong>的两种情况：</p>
<ul>
<li><strong>y=1的情况下（即$\theta^{T}x&gt;&gt;0$）</strong>：</li>
</ul>
<p>对于</p>
<p>$$<br>-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>由于$(1-y)=0$，所以我们只需考虑前半部分：</p>
<p>$$<br>-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}<br>$$</p>
<p>如果画出代价函数关于$z$的图，你会看到下图：</p>
<p><img src="/img/17_02_06/002.png" width="300" height="200" align="center"></p>
<p>我们可以看到，当$z$增大时(即$\theta^{T}x$增大时)，$z$对应的值会变得非常小，对整个代价函数而言，影响也非常小。</p>
<p>现在开始建立<strong>支持向量机</strong>，我们会从这个代价函数$-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}$开始，一点点的修改：</p>
<p>我们画出一个非常接近于逻辑回归函数的折线，这个折线经由$z=1$的一点的两条线段组成：</p>
<p><img src="/img/17_02_06/003.png" width="300" height="200" align="center"></p>
<p>到这里已经非常接近逻辑回归中使用的代价函数了，只是这里是由两条线段组成。先不要考虑线段的斜率，这并不重要，重要的是我们将在$y=1$的前提下使用新的代价函数。</p>
<ul>
<li><strong>y=0的情况下（即$\theta^{T}x&lt;&lt;0$）</strong>：</li>
</ul>
<p>对于</p>
<p>$$<br>-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>由于$y=0$，所以我们只需考虑后半部分：</p>
<p>$$<br>(1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>如果画出代价函数关于$z$的图，你会看到下图：</p>
<p><img src="/img/17_02_06/004.png" width="300" height="200" align="center"></p>
<p>用相似的方法，我们开始建立<strong>支持向量机</strong>：</p>
<p><img src="/img/17_02_06/005.png" width="300" height="200" align="center"></p>
<p>我们将在$y=0$的前提下使用新的代价函数。</p>
<hr>
<p>那么现在我们来给这两个方程命名：</p>
<p>对于这个函数：</p>
<p><img src="/img/17_02_06/003.png" width="300" height="200" align="center"></p>
<p>我们命名为<strong>$cost_{1}(z)$</strong>。</p>
<p>对于第二个函数：</p>
<p><img src="/img/17_02_06/005.png" width="300" height="200" align="center"></p>
<p>我们命名为<strong>$cost_{0}(z)$</strong>。</p>
<p>这里的下标指的是在函数中对应的$y=1$和$y=0$的情况。</p>
<hr>
<h3 id="构建支持向量机">构建支持向量机</h3><p>拥有了这些定义之后，现在我们就开始构建<strong>支持向量机</strong>。</p>
<h4 id="1-替换逻辑回归函数">1.替换逻辑回归函数</h4><p>这就是我们在逻辑回归中使用的代价函数$J(\theta)$：</p>
<p><img src="/img/17_02_06/006.png" alt=""></p>
<p>对于支持向量机而言，实际上，我们要将</p>
<ul>
<li><p>上面式子中的这一项：$(-logh_{\theta}(x^{(i)}))$替换为：$cost_{1}(z)$，即:$cost_{1}(\theta^{T}x^{(i)})$</p>
</li>
<li><p>同样，这一项：$((-log(1-h_{\theta}(x^{(i)}))))$替换为：$cost_{0}(z)$，即:$cost_{0}(\theta^{T}x^{(i)})$</p>
</li>
</ul>
<p>这里替换之后的$cost_{1}(z)$和$cost_{0}(z)$就是上面提到的那两条靠近逻辑回归函数的折线。</p>
<p>所以对于<strong>支持向量机</strong>的最小化代价函数问题，代价函数的形式如下：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{m}[<br>\sum_{i=1}^{m}<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{\lambda}{2m}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<h4 id="2-去除多余的常数项_$\frac{1}{m}$">2.去除多余的常数项 $\frac{1}{m}$</h4><p>现在按照<strong>支持向量机</strong>的惯例，我们去除$\frac{1}{m}$这一项，因为这一项是个常数项，即使去掉我们也可以得出相同的$\theta$最优值：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\sum_{i=1}^{m}<br>[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{\lambda}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<h4 id="3-正则化项系数的处理">3.正则化项系数的处理</h4><p>在逻辑回归的目标函数中，我们有两项表达式：</p>
<ul>
<li>来自于训练样本的代价函数:</li>
</ul>
<p>$$<br>\frac{1}{m}[<br>\sum_{i=1}^{m}<br>y^{(i)}<br>(-logh_{\theta}(x^{(i)}))+<br>(1-y^{(i)})<br>((-log(1-h_{\theta}(x^{(i)}))))<br>]<br>$$</p>
<ul>
<li>正则化项：</li>
</ul>
<p>$$<br>\frac{\lambda}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>我们不得不使用正则化项来平衡我们的代价函数。这就相当于：</p>
<p>$$<br>A + \lambda B<br>$$</p>
<p>其中A相当于上面的第一项，B相当于第二项。</p>
<p>我们通过修改不同的正则化参数$\lambda$来达到优化目的，这样我们就能够使得训练样本拟合的更好。</p>
<p>但对于<strong>支持向量机</strong>，按照惯例我们将使用一个不同的参数来替换这里使用的$\lambda$来实现权衡这两项的目的。这个参数我们称为<strong>C</strong>。同时将优化目标改为:</p>
<p>$$<br>CA + B<br>$$</p>
<p>因此，在逻辑回归中，如果给$\lambda$一个很大的值，那么就意味着给与$B$了一个很大的权重，而在<strong>支持向量机</strong>中，就相当于对$C$设定了一个非常小的值，这样一来就相当于对$B$给了比$A$更大的权重。</p>
<p>因此，这只是一种来控制这种权衡关系的不同的方式。当然你也可以把这里的$C$当做$farc{1}{\lambda}$来使用。</p>
<p>因此，这样就得到了在<strong>支持向量机</strong>中的我们的整个优化目标函数：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<hr>
<p>最后有别于<strong>逻辑回归</strong>的一点，对于<strong>支持向量机</strong>假设函数的形式如下：</p>
<p>$$<br>h_{\theta}(x) = 1 \ \ \ if \ \theta^Tx \ge 0<br>$$</p>
<p>$$<br>h_{\theta}(x) = 0 \ \ \ if \ \theta^Tx \lt 0<br>$$</p>
<p>而不是<strong>逻辑回归</strong>中的S型曲线：</p>
<p>$$<br>h_{\theta}(x)=\frac{1}{1+e^{-x}}<br>$$</p>
<h2 id="大间距的直觉">大间距的直觉</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/wrjaS/large-margin-intuition" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>人们有时将<strong>支持向量机</strong>看做是<strong>大间距分类器</strong>。在这一部分，我将介绍其中的含义，这有助于我们直观地理解SVM模型的假设是什么样的。</p>
</blockquote>
<p>这是我的支持向量机模型的代价函数：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<ul>
<li>如果你有一个正样本，即$y=1$时，那么代价函数$cost_{1}(z)$的图像如下：</li>
</ul>
<p><img src="/img/17_02_06/007.png" width="300" height="200" align="center"></p>
<p>可以看出，<strong>只有在$z\ge1$(即$\theta^{T}x\ge1$)时(不仅仅是$\ge0$)，代价函数$cost_{1}(z)$的值才等于0</strong>。</p>
<ul>
<li>反之，如果你有一个负样本，即$y=0$时，那么代价函数$cost_{0}(z)$的图像如下：</li>
</ul>
<p><img src="/img/17_02_06/008.png" width="300" height="200" align="center"></p>
<p>可以看出，<strong>只有在$z\le-1$(即$\theta^{T}x\le-1$)时(不仅仅是$\lt0$)，代价函数$cost_{0}(z)$的值才等于0</strong>。</p>
<p>这是<strong>支持向量机</strong>的一个有趣的性质。</p>
<h3 id="安全距离因子">安全距离因子</h3><p>事实上，在逻辑回归中：</p>
<ul>
<li><p>如果你有一个正样本，即$y=1$的情况下，我们仅仅需要$\theta^{T}x\ge0$；</p>
</li>
<li><p>如果你有一个负样本，即$y=0$的情况下，我们仅仅需要$\theta^{T}x\lt0$；</p>
</li>
</ul>
<p>就能将该样本恰当的分类了。</p>
<p>但是<strong>支持向量机</strong>的要求更高，不仅仅要求$\theta^{T}x\ge0$或$\theta^{T}x\lt0$，而且要求$\theta^{T}x$比0大很多，或小很多。比如这里要求$\theta^{T}x\ge1$以及$\theta^{T}x\le-1$。</p>
<p>这就相当于在<strong>支持向量机</strong>中嵌入了一个额外的安全因子（或者说是安全距离因子）。接下来让我们来看看这个因子会导致什么结果：</p>
<p>具体而言，我接下来会将代价函数中的常数项$C$设置成一个非常大的值，比如100000或者其他非常大的数，然后再来观察支持向量机会给出什么结果。</p>
<p>当代价函数中</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>$C$的值非常大时，则最小化代价函数的时候，我们会很希望找到一个使第一项：</p>
<p>$$<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]<br>$$</p>
<p>为0的最优解。</p>
<p>可以看到当输入一个正样本$y^{(i)}=1$时，我们想令上面这一项为0，从图中可以得出</p>
<p><img src="/img/17_02_06/007.png" width="300" height="200" align="center"></p>
<p>对于代价函数$cost_{1}(z)$我们需要使得$\theta^{T}x^{(i)}\ge1$。</p>
<p>类似地，对于一个负训练样本$y^{(i)}=0$时，我们想令上面这一项为0，从图中可以得出</p>
<p><img src="/img/17_02_06/008.png" width="300" height="200" align="center"></p>
<p>对于代价函数$cost_{0}(z)$我们需要使得$\theta^{T}x^{(i)}\le-1$。</p>
<hr>
<p>这样一来会产生下面这种优化问题：</p>
<p>因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C0+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>我们知道是$C0$的结果是0，因此可以删掉，所以最终得到的结果是：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>其中：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>这样我们就得到了一个非常有趣的决策边界。</p>
<h3 id="SVM决策边界：线性分割案例">SVM决策边界：线性分割案例</h3><p>具体而言，如果你仔细观察下面这个既有正样本又有负样本的数据集</p>
<p><img src="/img/17_02_06/009.png" width="300" height="200" align="center"></p>
<p>不难看出，这个数据集是线性可分的（即存在一条直线把正负样本分开）。可以看出有很多直线都可以把正负样本区分开，比如下面这两条看起来不太自然的直线：</p>
<p><img src="/img/17_02_06/010.png" width="300" height="200" align="center"></p>
<p>支持向量机会选择黑色的这一条直线：</p>
<p><img src="/img/17_02_06/011.png" width="300" height="200" align="center"></p>
<p>这条直线看起来好很多，因为它看起来更加稳健。在数学上来讲就是这条直线拥有相对于训练数据更大的最短距离，这个所谓的距离就是指<strong>间距(margin)</strong>：</p>
<p><img src="/img/17_02_06/012.png" width="300" height="200" align="center"></p>
<p>而之前两条粉线和蓝线距离训练样本非常近，在分离样本时就会表现的比黑线差。</p>
<p>这就是<strong>支持向量机</strong>拥有<a href="http://baike.baidu.com/link?url=My7Y1mL_9uj-XdR2DC2kyGLop-AaPdzSgdNmgRmaJVYV77puxNs-_A7ERwLv3uWih02JCu6esljRn90mc3EkMwKXBckm_6wSqU42EX06vC4ouxQhinIZco7crxr7HetC" target="_blank" rel="external">鲁棒性</a>的原因。因为它一直努力用一个最大间距来分离样本。因此支持向量机分类器有时又被称为<strong>大间距分类器</strong>。</p>
<p>也许你想知道支持向量机是如何做到产生这个大间距分类器的，目前我还没解释这一点，在下一节中我会直观的来解释这一点。目前这个例子只是用于理解<strong>支持向量机模型</strong>的做法，即努力将正负样本用最大的间距区分开。</p>
<h3 id="大间距分类器中的异常值">大间距分类器中的异常值</h3><p>最后要讲的一点是对于支持向量机中的异常数据的处理。在下面这组训练集中：</p>
<p><img src="/img/17_02_06/013.png" width="300" height="200" align="center"></p>
<p>我们通过使用支持向量机来进行分类，会得到这条黑色的决策边界，从而最大间距的区分这两种数据：</p>
<p><img src="/img/17_02_06/014.png" width="300" height="200" align="center"></p>
<p>当有一个异常值产生时：</p>
<p><img src="/img/17_02_06/015.png" width="300" height="200" align="center"></p>
<p>我们的算法会受到异常值的影响。这时我们将支持向量机中的正则化因子$C$设置的非常大，那么我们会得到类似这样一条粉色的决策边界：</p>
<p><img src="/img/17_02_06/016.png" width="300" height="200" align="center"></p>
<p>那么我们仅仅通过一个异常值，就将我们的决策边界旋转了这么大的角度，实在是不明智的。</p>
<p><img src="/img/17_02_06/017.png" width="300" height="200" align="center"></p>
<p>当我们的正则化因子$C$的值非常大时，支持向量机确实会如此处理，但如果我们适当的减小$C$的值，你最终还是会得到那条黑色的决策边界的。</p>
<p>如果数据是线性不可分的话，像这样：</p>
<p><img src="/img/17_02_06/018.png" width="300" height="200" align="center"></p>
<p>支持向量机也可以恰当的将它们分开。</p>
<p>值得提醒的是$C$的作用其实等同于$\frac{1}{\lambda}$，$\lambda$就是我们之前用到的正则化参数。在支持向量机中，$C$不是很大的时候，可以对包含异常数据、以及线性不可分的数据有比较好的处理效果。</p>
<p>稍后我们还会介绍支持向量机的偏差和方差，希望到那时候关于如何处理参数的这种平衡会变得更加清晰。</p>
<h2 id="大间距分类器背后的数学原理(选学)">大间距分类器背后的数学原理(选学)</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/3eNnh/mathematics-behind-large-margin-classification" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这一节将介绍大间距分类背后的数学原理。</p>
<p>本节作为选学内容，你完全可以跳过，但是听听这节课可能让你对支持向量机中的优化问题，以及如何得到大间距分类器产生更好的直观理解。</p>
</blockquote>
<h3 id="向量内积">向量内积</h3><p>首先，带大家复习一下<strong>向量内积</strong>的知识。</p>
<p>假设我们有两个二维向量：</p>
<p>$$<br>u=<br> \begin{bmatrix}<br>   u_1 \\<br>    u_2<br> \end{bmatrix}<br> \<br> v=<br> \begin{bmatrix}<br>   v_1 \\<br>   v_2<br> \end{bmatrix}<br>$$</p>
<p>我们把</p>
<p>$$<br>u^{T}v<br>$$</p>
<p>的计算结果称作向量$u$和$v$之间的<strong>内积</strong>。</p>
<p>由于这里我们用的是二维向量，因此我们可以把这两个向量绘制在同一坐标系内，向量$u$和$v$如下：</p>
<p><img src="/img/17_02_06/019.png" width="300" height="200" align="center"></p>
<p>其中我们用$||u||$来表示$u$的<strong>范数</strong>（即$u$的长度），因此$||u||$的计算公式如下：</p>
<p>$$<br>||u||=\sqrt{u_1^2+u_2^2}<br>$$</p>
<p>下面我们来看看<strong>向量内积</strong>具体是如何计算的：</p>
<p>将向量$v$投影到向量$u$上，如下图我们对向量$v$做一个相对于向量$u$的直角投影：</p>
<p><img src="/img/17_02_06/020.png" width="300" height="200" align="center"></p>
<p>投影之后的长度就是图中红线$p$的长度：</p>
<p>$$<br>p = 向量v投影到向量u上的长度<br>$$</p>
<p>同时也有另外一种计算内积的方式：</p>
<p>$$<br>u^Tv=p·||u||<br>$$</p>
<p>通过这种方式计算出来的内积，答案和之前也是一样的。</p>
<p>事实上，如果你想要使用将$u$投影到$v$上来用这种方式来计算内积，得到的答案也是相同的。</p>
<p>要注意的一点是，这里的$p$是有符号的，如果两个向量的夹角大于90°，像下图中这种情形，如果将$v$投影到$u$上会得到这样一种投影：</p>
<p><img src="/img/17_02_06/021.png" width="300" height="200" align="center"></p>
<p>此时的$p$就是一个负数。</p>
<blockquote>
<p>在向量的内积问题中，如果两个向量的夹角小于90°，那么$p$的符号就是为正；如果两个向量的夹角大于90°，那么$p$的符号就为负。</p>
</blockquote>
<p>这就是<strong>向量内积</strong>的知识，接下来我们尝试使用它来理解支持向量机中的目标函数。</p>
<h3 id="SVM决策边界">SVM决策边界</h3><p>下面是我们在支持向量机中的目标函数:</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>其中</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>接下来为了让目标函数更容易被分析，我们来忽略掉截距的影响，令$\theta_0=0$，这样更容易绘制示意图。并且我们将特征数$n$设置为2，因此我们仅有两个特征$x_1$和$x_2$：</p>
<p>$$<br>\begin{align*}<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>&amp;=<br>\frac{1}{2}<br>(\theta_1^2+\theta_2^2)<br>\\&amp;=<br>\frac{1}{2}<br>(\sqrt{\theta_1^2+\theta_2^2})^2<br>\end{align*}<br>$$</p>
<p>其中</p>
<p>$$<br>\sqrt{\theta_1^2+\theta_2^2} = ||\theta||<br>$$</p>
<p>因此可以得出：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>=<br>\frac{1}{2}||\theta||^2<br>$$</p>
<p>可见，<strong>支持向量机所做的事情，其实就是在极小化参数向量$\theta$范数的平方（或者说是长度的平方）</strong>。</p>
<hr>
<p>现在让我们来看看这两行的含义：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>想一想$\theta^{T}x^{(i)}$这一项等于什么呢？</p>
<p>在前面我们画出了$u^Tv$的示意图，这里$\theta^T$就相当于$u^T$、$x^{(i)}$就相当于$v$。让我们来看一下示意图：</p>
<p>我们考虑一个单一的样本$x^{(i)}$，其坐标为$(x^{(i)}_1,x^{(i)}_2)$</p>
<p><img src="/img/17_02_06/022.png" width="300" height="200" align="center"></p>
<p>这个训练样本点其实可以表示为一个训练样本向量：</p>
<p><img src="/img/17_02_06/023.png" width="300" height="200" align="center"></p>
<p>现在，我们有一个参数向量：</p>
<p><img src="/img/17_02_06/024.png" width="300" height="200" align="center"></p>
<p>那么我们向量内积的计算方式，通过使用之前的方法可以得出。训练样本向量投影到参数向量上的长度$p^{(i)}$，表示第i个训练样本在参数向量$\theta$上的投影：</p>
<p><img src="/img/17_02_06/025.png" width="300" height="200" align="center"></p>
<p>根据之前我们所学到的，我们可以知道：</p>
<p>$$<br>\begin{align*}<br>\theta^Tx^{(i)}<br>&amp;=p^{(i)}·||\theta||<br>\\<br>&amp;=\theta_1x_1^{(i)}+\theta_2x_2^{(i)}<br>\end{align*}<br>$$</p>
<p>那么，这告诉我们了什么呢？这说明：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>这里的约束项是可以用$p^{(i)}·||\theta||$来替代的：</p>
<ul>
<li>若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$</li>
<li>若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$</li>
</ul>
<p>因此，将其写入我们的优化目标后，<strong>完整的目标函数</strong>为：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>=<br>\frac{1}{2}||\theta||^2<br>$$</p>
<p>其中</p>
<ul>
<li>若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$</li>
<li>若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$</li>
</ul>
<hr>
<h4 id="实例">实例</h4><p>现在让我们考虑下面这里的训练样本：</p>
<p><img src="/img/17_02_06/026.png" width="300" height="200" align="center"></p>
<p>其中假设截距依然为0，即$\theta_0=0$，我们来看一下支持向量机会选择什么样的决策边界。</p>
<p>假设有这样一条决策边界：</p>
<p><img src="/img/17_02_06/027.png" width="300" height="200" align="center"></p>
<p>很明显，这不是一个好的决策边界，因为这个决策边界离训练样本很近，我们来看一下为什么支持向量机不会选择它。</p>
<p>由于<strong>决策边界和参数向量是正交的(斜率相乘结果为-1)</strong>(<a href="https://zhidao.baidu.com/question/1992397864989257747.html" target="_blank" rel="external">为什么决策边界和参数向量是正交的</a>)，我们可以绘制出对应的参数向量$\theta$：</p>
<p><img src="/img/17_02_06/028.png" width="300" height="200" align="center"></p>
<blockquote>
<p>这里由于我们指定了$\theta_0=0$，也就意味着决策边界是过原点的。</p>
</blockquote>
<p>假设我们以这一点为第一个训练样本：</p>
<p><img src="/img/17_02_06/029.png" width="300" height="200" align="center"></p>
<p>我们可以画出这个样本向量到$\theta$的投影$p^{(1)}$：</p>
<p><img src="/img/17_02_06/030.png" width="300" height="200" align="center"></p>
<p>类似的，我们也可以画出第二个样本向量到$\theta$的投影$p^{(2)}$：</p>
<p><img src="/img/17_02_06/031.png" width="300" height="200" align="center"></p>
<p>我们会发现，这些$p^{(i)}$将会是一些非常小的数。因此当我们考察优化目标函数的时候：</p>
<ul>
<li><p>对于<strong>正样本($y^{(i)}=1$，即图中的”x”样本)</strong>而言，我们需要$p^{(i)}·||\theta||\ge1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。</p>
</li>
<li><p>对于<strong>负样本($y^{(i)}=-1$，即图中的”o”样本)</strong>而言，我们需要$p^{(i)}·||\theta||\le-1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。</p>
</li>
</ul>
<p>但我们的实际目标是希望找到一个参数$\theta$，使得它的范数$||\theta||$是尽可能小的，因此这并不是一个好的决策边界，因为我们的$||\theta||$比较大。</p>
<hr>
<p>对于下面这个决策边界来说，情况就会有很大的不同：</p>
<p><img src="/img/17_02_06/032.png" width="300" height="200" align="center"></p>
<p>这里，我们以纵坐标作为决策边界，那么我们的参数向量的方向就是垂直于它的方向：</p>
<p><img src="/img/17_02_06/033.png" width="300" height="200" align="center"></p>
<p>如果我们现在再来绘制出样本向量在参数向量上的投影$p^{(1)}$和$p^{(2)}$的话，你会发现这些投影的长度比之前长多了：</p>
<p><img src="/img/17_02_06/034.png" width="300" height="200" align="center"></p>
<p>因为投影$p$的长度变大了，随之$\theta$的范数$||\theta||$也相应的变小了。这就意味着通过选择第二种远离样本的决策边界，支持向量机可以使参数$\theta$的范数$||\theta||$变小很多。</p>
<p>这就是<strong>为什么支持向量机可以产生大间距分类的原因</strong>。</p>
<hr>
<p>最后一点，我们的推导自始至终都使用了<strong>截距为0（即$\theta_0=0$）</strong>这个简化假设。这样做的作用就是可以使得决策边界始终是通过原点的，如果你的决策边界不过原点，那么$\theta_0\ne0$，但支持向量机会产生大间距分类器的结论依然成立（具体推导过程不再叙述，和这里很类似）。但是可以说明的是，即便$\theta_0\ne0$，支持向量机仍然会找到正样本和负样本之间的大间距分隔。总之 我们解释了为什么支持向量机是一个大间距分类器。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/04/【Python3教程 第3章】方法和模块/" itemprop="url">
                【Python3教程 第3章】方法和模块
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-04T17:17:58+08:00" content="2017-02-04">
            2017-02-04
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/python/" itemprop="url" rel="index">
                  <span itemprop="name">python</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/04/【Python3教程 第3章】方法和模块/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/04/【Python3教程 第3章】方法和模块/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="代码复用">代码复用</h2></span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/04/斯坦福机器学习课程 第六周 (5)使用大数据集/" itemprop="url">
                斯坦福机器学习课程 第六周 (5)使用大数据集
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-04T00:55:58+08:00" content="2017-02-04">
            2017-02-04
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/04/斯坦福机器学习课程 第六周 (5)使用大数据集/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/04/斯坦福机器学习课程 第六周 (5)使用大数据集/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="机器学习的数据">机器学习的数据</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/XcNcz/data-for-machine-learning" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在之前的视频中，我们讨论了评价指标。在本节课的视频中，我们来讨论一下机器学习系统设计中另一个重要的方面，这往往涉及到用来训练的数据有多少。</p>
<p>在之前的一些视频中，我曾告诫大家不要一开始就盲目地花大量的时间来收集大量的数据，因为这种做法只在某些情况下能起到实际的作用。但事实证明，在一定条件下，得到大量的数据，并在某种类型的学习算法中进行训练，却是一种有效的获得一个具有良好性能的学习算法的方法。而这种情况往往出现在这些条件对于你的问题都成立，并且你能够得到大量数据的情况下。这是一个很好的获得非常高性能的学习算法的方式。</p>
</blockquote>
<p>我先讲一个故事。很多很多年前，我认识的两位研究人员 Michele Banko 和 Eric Brill，他们进行了一项有趣的研究：研究在不同的数据集上使用不同的学习算法的效果。他们当时考虑的问题是混淆词分类问题，例如，下面这个句子：</p>
<ul>
<li><strong>For breakfast I ate __ eggs.</strong></li>
</ul>
<p>在这个句子中，空白处的可选词是：</p>
<ul>
<li><strong>to</strong></li>
<li><strong>two</strong></li>
<li><strong>too</strong></li>
</ul>
<p>在这个例子中应该填入<strong>two</strong>。</p>
<p>于是他们把诸如这样的机器学习问题当做一类监督学习问题，并尝试将其分类：什么样的词在一个英文句子特定的位置才是合适的。</p>
<p>他们用了下面几种不同的学习算法，这些算法在他们2001年进行研究的时候都已经 被公认是比较领先的学习算法：</p>
<ul>
<li><strong>感知器(Perceptron) (逻辑回归)</strong></li>
<li><strong>Winnow(类似于回归问题，但不完全相同。过去常用，但现在很少用到)</strong></li>
<li><strong>基于内存的学习方法(Memory-based)</strong></li>
<li><strong>朴素贝叶斯</strong></li>
</ul>
<p>这些算法的具体细节不重要，总之他们使用了这几种算法。他们所做的就是改变了训练数据集的大小，并尝试将这些学习算法用于不同大小的训练数据集中。这就是他们得到的结果：</p>
<p><img src="/img/17_02_04/001.png" width="300" height="200" align="center"></p>
<p>横轴代表以百万为单位的数据集大小，即从0.1百万到1000百万（10亿）规模的数据集；纵轴代表精确度，范围是0到1。</p>
<p>从图中可以看出趋势非常明显。首先大部分算法都具有相似的性能，其次随着训练数据集的增大，这些算法的准确性也都对应的增强了。事实上，如果你选择任意一个算法，例如选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些列子中看起来，它很有可能会其他算法更好，甚至会比”优等算法”更好。</p>
<p>由于这项原始的研究非常具有影响力，已经有一系列不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的效果，虽然这些效果的表现还取决于一些细节，但是真正能提高算法性能(准确度)的，就是你能够给到一个算法的大量的训练数据。类似这样的结论引起了一种在机器学习中的普遍共识：</p>
<ul>
<li><p><strong>It`s not who has the best algrithm that wins.It`s who has the most data.</strong></p>
</li>
<li><p><strong>取得成功的人不是因为拥有最好算法，而是因为拥有最多数据。</strong></p>
</li>
</ul>
<p>那么这种说法在什么时候是靠谱的，什么时候是不靠谱的呢？因为如果上面的说法在所有情况下都是对的话，那么我们保证我们得到一个高性能的学习算法的最佳方式应该是获取大量的数据，而不是考虑该使用什么学习算法。</p>
<p>其实，在下面这种情况下，获取大量的数据是提高算法性能的好方法：</p>
<p><strong>在特征值x包含足够多的用来准确预测y的信息时，获取大量的数据是提高算法性能的好方法。</strong></p>
<p>例如在上面介绍的混淆词的例子中：</p>
<ul>
<li><strong>For breakfast I ate __ eggs.</strong></li>
</ul>
<p>空白词附近的词就是我们需要捕捉的特征<strong>x</strong>，这些词中包含了大量的信息来告诉我空白处应该填写<strong>two</strong>而不是<strong>to</strong>或者<strong>too</strong>。实际上对于特征捕捉，哪怕是周围词语中的一个词，就能够给我足够的信息来确定出标签<strong>y</strong>是什么了。</p>
<p>这就是通过一个有充足的信息的特征值<strong>x</strong>的来确定<strong>y</strong>的例子。举一个反例：</p>
<p>设想在房屋价格预测问题中，我们获取的房屋信息中只有房屋面积信息，没有其他的特征值，那么如果我告诉你这个房子有500平方英尺，但是我没有告诉你其他的特征信息，我也不告诉你这个房子位于这个城市房价比较昂贵的区域还是便宜的区域，也不告诉你这所房子的房间数量，它里面陈设了多漂亮的家具，以及这个房子是新的还是旧的。我不告诉你除了这个房子有500平方英尺以外的其他任何信息，然而除此之外还有许多其他因素会影响房子的价格，不仅仅是房子的大小。但如果你仅仅知道房屋的尺寸，那么事实上是很难准确预测它的价格的。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/01/19/斯坦福机器学习课程 第六周 (4)操作偏斜数据/" itemprop="url">
                斯坦福机器学习课程 第六周 (4)操作偏斜数据
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-01-19T23:29:58+08:00" content="2017-01-19">
            2017-01-19
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/01/19/斯坦福机器学习课程 第六周 (4)操作偏斜数据/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/01/19/斯坦福机器学习课程 第六周 (4)操作偏斜数据/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="偏移类的错误度量">偏移类的错误度量</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/tKMWX/error-metrics-for-skewed-classes" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在前面的课程中,我提到了<strong>误差分析</strong>以及设定误差度量值的重要性。那就是设定某个实数来评估你的学习算法，并衡量它的表现。有了算法的评估和误差度量值，有一件重要的事情要注意，就是使用一个合适的误差度量值。这有时会对于你的学习算法造成非常微妙的影响。</p>
<p>这件重要的事情就是<strong>偏斜类（skewed classes）</strong>的问题。</p>
</blockquote>
<p>想一想之前的癌症分类问题，我们拥有内科病人的特征变量，我们希望知道他们是否患有癌症，这就像我们之前讲过的恶性与良性肿瘤的分类问题。</p>
<p><strong>癌症分类实例</strong></p>
<ul>
<li>训练逻辑回归模型$h_{\theta}(x)$。(如果是癌症$y=1$，否则$y=0$)</li>
<li>发现你在测试集上有1%的错误率。(99%的诊断是正确的)</li>
<li>只有0.50%的病人患有癌症</li>
</ul>
<p>我们训练逻辑回归模型，假设我们用测试集检验了这个分类模型，并且发现它只有1%的错误，因此我们99%会做出正确诊断。看起来是非常不错的结果，我们99%的情况都是正确的。</p>
<p>但是，假如我们发现，在测试集中只有0.5%的患者真正得了癌症，那么在这个例子中 1%的错误率就不再显得那么好了。</p>
<p>举个具体的例子，这里有一行代码：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">predictCancer</span><span class="params">(x)</span></span></div><div class="line">	y = <span class="number">0</span>; <span class="comment">%ignore x!</span></div><div class="line"><span class="keyword">return</span></div></pre></td></tr></table></figure>
<p>不是机器学习代码，它忽略了输入值<code>X</code>，它让<code>y</code>总是等于0，因此它总是预测没有人得癌症，那么这个算法实际上只有<code>0.5%</code>的错误率。因此这甚至比我们之前得到的<code>1%</code>的错误率更好。这是一个非机器学习算法，因为它只是预测<code>y</code>总是等于<code>0</code>。</p>
<p>这种情况发生在正例和负例的比率非常接近于一个极端值，在这个例子中，正样本的数量与负样本的数量相比，非常非常少。因为<code>y=1</code>非常少，我们把这种情况叫做<strong>偏斜类</strong>。</p>
<p>一个类中的样本数与另一个类的数据相比多很多，通过总是预测<code>y=0</code>或者总是预测<code>y=1</code>算法可能表现非常好。因此使用分类误差或者分类精确度来作为评估度量可能会产生如下问题：</p>
<p>假如说你有一个算法，它的精确度是<code>99.2%</code>，因此它只有<code>0.8%</code>的误差。假设你对你的算法做出了一点改动，现在你得到了<code>99.5%</code>的精确度，只有<code>0.5%</code>的误差，这到底是不是算法的一个提升呢？</p>
<p>用某个实数来作为评估度量值的一个好处就是，它可以帮助我们<strong>迅速决定我们是否需要对算法做出一些改进</strong>。</p>
<p>将精确度从<code>99.2%</code>提高到<code>99.5%</code>，但是我们的改进到底是有用的，还是说我们只是把代码替换成了像<code>y=0</code>这样的东西？</p>
<p>因此如果你有一个偏斜类，用分类精确度并不能很好地衡量算法，因为你可能会获得一个很高的精确度，非常低的错误率。但是我们并不知道我们是否真的提升了分类模型的质量，因为<code>y=0</code>并不是一个好的分类模型。但是<code>y=0</code>会将你的误差降低至<code>0.5%</code>。</p>
<h3 id="查准率（precision）和召回率（recall）">查准率（precision）和召回率（recall）</h3><p>当我们遇到这样一个偏斜类时，我们希望有一个不同的误差度量值，或者不同的评估度量值。其中一种评估度量值叫做<strong>查准率（precision）</strong>和<strong>召回率（recall）</strong>。</p>
<p>让我来解释一下：</p>
<p>假设我们正在用测试集来评估一个分类模型，对于测试集中的样本，每个测试集中的样本都会等于0或者1(假设这是一个二分问题)我们的学习算法要做的是：做出值的预测，并且学习算法会为每一个测试集中的实例做出预测，预测值也是等于0或1。</p>
<p>让我画一个2x2的表格：</p>
<p><img src="/img/17_01_19/001.png" alt=""></p>
<p>基于实际的类与预测的类：</p>
<ul>
<li>如果有一个样本它实际所属的类是1，预测的类也是1，那么我们把这个样本叫做<strong>真阳性（true positive）</strong>，意思是说我们的学习算法 预测这个值为阳性，实际上这个样本也确实是阳性。</li>
<li>如果我们的学习算法预测某个值是阴性(等于0)，实际的类也确实属于0，那么我们把这个叫做<strong>真阴性（true negative）</strong>，我们预测为0的值实际上也等于0。</li>
<li>如果我们的学习算法预测某个值等于1，但是实际上它等于0，这个叫做<strong>假阳性（false positive）</strong>。<blockquote>
<p>比如我们的算法预测某些病人患有癌症，但是事实上他们并没有得癌症。</p>
</blockquote>
</li>
<li>最后如果我们的学习算法预测某个值等于0，但是实际上它等于1，这个叫做<strong>假阴性（false negative）</strong>。因为我们的算法预测值为0，但是实际值是1。</li>
</ul>
<p>这样我们有了一个另一种方式来评估算法的表现。</p>
<p>我们要计算两个数字：</p>
<h4 id="查准率（Precision）">查准率（Precision）</h4><p>第一个叫做<strong>查准率</strong>这个意思是，对于所有我们预测他们患有癌症的病人，有多大比率的病人是真正患有癌症的，让我把这个写下来：</p>
<p>$$<br>\frac{真阳性的数量} {预测值为阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阳性的数量}<br>$$</p>
<p>这个就叫做<strong>查准率</strong>，<strong>查准率</strong>越高就越好。这是说，对于那些病人，我们告诉他们：”非常抱歉，我们认为你得了癌症”，高查准率说明对于这类病人我们对预测他们得了癌症有很高的准确率。</p>
<h4 id="召回率（Recall）">召回率（Recall）</h4><p>另一个数字我们要计算的，叫做<strong>召回率</strong>，<strong>召回率</strong>是如果所有的在数据集中的病人（假设测试集中的病人，或者交叉验证集中的病人）确实得了癌症，有多大比率 我们正确预测他们得了癌症。召回率被定义为：</p>
<p>$$<br>\frac{真阳性的数量} {实际阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阴性的数量}<br>$$</p>
<p>同样地，召回率越高越好。</p>
<p>通过计算<strong>查准率</strong>和<strong>召回率</strong>我们能更好的知道分类模型到底好不好。</p>
<p>具体地说，如果我们有一个算法，总是预测<code>y=0</code>，即它总是预测没有人患癌症，那么这个分类模型的召回率等于0，因为它不会有<strong>真阳性</strong>。因此我们能会快发现这个分类模型不是一个好的模型。</p>
<p>总的来说，即使我们有一个非常偏斜的类，算法也不能够”欺骗”我们。我们能够更肯定的是：拥有高查准率或者高召回率的模型是一个好的分类模型。这给予了我们一个更好的评估值，给予我们一种更直接的方法来评估模型的好与坏。</p>
<p>最后一件需要记住的事，在查准率和召回率的定义中，我们总是习惯性地用<code>y=1</code>，因此如果我们试图检测某种很稀少的情况（比如癌症，我希望它是个很稀少的情况），查准率和召回率会被定义为<code>y=1</code>而不是<code>y=0</code>作为某种我们希望检测的出现较少的类。通过使用查准率和召回率，我们发现即使我们拥有非常偏斜的类，算法不能够通过总是预测<code>y=1</code>或<code>y=0</code>来”欺骗”我们。因为它不能够获得高查准率和召回率。具体地说，如果一个分类模型拥有高查准率和召回率，那么我们可以确信地说这个算法表现很好，即便它是一个很偏斜的类。</p>
<p>因此对于偏斜类的问题，查准率和召回率给予了我们更好的方法来检测学习算法表现如何，这是一种更好地评估学习算法的标准。当出现偏斜类时，比仅仅只用分类误差或者分类精度好。</p>
<h2 id="查准率和召回率练习">查准率和召回率练习</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/CuONQ/trading-off-precision-and-recall" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在之前的课程中，我们谈到<strong>查准率</strong>和<strong>召回率</strong>作为遇到偏斜类问题的评估度量值。在很多应用中，我们希望能够保证<strong>查准率</strong>和<strong>召回率</strong>的相对平衡。在这节课中，我将告诉你应该如何处理此类问题，同时也向你展示一些查准率和召回率 作为算法评估度量值的更有效的方式。</p>
</blockquote>
<p>回忆一下，这是我们在上一节中讲到的<strong>查准率</strong>和<strong>召回率</strong>的定义：</p>
<p>$$<br>查准率 = \frac{真阳性的数量} {预测值为阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阳性的数量}<br>$$</p>
<p>$$<br>召回率 = \frac{真阳性的数量} {实际阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阴性的数量}<br>$$</p>
<p>让我们继续用癌症分类的例子。</p>
<p>如果病人患癌症，则<code>y=1</code>反之则<code>y=0</code>。</p>
<p>假设我们用<strong>逻辑回归模型</strong>训练了数据，输出概率是在<code>0-1</code>之间的值：</p>
<p>$$<br>0\le h_{\theta}(x)\le1<br>$$</p>
<p>因此可以得到：</p>
<ul>
<li>如果$h_{\theta}(x)\ge0.5$，预测值为1 </li>
<li>如果$h_{\theta}(x)\lt0.5$，预测值为0 </li>
</ul>
<p>这个回归模型能够计算<strong>查准率</strong>和<strong>召回率</strong>。但是现在，假如我们希望在我们非常确信地情况下，才预测一个病人得了癌症。</p>
<blockquote>
<p>因为你知道如果你告诉一个病人，说他得了癌症，他会非常震惊，因为这是一个非常坏的消息，而且他们会经历一段非常痛苦的治疗过程，因此我们希望只有在我们非常确信的情况下，才告诉这个人他得了癌症。</p>
</blockquote>
<p>这样做的一种方法是修改算法。</p>
<h3 id="高查准率低召回率情况">高查准率低召回率情况</h3><p>我们不再将临界值设为<code>0.5</code>，也许我们只在$h_{\theta}(x)\ge0.7$的情况下，才预测$y=1$。因此我们会在我们认为他有$\ge70\%$得癌症的概率情况下，告诉一个人他得了癌症。</p>
<p>如果你这么做，那么你只在非常确信地情况下才去预测癌症，那么你的回归模型会有较高的<strong>查准率</strong>，因为在所有你准备告诉他们患有癌症的病人中，他们有较高的可能性真的患有癌症。</p>
<p>你预测患有癌症的病人中有较大比率的人，他们确实患有癌症。因为这是我们在非常确信的情况下做出的预测。</p>
<p>与之相反，这个回归模型会有较低的<strong>召回率</strong>，因为当我们做预测的时候，我们只给很小一部分的病人预测$y=1$，现在我们把这个情况夸大一下，我们不再把临界值 设在$0.7$，我们把它设为$0.9$，我们只在至少$90\%$肯定这个病人患有癌症的情况下才预测$y=1$。那么这些病人当中，有非常大的比率，真正患有癌症。因此这是一个<strong>高查准率</strong>的模型。但是<strong>召回率</strong>会变低，因为我们希望能够正确检测患有癌症的病人。</p>
<h3 id="高召回率低查准率情况">高召回率低查准率情况</h3><p>现在考虑一个不同的例子。</p>
<p>假设我们希望避免遗漏掉患有癌症的人，即我们希望避免<strong>假阴性</strong>。具体地说，如果一个病人实际患有癌症，但是我们并没有告诉他患有癌症，那这可能会造成严重后果。因为如果我们告诉病人他们没有患癌症，那么他们就不会接受治疗。但是如果他们真的患有癌症，我们又没有告诉他们，那么他们就根本不会接受治疗，这可能会造成严重后果，甚至使病人丧失生命，因为我们没有告诉他患有癌症，他没有接受治疗。这种情况下，我们希望预测病人患有癌症，即$y=1$。这样他们会做进一步的检测，然后接受治疗以避免他们真的患有癌症。</p>
<p>在这个例子中，我们不再设置高的<strong>临界值</strong>，我们会设置另一个值，将<strong>临界值</strong>设得较低，比如$0.3$。这样以来，我们认为这些病人有$\gt30\%$的概率患有癌症，我们以更加保守的方式来告诉他们患有癌症，因此他们能够接受治疗。但是在这种情况下，我们会有一个较<strong>高召回率</strong>的模型。因为在患有癌症的病人中，有很大一部分被我们正确标记出来了，但是我们会得到较低的查准率，因为我们预测患有癌症的病人比例越大，那么就有较大比例的人其实没有患癌症。</p>
<hr>
<p>顺带一提，当我在给别的学生讲这个的时候，令人惊讶的是有的学生问，怎么可以从两面来看这个问题？为什么我总是只想要高查准率或高召回率，但是这看起来可以使两边都提高。更普遍的一个原则是：这取决于你想要什么。你想要<strong>高查准率低召回率</strong>，还是<strong>高召回率低查准率</strong>？你可以预测$y=1$当$h_\theta(x)\ge临界值$。因此总的来说，对于大多数的回归模型，你得权衡查准率和召回率。</p>
<h3 id="临界值的选取">临界值的选取</h3><p>当你改变临界值的值时，你可以画出曲线来权衡查准率和召回率：</p>
<p><img src="/img/17_01_19/002.png" width="300" height="200" align="center"></p>
<p>这里的一个值，反应出一个较高的临界值，这个临界值可能等于$0.99$，我们假设 只在有大于$99\%$的确信度的情况下，才预测$y=1$。至少，有$99\%$的可能性。因此这个点反应<strong>高查准率低召回率</strong>：</p>
<p><img src="/img/17_01_19/003.png" width="300" height="200" align="center"></p>
<p>然而这里的一个点，反映一个较低的临界值，比如说$0.01$，毫无疑问，在这里预测$y=1$，如果你这么做，你最后会得到<strong>低查准率高的召回率</strong>的预测结果：</p>
<p><img src="/img/17_01_19/004.png" width="300" height="200" align="center"></p>
<p>当你改变临界值时，如果你愿意，你可以画出回归模型的所有曲线，来看看你能得到的查准率和召回率的范围。</p>
<p>顺带一提，查准率-召回率曲线可以是各种不同的形状，有时它看起来是这样：</p>
<p><img src="/img/17_01_19/005.png" width="300" height="200" align="center"></p>
<p>有时是那样：</p>
<p><img src="/img/17_01_19/006.png" width="300" height="200" align="center"></p>
<p>查准率-召回率曲线的形状有很多可能性，这取决于回归模型的具体算法。因此这又产生了另一个有趣的问题，那就是<strong>有没有办法自动选取临界值</strong>，或者更广泛地说，如果我们有不同的算法，我们如何比较不同的查准率和召回率呢？</p>
<h4 id="选取临界值的方式：$F_{1}Score$(F_score)">选取临界值的方式：$F_{1}Score$(F score)</h4><p>具体来说，假设我们有三个不同的学习算法；或者这三个不同的学习曲线是同样的算法，但是临界值不同。我们怎样决定哪一个算法是最好的？</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">查准率(P)</th>
<th style="text-align:center">召回率(R)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">算法1</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">算法2</td>
<td style="text-align:center">0.7</td>
<td style="text-align:center">0.1</td>
</tr>
<tr>
<td style="text-align:center">算法3</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">1.0</td>
</tr>
</tbody>
</table>
<p>我们之前讲到的其中一件事就是评估度量值的重要性。这个概念是通过一个具体的数字来反映你的回归模型到底如何。但是查准率和召回率的问题我们却不能这样做，因为在这里我们有两个可以判断的数字。</p>
<p>因此我们经常会不得不面对这样的情况：如果我们正在试图比较<strong>算法1</strong>和<strong>算法2</strong>，我们最后问自己，到底是<code>0.5</code>的查准率与<code>0.4</code>的召回率好；还是说<code>0.7</code>的查准率与<code>0.1</code>的召回率好？如果你最后这样坐下来思考，这回降低你的决策速度。思考到底哪些改变是有用的，应该被融入到你的算法中。</p>
<p>与此相反的是，如果我们有一个评估度量值，一个数字，能够告诉我们到底是算法1好还是算法2好，这能够帮助我们更快地决定哪一个算法更好，同时也能够更快地帮助我们评估不同的改动，哪些应该被融入进算法里面。那么我们怎样才能得到这个评估度量值呢？</p>
<p>你可能会去尝试的一件事情是计算一下查准率和召回率的平均值:</p>
<p>$$<br>平均值：\frac{P+R} {2}<br>$$</p>
<p>用<strong>P</strong>和<strong>R</strong>来表示查准率和召回率，你可以做的是计算它们的平均值，看一看哪个模型有最高的均值:</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">查准率(P)</th>
<th style="text-align:center">召回率(R)</th>
<th style="text-align:center">平均值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">算法1</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">0.4</td>
<td style="text-align:center">0.45</td>
</tr>
<tr>
<td style="text-align:center">算法2</td>
<td style="text-align:center">0.7</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">算法3</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.51</td>
</tr>
</tbody>
</table>
<p>但是这可能并不是一个很好的解决办法，因为像我们之前的例子一样,如果我们的回归模型总是预测$y=1$，这么做你可能得到非常高的召回率得到非常低的查准率；相反地，如果你的模型总是预测$y=0$，就是说如果很少预测$y=1$，对应的设置了一个高临界值，最后你会得到非常高的查准率和非常低的召回率。</p>
<p>这两个极端情况一个有<strong>非常高的临界值</strong>，一个有<strong>非常低的临界值</strong>，它们中的任何一个都不是一个好的模型，我们可以通过非常低的查准率，或者非常低的召回率来判断这不是一个好模型。</p>
<p>在这里如果使用平均值的方式来计算，那么<strong>算法3</strong>的计算结果是最高的，但这并不是一个好模型，因为你总是预测$y=1$，这并不是一个有用的模型，因为它只能输出$y=1$。</p>
<p>所以，我们认为通过计算查准率和召回率平均值的方式来评估算法的好坏，不是一个好方法。</p>
<hr>
<p>相反的，有一种结合查准率和召回率的另一种方式，叫做<strong>F score</strong>，公式如下：</p>
<p>$$<br>F_{1}Score : 2\frac{PR} {P+R}<br>$$</p>
<p>通过<strong>F score</strong>方法我们可以得出：</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">查准率(P)</th>
<th style="text-align:center">召回率(R)</th>
<th style="text-align:center">平均值</th>
<th style="text-align:center">$F_{1}Score$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">算法1</td>
<td style="text-align:center">0.5</td>
<td style="text-align:center">0.4</td>
<td style="text-align:center">0.45</td>
<td style="text-align:center">0.444</td>
</tr>
<tr>
<td style="text-align:center">算法2</td>
<td style="text-align:center">0.7</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">0.4</td>
<td style="text-align:center">0.175</td>
</tr>
<tr>
<td style="text-align:center">算法3</td>
<td style="text-align:center">0.02</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.51</td>
<td style="text-align:center">0.0392</td>
</tr>
</tbody>
</table>
<p>可以看出<strong>算法1</strong>的<strong>F score</strong>最高，所以我们会选择使用<strong>算法1</strong>。</p>
<blockquote>
<p><strong>F score</strong>也叫作$F_{1}Score$，但通常人们叫它<strong>F score</strong>。</p>
</blockquote>
<p><strong>F score</strong>的定义会考虑一部分查准率和召回率的平均值，但是它会给查准率和召回率中较低的值更高的权重，因此你可以看到<strong>F score</strong>的分子是查准率和召回率的乘积，如果查准率等于0或者召回率等于0，<strong>F score</strong>也会等于0。因此它结合了查准率和召回率，对于一个较大的<strong>F score</strong>，查准率和召回率都必须较大。</p>
<p>我必须说明的是，有较多的公式可以结合查准率和召回率，<strong>F score</strong>公式只是 其中一个，但是出于历史原因和习惯问题，人们在机器学习中普遍使用<strong>F score</strong>。<strong>F score</strong>这个术语没有什么特别的意义，所以不要担心它到底为什么叫做<strong>F score</strong>或者$F_{1}Score$。</p>
<p><strong>F score</strong>给出了你所需要的有效方法，因为无论是查准率等于0，还是召回率等于0，它都会得到一个很低的<strong>F score</strong>。因此，如果要得到一个很高的<strong>F score</strong>，你的算法的查准率和召回率都要接近于1。具体地说，如果$P=0$或者$R=0$，你的<strong>F score</strong>也会等于0。</p>
<blockquote>
<p>在这次的视频中，我们讲到了如何权衡查准率和召回率，以及我们如何变动临界值 来决定我们希望预测$y=1$还是$y=0$。比如我们需要一个$70\%$还是$90\%$置信度的临界值，或者别的，来预测$y=1$。</p>
<p>通过变动临界值，你可以控制权衡查准率和召回率。</p>
<p>之后我们讲到了<strong>F score</strong>，它权衡查准率和召回率，给了你一个评估度量值。当然，如果你的目标是自动选择临界值来决定你希望预测$y=1$还是$y=0$，那么一个比较理想的办法是试一试不同的临界值，然后评估这些不同的临界值在交叉检验集上进行测试，然后选择哪一个临界值能够在交叉检验集上得到最高的<strong>F score</strong>，这是自动选择临界值的较好办法。</p>
</blockquote>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/">&raquo;</a>
  </nav>

 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://dannylee1991.github.io/images/avatar.jpg" alt="DannyLee佳楠" itemprop="image"/>
          <p class="site-author-name" itemprop="name">DannyLee佳楠</p>
        </div>
        <p class="site-description motion-element" itemprop="description">一只在迈向机器学习道路上狂奔的程序猿.</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">95</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">16</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">18</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DannyLee1991" target="_blank">GitHub</a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee佳楠</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dannylee1991"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
