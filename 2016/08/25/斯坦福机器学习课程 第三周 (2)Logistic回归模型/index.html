<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿." />



  <meta name="keywords" content="机器学习,斯坦福课程," />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="逻辑回归的代价函数视频地址
这节课将讲解逻辑回归中，如何定义用来拟合参数的代价函数。

如上图所示，我们现在有一组训练集，其中有$m$个训练样本，由于是一个分类问题，我们的$y$取值范围就是0和1。并且我们给出了假设函数$h_{\theta}(x)$。现在的问题就是，我们如何确定我们的参数$\theta$的值呢？也就是说如何拟合参数$\theta$呢？

上图是之前求线性回归问题时的代价函数，现在">
<meta property="og:type" content="article">
<meta property="og:title" content="斯坦福机器学习课程 第三周 (2)Logistic回归模型">
<meta property="og:url" content="http://dannylee1991.github.io/2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/index.html">
<meta property="og:site_name" content="DannyLee">
<meta property="og:description" content="逻辑回归的代价函数视频地址
这节课将讲解逻辑回归中，如何定义用来拟合参数的代价函数。

如上图所示，我们现在有一组训练集，其中有$m$个训练样本，由于是一个分类问题，我们的$y$取值范围就是0和1。并且我们给出了假设函数$h_{\theta}(x)$。现在的问题就是，我们如何确定我们的参数$\theta$的值呢？也就是说如何拟合参数$\theta$呢？

上图是之前求线性回归问题时的代价函数，现在">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/001.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/002.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/003.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/004.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/005.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/006.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/007.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/008.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/009.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/010.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/011.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/012.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/013.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/014.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/015.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/016.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/018.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/019.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/020.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/021.png">
<meta property="og:image" content="http://dannylee1991.github.io/img/16_08_25/022.png">
<meta property="og:updated_time" content="2016-09-04T15:02:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="斯坦福机器学习课程 第三周 (2)Logistic回归模型">
<meta name="twitter:description" content="逻辑回归的代价函数视频地址
这节课将讲解逻辑回归中，如何定义用来拟合参数的代价函数。

如上图所示，我们现在有一组训练集，其中有$m$个训练样本，由于是一个分类问题，我们的$y$取值范围就是0和1。并且我们给出了假设函数$h_{\theta}(x)$。现在的问题就是，我们如何确定我们的参数$\theta$的值呢？也就是说如何拟合参数$\theta$呢？

上图是之前求线性回归问题时的代价函数，现在">
<meta name="twitter:image" content="http://dannylee1991.github.io/img/16_08_25/001.png">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>

<!--baidu统计-->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2f967e5ec4f276411160d27aeace7722";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  <title> 斯坦福机器学习课程 第三周 (2)Logistic回归模型 | DannyLee </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">DannyLee</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu menu-left">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      

      
      
        <li class="menu-item menu-item-search">
          <a href="#" class="st-search-show-outputs">
            <i class="menu-item-icon icon-next-search"></i> <br />
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'ss9-_Hsd4DyhyGw4m99P','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              斯坦福机器学习课程 第三周 (2)Logistic回归模型
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-08-25T23:54:00+08:00" content="2016-08-25">
            2016-08-25
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><h2 id="逻辑回归的代价函数">逻辑回归的代价函数</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function" target="_blank" rel="external">视频地址</a></p>
<p>这节课将讲解逻辑回归中，如何定义用来拟合参数的<strong>代价函数</strong>。</p>
<p><img src="/img/16_08_25/001.png" alt=""></p>
<p>如上图所示，我们现在有一组训练集，其中有$m$个训练样本，由于是一个分类问题，我们的$y$取值范围就是0和1。并且我们给出了假设函数$h_{\theta}(x)$。现在的问题就是，我们如何确定我们的参数$\theta$的值呢？也就是说如何<strong>拟合参数$\theta$</strong>呢？</p>
<p><img src="/img/16_08_25/002.png" alt=""></p>
<p>上图是之前求线性回归问题时的代价函数，现在我将上图的公式进行改写：</p>
<p>$$<br>Cost(h_{\theta}(x^{(i)}),y^{(i)})=<br>\frac{1}{2}(h_{θ}(x^{(i)})-y^{(i)})^{2}<br>$$</p>
<p>为了简写方便，去掉上标之后的代价函数如下：</p>
<p>$$<br>Cost(h_{\theta}(x),y)=<br>\frac{1}{2}(h_{θ}(x)-y)^{2}<br>$$</p>
<p>在线性回归问题中，代价函数会被定义为以上这种形式，但在逻辑回归里，如果我们最小化代价函数后，使用这个最小代价的值也是可以正常工作的，但是我们的这个代价函数会变成参数$\theta$的非凸函数。</p>
<p>因为这里：</p>
<p>$$<br>\begin{align*}<br>h_{\theta}(x)<br>= \frac{1}{1+e^{-\theta^{T}x}}<br>\end{align*}<br>$$ </p>
<p>$h_{\theta}(x)$是非线性的，如果将这个值带入Cost函数，然后在求代价函数$J(\theta)$，我们会得到如下的有多个局部最优的函数：</p>
<p><img src="/img/16_08_25/003.png" alt=""></p>
<p>这种函数的官方名称叫做：<strong>非凸函数</strong>。</p>
<p>你大概可以感觉到如果把梯度下降算法用在这样一个函数上，不能保证它会收敛到全局最小值。</p>
<p>相应的，我们期望我们的代价函数$J(\theta)$是一个凸函数：</p>
<p><img src="/img/16_08_25/004.png" alt=""></p>
<p>对于这样的函数，我们使用梯度下降算法，才能够保证顺利的收敛到最小值。</p>
<p>这里导致我们的代价函数是非凸函数的根本原因就在于使用逻辑回归的那一步：</p>
<p>$$<br>\begin{align*}<br>h_{\theta}(x)<br>= \frac{1}{1+e^{-\theta^{T}x}}<br>\end{align*}<br>$$ </p>
<p>所以我们想做的就是，<strong>另找一个是凸函数的代价函数</strong>使得我们能够方便的找到代价函数的全局最小值。</p>
<p>因此对于<strong>逻辑回归的代价函数</strong>，形式如下：</p>
<p><img src="/img/16_08_25/005.png" alt=""></p>
<p>对应的函数的图形如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$y=1$时</th>
<th style="text-align:center">$y=0$时</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/16_08_25/006.png" alt=""></td>
<td style="text-align:center"><img src="/img/16_08_25/007.png" alt=""></td>
</tr>
</tbody>
</table>
<p>现在这种形式的代价函数，有着很多很有趣的性质：</p>
<ul>
<li>在$y=1$的情况下：<ul>
<li>如果假设函数$h_{\theta}(x) = 1$，那么代价函数为0:$Cost=0$。</li>
<li>如果假设函数$h_{\theta}(x) \rightarrow 0$，那么代价函数趋近于∞:$Cost \rightarrow ∞$</li>
</ul>
</li>
<li>在$y=0$的情况下：<ul>
<li>如果假设函数$h_{\theta}(x) \rightarrow 1$，那么代价函数为0:$Cost \rightarrow ∞ $。</li>
<li>如果假设函数$h_{\theta}(x) = 0$，那么代价函数等于0:$Cost = 0$</li>
</ul>
</li>
</ul>
<p>这就是我们所期望的效果：<strong>当假设函数和期望值越接近时，代价函数值越小，反之越大。</strong></p>
<h2 id="简化代价函数以及梯度下降">简化代价函数以及梯度下降</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent" target="_blank" rel="external">视频地址</a></p>
<p>这节课我们来讲解一种<strong>稍微简单一些的方式来写代价函数</strong>，来替代我们现有的方法。同时也要讲清楚，<strong>如何运用梯度下降法来拟合出逻辑回归的参数$\theta$</strong>。因此，听了这节课，你应该就能够实现一个完整的逻辑回归算法了。</p>
<h3 id="简化单个样本的代价函数$Cost(h_{\theta}(x),y)$">简化单个样本的代价函数$Cost(h_{\theta}(x),y)$</h3><p>之前我们讲解了逻辑回归中的代价函数：</p>
<p><img src="/img/16_08_25/008.png" alt=""></p>
<p>上图中有：</p>
<ul>
<li>我们定义的逻辑回归的整体的代价函数$J(\theta)$</li>
<li>针对于单个样本的代价函数$Cost(h_{\theta}(x),y)$</li>
<li>并且我们知道，作为分类问题，我们的$y$的取值永远都是0或者1。</li>
</ul>
<p>其中对于单个样本的代价函数$Cost(h_{\theta}(x),y)$，我们可以把$y=0$和$y=1$的两种情况合二为一，写成一个统一的式子：</p>
<p>$$<br>Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))<br>$$</p>
<p>这个表达式和我们之前写的代价函数的表达式，效果是完全一样的，但后者更加紧凑。</p>
<blockquote>
<p>我们知道在上面的式子中，$y$的取值非0即1，如果$y=1$那么$Cost(h_{\theta}(x),y)=-log(h_{\theta}(x))$，如果$y=0$那么$Cost(h_{\theta}(x),y)=-log(1-h_{\theta}(x))$。效果和分开写是完全一样的。</p>
</blockquote>
<h3 id="拟合逻辑回归的参数$\theta$">拟合逻辑回归的参数$\theta$</h3><p>这样一来，我们就可以写出完整的逻辑回归的代价函数了：</p>
<p><img src="/img/16_08_25/009.png" alt=""></p>
<blockquote>
<p>似乎我们有其他方式来表示单个样本的代价函数$Cost(h_{\theta}(x),y)$，但在这里不做详细的说明，但是我可以告诉你，现在我们使用的这种形式的代价函数的式子，是从统计学中的<strong>极大似然法</strong>得来的。不过如果你不理解什么是<strong>极大似然法</strong>，以及逻辑回归中的代价函数为什么是这种形式，也没关系，也不会影响你后面的学习。</p>
</blockquote>
<p>接下来的问题就是找到使得代价函数$j(\theta)$最小的参数$\theta$了：</p>
<p><img src="/img/16_08_25/010.png" alt=""></p>
<p>一旦我们得到了这个合适的$\theta$那么我们就可以对一个给定的样本进行预测了：</p>
<p><img src="/img/16_08_25/011.png" alt=""></p>
<blockquote>
<p>注意：$p(y=1|x;\theta)$就是指关于$x$以$\theta$为参数，$y=1$的概率。</p>
</blockquote>
<p>由此可见，我们问题的核心在于求解最小化代价函数：</p>
<p><img src="/img/16_08_25/012.png" alt=""></p>
<p>上图就是逻辑回归问题中最小化代价函数的方法，是通过梯度下降算法来实现。</p>
<p>对上图中的偏导数项展开后就是下面的形式：</p>
<p><img src="/img/16_08_25/013.png" alt=""></p>
<p>如果你把上面的式子放到线性回归问题中进行对比的话，<strong>你会发现这个式子正是用来做线性回归梯度下降的。</strong></p>
<p>那么我们不禁要问一下自己，线性回归算法和逻辑回归算法是一样的吗？</p>
<p>要回答这个问题，就要看从线性回归，到逻辑回归，到底发生了哪些变化。</p>
<p>实际上对于假设函数$h_{\theta}(x)$的定义发生了变化：</p>
<ul>
<li>线性回归中，假设函数定义为$h_{\theta}(x)=\theta^{T}x$</li>
<li>在逻辑回归中，假设函数的定义为$h_{\theta}(x)= \frac{1}{1+e^{-\theta^{T}x}}$</li>
</ul>
<p>由于假设函数的不同，实际上线性回归和逻辑回归的梯度下降是两个完全不同的东西。</p>
<p>当我们在讨论线性回归的梯度下降算法时，我们谈到了如何监控梯度下降法以确保其收敛。其实我们通常也把这种方法运用在逻辑回归中，来检测梯度下降，以确保其正常收敛。这里就不多做介绍，可以参考<strong>斯坦福机器学习课程 第一周 (5)参数学习-梯度下降算法</strong>来实现。</p>
<h2 id="高级优化">高级优化</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization" target="_blank" rel="external">视频地址</a></p>
<p>这节课中会介绍一些高级优化概念和技巧。利用这些技巧，我们就使通过梯度下降进行逻辑回归的速度大大提高，这也将使得算法更加适合解决大型的机器学习问题。</p>
<h3 id="优化算法概念引入">优化算法概念引入</h3><p>现在我们换个角度来看看什么是梯度下降：</p>
<p>我们有个代价函数$J(\theta)$，并且我们希望最小化代价函数$min_{\theta}J(\theta)$。</p>
<p>为了得到最小化代价函数$min_{\theta}J(\theta)$，我们需要计算出$J(\theta)$关于每一项$\theta$的偏导：</p>
<p><img src="/img/16_08_25/014.png" alt=""></p>
<p>有了上面的$J(\theta)$关于每一项$\theta$的偏导，为了求得合适的$\theta$，我们就可以利用梯度下降算法来递减$\theta$，使其接近全局最小值了：</p>
<p><img src="/img/16_08_25/015.png" alt=""></p>
<p>但事实上，<strong>梯度下降并不是我们可以使用的唯一算法</strong>，下面是我们常用的几种算法：</p>
<ul>
<li><strong>梯度下降算法(Gradient descent)</strong></li>
<li><strong>共轭梯度法(Conjugate gradient)</strong></li>
<li><strong>变尺度法(BFGS)</strong></li>
<li><strong>限制变尺度法(L-BFGS)</strong></li>
</ul>
<p>除了我们常说的梯度下降算法外的其他三种算法，都有着更复杂的方式来求最小化代价函数。但是这三种算法的细节，超出了本门课程的范畴(实际上如果你想要研究这些算法，可能你需要花费几周的时间，你可以专门学一门课程来提高数值计算能力)。不过我会告诉你他们的一些特性。</p>
<h3 id="优缺点">优缺点</h3><p>这三种算法有许多优点：</p>
<p><strong>优点</strong>：</p>
<ul>
<li>不需要手动选择学习率$a$<ul>
<li>所以对这些算法的一种思路是 给出计算导数项和代价函数$J(\theta)$的方法，你可以认为算法有一个智能的内部循环，这种智能的内部循环被称为<strong>线性搜索(line search)</strong>算法，它可以自动尝试不同的学习速率$a$，并自动选择一个好的学习速率$a$。因此它甚至可以在每次迭代时都选择不同的学习速率，那么你就不需要自己选择$a$了。</li>
</ul>
</li>
<li>通常情况下收敛速度比梯度下降算法更快<ul>
<li>这些算法实际上在做更复杂的事情，而不仅仅是选择一个好的学习速率$a$，所以他们往往最终收敛的远远快于梯度下降。不过关于它们是如何做到的细节，已经超过了本门课程的范畴。</li>
</ul>
</li>
</ul>
<blockquote>
<p>实际上，我过去十年我都在相当频繁的使用这几种算法。但直到前几年，我才真正搞清楚共轭梯度法、BFGS和L-BFGS的细节。因此，实际上你完全可以在不了解这些算法的内环间在做什么的的情况下使用这些算法，并应用于许多不同的学习问题。</p>
</blockquote>
<p>如果说这些算法优缺点的话，那么它的缺点就是：</p>
<p><strong>缺点</strong></p>
<ul>
<li>比梯度下降法复杂多了<ul>
<li>如果你不是数值计算方面的专家，不建议你亲自去写共轭梯度法、BFGS和L-BFGS，而是去使用一些现成的库。实际上Octave和Matlab都有很理想的库来实现这些先进的优化算法。值得一提的是，这些算法在不同语言上的实现，是有差别的，有的语言上的库可能会表现的不太好。所以建议对比选择合适的实现。</li>
</ul>
</li>
</ul>
<h3 id="使用实例">使用实例</h3><p>接下来来举例说明如何使用这些算法。</p>
<p>比如你有一个包含两个参数的问题：</p>
<p><img src="/img/16_08_25/016.png" alt=""></p>
<p>对于上图中的代价函数，我们可以求得当$\theta_{1}=5$,$\theta_{2}=5$时，$J(\theta)$的值最小。</p>
<p>下面就看看怎么使用Octave程序来计算$\theta_{1}$和$\theta_{2}$吧。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">function [jVal, gradient] = costFunction(theta)</div><div class="line"></div><div class="line">jVal = (theta(<span class="number">1</span>)<span class="number">-5</span>)^<span class="number">2</span> + (theta(<span class="number">2</span>)<span class="number">-5</span>)^<span class="number">2</span>;</div><div class="line"></div><div class="line">gradient = zeros(<span class="number">2</span>,<span class="number">1</span>);</div><div class="line">gradient(<span class="number">1</span>) = <span class="number">2</span>*(theta(<span class="number">1</span>)<span class="number">-5</span>);</div><div class="line">gradient(<span class="number">2</span>) = <span class="number">2</span>*(theta(<span class="number">2</span>)<span class="number">-5</span>);</div></pre></td></tr></table></figure>
<p>上面这段代码定义了一个函数<code>costFunction(theta)</code>，参数是一个<code>theta</code>向量，包含$\theta_{1}$和$\theta_{2}$，返回值是<code>jVal</code>和<code>gradient</code>。通过函数的定义，我们可以看出来<code>jVal</code>表示的是代价函数$J(\theta)$：</p>
<p><img src="/img/16_08_25/018.png" alt=""></p>
<p><code>gradient</code>是一个列量，其中两个值分别代表对$\theta_{1}$和$\theta_{2}$的偏导：</p>
<p><img src="/img/16_08_25/019.png" alt=""></p>
<p>接下来，调用高级优化函数<code>fminunc</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="string">'100'</span>);</div><div class="line">initialTheta = zeros(2,1);</div><div class="line">[optTheta, <span class="keyword">function</span>Val, <span class="built_in">exit</span>Flag] = fminunc(@costFunction, initialTheta, options);</div></pre></td></tr></table></figure>
<p><code>fminunc</code>表示Octave里无约束最小化函数，调用这个函数时，需要传入一个存有配置信息的变量<code>options</code>。上面的代码中，我们的设置项中<code>&#39;GradObj&#39;, &#39;on&#39;,</code>代表设置梯度目标参数为打开状态(on)，这也意味着你现在确实要给这个算法提供一个梯度。<code>&#39;MaxIter&#39;, &#39;100&#39;</code>代表设置最大迭代次数为100次。<code>initialTheta</code>代表我们给出的一个$\theta$的猜测初始值。</p>
<p>然后我们调用<code>fminunc</code>这个函数，传入三个参数，其中第一个参数<code>@costFunction</code>这里的<code>@</code>符号代表指向之前我们定义的<code>costFunction</code>函数的指针。后面两个参数分别是我们定义的$theta$初始值和配置信息<code>options</code>。</p>
<p>当我们调用这个<code>fminunc</code>函数时，它会自动的从众多高级优化算法中挑选一个来使用(你也可以把它当做一个可以自动选择合适的学习速率$a$的梯度下降算法)。</p>
<p>最终我们会得到三个返回值，分别是满足最小化代价函数$J(\theta)$的$\theta$值<code>optTheta</code>，<code>costFunction</code>中定义的<code>jVal</code>的值<code>functionVal</code>，以及标记是否已经收敛的状态值<code>exitFlag</code>，如果已收敛，标记为<code>1</code>，否则为<code>0</code>。</p>
<p>下面我们看看在Octave中具体怎么操作：</p>
<p>首先在当前工作目录下创建costFunction.m脚本，写入costFunction函数:</p>
<p><img src="/img/16_08_25/020.png" alt=""></p>
<p>然后在Octave终端下带入<code>fminunc</code>函数：</p>
<p><img src="/img/16_08_25/021.png" alt=""></p>
<h3 id="在逻辑回归中使用优化算法">在逻辑回归中使用优化算法</h3><p>上面的例子是一个简单的二次函数中使用优化算法的例子，下面我们介绍一些在逻辑回归中使用这些算法：</p>
<p><img src="/img/16_08_25/022.png" alt=""></p>
<p>上图就是逻辑回归问题的代价函数的求解代码。可以看到大体流程和之前的例子是一致的，你只需要把相应的计算<code>jVal</code>以及各个<code>gradient</code>的代码，按照逻辑回归的规则填写完整即可。</p>
<blockquote>
<p>使用了这些复杂的优化算法之后，虽然它将使得调试变得更为困难，但是这些算法的速度远远大于普通的梯度下降算法的速度，因此可以用来处理很大的机器学习问题的时候，可以使用这些高级算法，而不是梯度下降算法。</p>
</blockquote>
</span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag">#机器学习</a>
          
            <a href="/tags/斯坦福课程/" rel="tag">#斯坦福课程</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/09/04/斯坦福机器学习课程 第三周 (3)多类别分类问题：一对多/" rel="prev">斯坦福机器学习课程 第三周 (3)多类别分类问题：一对多</a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/08/21/斯坦福机器学习课程 第三周 (1)分类和表达式/" rel="next">斯坦福机器学习课程 第三周 (1)分类和表达式</a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
              <div class="ds-thread" data-thread-key="2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/"
                   data-title="斯坦福机器学习课程 第三周 (2)Logistic回归模型" data-url="http://dannylee1991.github.io/2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/">
              </div>
            
          </div>
        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://dannylee1991.github.io/images/avatar.jpg" alt="DannyLee佳楠" itemprop="image"/>
          <p class="site-author-name" itemprop="name">DannyLee佳楠</p>
        </div>
        <p class="site-description motion-element" itemprop="description">一只在迈向机器学习道路上狂奔的程序猿.</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">114</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">16</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">19</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DannyLee1991" target="_blank">GitHub</a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑回归的代价函数"><span class="nav-number">1.</span> <span class="nav-text">逻辑回归的代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#简化代价函数以及梯度下降"><span class="nav-number">2.</span> <span class="nav-text">简化代价函数以及梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简化单个样本的代价函数$Cost(h_{\theta}(x),y)$"><span class="nav-number">2.1.</span> <span class="nav-text">简化单个样本的代价函数$Cost(h_{\theta}(x),y)$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#拟合逻辑回归的参数$\theta$"><span class="nav-number">2.2.</span> <span class="nav-text">拟合逻辑回归的参数$\theta$</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高级优化"><span class="nav-number">3.</span> <span class="nav-text">高级优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#优化算法概念引入"><span class="nav-number">3.1.</span> <span class="nav-text">优化算法概念引入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#优缺点"><span class="nav-number">3.2.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用实例"><span class="nav-number">3.3.</span> <span class="nav-text">使用实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在逻辑回归中使用优化算法"><span class="nav-number">3.4.</span> <span class="nav-text">在逻辑回归中使用优化算法</span></a></li></ol></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee佳楠</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dannylee1991"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.1" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"slide":{"type":"slide","bdImg":"3","bdPos":"left","bdTop":"250"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>



  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
