<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿." />



  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta property="og:type" content="website">
<meta property="og:title" content="DannyLee">
<meta property="og:url" content="http://dannylee1991.github.io/page/6/index.html">
<meta property="og:site_name" content="DannyLee">
<meta property="og:description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DannyLee">
<meta name="twitter:description" content="一只在迈向机器学习道路上狂奔的程序猿.">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>

<!--baidu统计-->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2f967e5ec4f276411160d27aeace7722";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  <title> DannyLee </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">DannyLee</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu menu-left">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      

      
      
        <li class="menu-item menu-item-search">
          <a href="#" class="st-search-show-outputs">
            <i class="menu-item-icon icon-next-search"></i> <br />
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'ss9-_Hsd4DyhyGw4m99P','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/10/16/斯坦福机器学习课程 第五周 (4)神经网络实现自动驾驶/" itemprop="url">
                斯坦福机器学习课程 第五周 (4)神经网络实现自动驾驶
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-10-16T00:18:58+08:00" content="2016-10-16">
            2016-10-16
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/10/16/斯坦福机器学习课程 第五周 (4)神经网络实现自动驾驶/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/10/16/斯坦福机器学习课程 第五周 (4)神经网络实现自动驾驶/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="使用神经网络实现自动驾驶">使用神经网络实现自动驾驶</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/zYS8T/autonomous-driving" target="_blank" rel="external">视频地址</a></p>
<p>这一节将介绍一个具有历史意义的神经网络学习的例子，那就是使用神经网络来实现自动驾驶。也就是说汽车通过学习来自己驾驶。</p>
<p>这是Dean Pomerleau那里得到的一段视频，Dean Pomerleau任职于美国东海岸的卡耐基梅隆大学，下面的视频中你会明白可视化技术到底是什么。</p>
<p><img src="/img/16_10_16/001.png" alt=""></p>
<p>在上面这张图中，左下方就是汽车所看到的前方路况图像，是汽车前方摄像头每两秒采集的并且进过压缩处理之后得到的一张30 × 32的图像。这张图片中，你依稀可以看到一条道路。</p>
<p><img src="/img/16_10_16/002.png" alt=""></p>
<p>左上角第一个水平的进度条显示的是驾驶员所选择的方向，就是那条白亮的区段显示的就是人类驾驶者选择的方向，白色区段偏左，就是向左转，反之，则是向右转。</p>
<p><img src="/img/16_10_16/003.png" alt=""></p>
<p>第二个水平的进度条则是学习算法选出的行驶方向，代表的含义和第一个条一样，白色区段偏左，就是向左转，反之，则是向右转。</p>
<p><img src="/img/16_10_16/004.png" alt=""></p>
<p>实际上神经网络在开始学习之前，你会看到网络的输出是一条灰色的区段，覆盖着整个区域，只有在学习算法运行足够长的时间之后，亮白色的区段才能逐渐显现。</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ilP4aPDTBPE" frameborder="0" allowfullscreen></iframe>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/10/13/斯坦福机器学习课程 第五周 (2)BP算法练习/" itemprop="url">
                斯坦福机器学习课程 第五周 (2)BP算法练习
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-10-13T00:08:00+08:00" content="2016-10-13">
            2016-10-13
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/10/13/斯坦福机器学习课程 第五周 (2)BP算法练习/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/10/13/斯坦福机器学习课程 第五周 (2)BP算法练习/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="将参数从矩阵展开成向量">将参数从矩阵展开成向量</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/60Uxp/implementation-note-unrolling-parameters" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在这一节中，我想快速地向你介绍一个细节的实现过程，怎样把你的参数从矩阵展开成向量，以便我们在高级最优化步骤中的使用需要。</p>
</blockquote>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jVal, gradient]</span> = <span class="title">costFunction</span><span class="params">(theta)</span></span></div><div class="line">	...</div><div class="line">optTheta = fminunc(@costFunction, initialTheta, options)</div></pre></td></tr></table></figure>
<p>具体来讲，对代价函数<code>costFunction(theta)</code>传入参数<code>theta</code>，函数返回值是代价函数<code>jVal</code>以及导数值<code>gradient</code>，然后你可以将返回值传递给高级最优化算法<code>fminunc</code>。</p>
<blockquote>
<p>顺便提一下，<code>fminunc</code>并不是唯一的算法，你也可以使用别的优化算法，</p>
</blockquote>
<p>其中<code>costFunction</code>中的参数、返回值<code>gradient</code>以及<code>fiminunc</code>的参数<code>initialTheta</code>都是一个$R^{n+1}$阶的向量。</p>
<p><img src="/img/16_10_13/001.png" alt=""></p>
<p>这部分在我们使用逻辑回归的时候运行顺利，但现在对于神经网络，我们的参数将不再是向量，而是矩阵了。</p>
<p>以一个拥有4层的完整的神经网络为例：</p>
<p>其参数<code>theta</code>所代表的参数矩阵为矩阵为$Θ^{(1)}$,$Θ^{(2)}$,$Θ^{(3)}$，在Octave中，我们可以设为<code>Theta1</code>,<code>Theta2</code>,<code>Theta3</code>。</p>
<p><img src="/img/16_10_13/003.png" alt=""></p>
<p><img src="/img/16_10_13/002.png" alt=""></p>
<p>类似的，这些梯度项<code>gradient</code>也是<code>costFunction</code>的返回值之一，在之前的视频中我们演示了如何计算这些梯度矩阵，它们的计算结果是$D^{(1)}$,$D^{(2)}$,$D^{(3)}$，在Octave中用<code>D1</code>,<code>D2</code>,<code>D3</code>来表示。</p>
<p><img src="/img/16_10_13/005.png" alt=""></p>
<p><img src="/img/16_10_13/004.png" alt=""></p>
<p>在这一节中，我想很快地向你介绍怎样取出这些矩阵，并将他们展开成向量，以便它们最终成为恰当的格式，能够传入这里的<code>initialTheta</code>:</p>
<p><img src="/img/16_10_13/006.png" alt=""></p>
<p>并且得到正确的梯度返回值<code>gradient</code>。</p>
<hr>
<p>具体来说，假设我们有这样一个神经网络：</p>
<p><img src="/img/16_10_13/007.png" alt=""></p>
<p>其输入层有10个输入单元($s_{1}=10$)，隐藏层有10个单元($s_{2}=10$)，最后的输出层只有一个输出单元($s_{3}=1$)。</p>
<p>在这种情况下矩阵$Θ$的维度，和矩阵$D$的维度将有这个神经网络的结构所决定，比如$Θ^{(1)}$是一个$10 × 11$的矩阵。</p>
<p>因此，在Octave中，如果你想讲这些矩阵向量化，那么你要做的是取出你的$Θ^{(1)}$、$Θ^{(2)}$、$Θ^{(3)}$，然后使用下面这段代码，这段代码将取出三个$Θ$矩阵中的所有元素，然后把他们全部展开，成为一个很长的向量，也就是<code>thetaVec</code>。</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">thetaVec = [Theta1<span class="comment">(:);Theta2(:)</span>;Theta3<span class="comment">(:)];</span></div></pre></td></tr></table></figure>
<p>同样的，下面这段代码将取出$D$矩阵的所有元素，然后展开成一个长向量<code>DVec</code>：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">DVec = [D1<span class="comment">(:);D2(:)</span>;D3<span class="comment">(:)];</span></div></pre></td></tr></table></figure>
<p>最后，如果你想要从向量表达式返回到矩阵表达式的话，你要做的就是使用<code>reshape</code>函数，传入向量的区间以及矩阵的行数和列数，即可得到对应的矩阵：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Theta1 = reshape(<span class="name">thetaVec</span>(<span class="number">1</span>:<span class="number">110</span>),<span class="number">10</span>,<span class="number">11</span>)<span class="comment">;</span></div><div class="line">Theta2 = reshape(<span class="name">thetaVec</span>(<span class="number">111</span>:<span class="number">220</span>),<span class="number">10</span>,<span class="number">11</span>)<span class="comment">;</span></div><div class="line">Theta3 = reshape(<span class="name">thetaVec</span>(<span class="number">221</span>:<span class="number">231</span>),<span class="number">10</span>,<span class="number">11</span>)<span class="comment">;</span></div></pre></td></tr></table></figure>
<hr>
<p>下面用一个Octave例子来展示上面的计算过程：</p>
<p>首先，让我们假设<code>Theta1</code>是一个10行11列的单位矩阵：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; Theta1 = ones(<span class="number">10</span>,<span class="number">11</span>);</div><div class="line">&gt;&gt; Theta1</div><div class="line">Theta1 =</div><div class="line"></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div><div class="line">   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span>   <span class="number">1</span></div></pre></td></tr></table></figure>
<p><code>Theta2</code>是一个元素都为2的10行11列的矩阵：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; Theta2 = <span class="number">2</span>*ones(<span class="number">10</span>,<span class="number">11</span>);</div><div class="line">&gt;&gt; Theta2</div><div class="line">Theta2 =</div><div class="line"></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div><div class="line">   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span>   <span class="number">2</span></div></pre></td></tr></table></figure>
<p>然后假设<code>Theta3</code>是一个<code>1×11</code>的元素均为3的矩阵：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; Theta3 = <span class="number">3</span>*ones(<span class="number">1</span>,<span class="number">11</span>);</div><div class="line">&gt;&gt; Theta3</div><div class="line">Theta3 =</div><div class="line"></div><div class="line">   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span>   <span class="number">3</span></div></pre></td></tr></table></figure>
<p>现在，我们想把这些所有的矩阵变成一个向量<code>thetaVec</code> ：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; thetaVec = [ Theta1<span class="comment">(:); Theta2(:)</span>; Theta3<span class="comment">(:)];</span></div><div class="line">&gt;&gt; thetaVec</div><div class="line"></div><div class="line">thetaVec =</div><div class="line"></div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   1</div><div class="line">   ...</div></pre></td></tr></table></figure>
<p>我们可以看到<code>thetaVec</code>是一个$231×1$的向量，这里包含了所有矩阵的元素：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;</span> size(thetaVec)</div><div class="line">ans =</div><div class="line"></div><div class="line">   <span class="number">231</span>     <span class="number">1</span></div></pre></td></tr></table></figure>
<p>如果我想重新得到我最初的三个矩阵，我可以对<code>thetaVec</code>使用<code>reshape</code>命令。</p>
<p>比如，我们可以抽出前110个元素，来重组一个$10×11$的矩阵，即<code>Theta1</code>：</p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; reshape(thetaVec(1:110),10,11)</div><div class="line">ans =</div><div class="line"></div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div><div class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span> <span class="number"> 1 </span>  1</div></pre></td></tr></table></figure>
<p>我们也可以用同样的方式来取接下来的110个元素，来重组<code>Theta2</code>：</p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; reshape(thetaVec(111:220),10,11)</div><div class="line">ans =</div><div class="line"></div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div><div class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span> <span class="number"> 2 </span>  2</div></pre></td></tr></table></figure>
<p>最后再抽出221到231的元素，重组<code>Theta3</code>：</p>
<figure class="highlight tap"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; reshape(thetaVec(221:231),1,11)</div><div class="line">ans =</div><div class="line"></div><div class="line">  <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span> <span class="number"> 3 </span>  3</div></pre></td></tr></table></figure>
<hr>
<p>为了使这个过程更形象，下面我们来看怎样将这一方法应用于我们的学习算法：</p>
<p>假设你有一些初始参数值:$Θ^{(1)}$，$Θ^{(2)}$，$Θ^{(3)}$。</p>
<p>我们要做的是取出这些参数，并且将它们展开为一个长向量作为<code>initialTheta</code>，带入<code>fminunc</code>函数：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="title">fminunc</span><span class="params">(@costFunction, initialTheta, options)</span></span></div></pre></td></tr></table></figure>
<p>我们要做的另一件事是执行代价函数<code>@costFunction</code>，实现算法如下：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jval, gradientVec]</span> = <span class="title">costFunction</span><span class="params">(thetaVec)</span></span></div></pre></td></tr></table></figure>
<p>代价函数<code>costFunction</code>传入参数<code>thetaVec</code>，这也是我所有参数的向量，是将所有的参数展开成一个向量的形式。</p>
<p>因此，我要做的第一件事就是通过向量<code>thetaVec</code>使用重组函数<code>reshape</code>来得到$Θ^{(1)}$,$Θ^{(2)}$,$Θ^{(3)}$。</p>
<p>这样我就能执行<strong>向前传播</strong>和<strong>反向传播</strong>来计算出导数$D^{(1)}$,$D^{(2)}$,$D^{(3)}$和代价函数$J(Θ)$。</p>
<p>最后，我可以取出这些导数值，然后让它们保存和我展开的$Θ$值相同的顺序来展开它们(按照$D^{(1)}$,$D^{(2)}$,$D^{(3)}$的顺序)，得到<code>gradientVec</code>，这个值由我的代价函数返回，它可以以一个向量的形式返回这些导数值。</p>
<blockquote>
<p>现在，你应该对怎样进行参数的矩阵表达式和向量表达式之间的转换，有了一个更清晰的认识。</p>
<p>使用矩阵表达式的好处是：当你的参数以矩阵的形式存储时，你在进行正向传播和反向传播时，你会觉得更加方便。当你将参数存储为矩阵时，一个大好处是充分利用了向量化的实现过程。</p>
<p>相反地，向量表达式的优点是如果你有像thetaVec或者DVec这样的矩阵，当你使用一些高级的优化算法时，这些算法通常要求你所有的参数都展开成一个长向量的形式。</p>
</blockquote>
<h2 id="梯度检验（Gradient_Checking）">梯度检验（Gradient Checking）</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在之前的视频中，我们讨论了如何使用向前传播和反向传播计算神经网络中的导数，但反向传播作为一个有很多细节的算法，在实现的时候会有点复杂，而且有一个不好的方面是在实现反向传播时，会遇到很多细小的错误。所以如果你把它和梯度下降算法或者其他优化算法一起运行时，可能看起来它运行正常，并且你的代价函数$J(Θ)$最后可能在每次梯度下降法迭代时，都会减小，即使在实现反向传播时有一些小错误，可能也会检查不出来。</p>
<p>所以它看起来是$J(Θ)$在减小，但是可能你最后得到的神经网络误差比没有错误的要高，而且你很可能就不知道你的这些结果是这些小错误导致的。那你应该怎么办呢？</p>
<p>有一个想法叫做<strong>梯度检验(Gradient Checking)</strong>可以解决基本所有的问题。我现在每次实现神经网络的反向传播或者类似的梯度下降算法或者其他比较复杂的模型，我都会使用梯度检验，如果你这么做，它会帮你确定并且能很确信你实现的向前传播和反向传播或者其他的什么算法是100%正确的。</p>
<p>在之前的视频中，我一般是让你相信我给出的那些计算就是代价函数的梯度，但一旦你们实现数值梯度检验，也就是这节视频的主题，你就能自己验证你写的代码确实是在计算代价函数$J(Θ)$了。</p>
</blockquote>
<h3 id="梯度检验原理">梯度检验原理</h3><p>看下面这样的例子：</p>
<p><img src="/img/16_10_13/008.png" alt=""></p>
<p>加入我有一个代价函数$J(\theta)$，并且我有一个实数值$\theta$：</p>
<p><img src="/img/16_10_13/009.png" alt=""></p>
<p>假如说我想估计这个函数在这一点的导数，这个导数就等于这样一条直线的斜率：</p>
<p><img src="/img/16_10_13/010.png" alt=""></p>
<p>下面我要用数值计算的方法来计算近似的导数，这个是用数值方法计算近似导数的过程：</p>
<p>我要找到$\theta+ε$和$\theta-ε$这两个点，然后用一条直线把这两点连起来：</p>
<p><img src="/img/16_10_13/011.png" alt=""></p>
<p>然后用这条红线的斜率来作为导数的近似值。在数学上，这条红线的斜率等于两点之间垂直方向的差值除以水平方向的差值：</p>
<p><img src="/img/16_10_13/012.png" alt=""></p>
<p>所以垂直方向上的差值为：</p>
<p>$$<br>J(\theta+ε) - J(\theta-ε)<br>$$</p>
<p>水平方向的差值为：</p>
<p>$$<br>2ε<br>$$</p>
<p>那么我们的近似是这样的：</p>
<p>$$<br>\frac{\partial}{\partial \theta}J(\theta)≈<br>\frac{J(\theta+ε) - J(\theta-ε)}{2ε}<br>$$</p>
<blockquote>
<p>通常，我给$ε$取很小的值，比如可能取$ε=10^{-4}$，$ε$的取值在一个很大的范围内都是可行的，实际上，如果你让$ε$非常小，那么数学上上面的式子实际上就是导数。只是我们不用想非常非常小的$ε$，因为可能会产生数值问题，所以我通常让$ε$取$ε=10^{-4}$。</p>
<p>顺便说一下，可能你们见过这种估计导数的公式：$\frac{J(\theta+ε) - J(\theta)}{ε}$，这种方式称为<strong>单侧差分</strong>，而上面的那种方式叫做<strong>双侧差分</strong>。双侧差分给我们了一个稍微精确些的估计，所以，我通常用双侧差分，而不用单侧差分估计。</p>
</blockquote>
<h3 id="Octave中实现梯度检验">Octave中实现梯度检验</h3><p>具体地说，你在Octave中实现时，要使用下面这个代码：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gradApprox = (<span class="name">J</span>(<span class="name">theta</span> + EPSILON) - J(<span class="name">theta</span> - EPSILON))/(<span class="number">2</span>*EPSILON)</div></pre></td></tr></table></figure>
<p>你的程序要调用<code>gradApprox</code>来计算这个函数。这个函数会通过这个公式：$\frac{J(\theta+ε) - J(\theta-ε)}{2ε}$，它会给出这点导数的数值估计。</p>
<h3 id="当$\theta$是向量时">当$\theta$是向量时</h3><p>在之前的例子中，$\theta$是一个实数，接下来让我们来讨论更普遍的一种情况：当$\theta$是一个向量参数的情况。</p>
<p>假如说$\theta$是一个$n$维向量（它可能是我们的神经网络参数$Θ^{1}$，$Θ^{2}$，$Θ^{3}$的展开形式）</p>
<p>所以$\theta$是一个有n个元素的向量。</p>
<p>$$<br>\theta = [\theta_{1},\theta_{2},\theta_{3},…,\theta_{n}]<br>$$</p>
<hr>
<p>我们可以用类似的想法来估计所有的偏导数项：</p>
<p>$$<br>\frac{\partial}{\partial \theta_{1}}J(\theta)<br>≈<br>\frac{<br>J(\theta_{1} + ε,\theta_{2},\theta_{3},…,\theta_{n})<br>-<br>J(\theta_{1} - ε,\theta_{2},\theta_{3},…,\theta_{n})<br>}<br>{2ε}<br>$$</p>
<p>$$<br>\frac{\partial}{\partial \theta_{2}}J(\theta)<br>≈<br>\frac{<br>J(\theta_{1},\theta_{2} + ε,\theta_{3},…,\theta_{n})<br>-<br>J(\theta_{1},\theta_{2} - ε,\theta_{3},…,\theta_{n})<br>}<br>{2ε}<br>$$</p>
<p>$$…$$</p>
<p>$$<br>\frac{\partial}{\partial \theta_{n}}J(\theta)<br>≈<br>\frac{<br>J(\theta_{1},\theta_{2},\theta_{3},…,\theta_{n} + ε)<br>-<br>J(\theta_{1},\theta_{2},\theta_{3},…,\theta_{n} - ε)<br>}<br>{2ε}<br>$$</p>
<p>分别对$\theta$向量的每个元素使用<strong>双侧差分</strong>来计算导数。</p>
<p>上面的这些公式给出了一个队任意参数求近似偏导数的方法。具体地说，你要实现的是下面这个程序：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:n,</div><div class="line">	thetaPlus = theta;</div><div class="line">	thetaPlus(<span class="built_in">i</span>) = thetaPlus(<span class="built_in">i</span>) + EPSILON;</div><div class="line">	thetaMinus = theta;</div><div class="line">	thetaMinus(<span class="built_in">i</span>) = thetaMinus(<span class="built_in">i</span>) - EPSILON;</div><div class="line">	gradApprox(<span class="built_in">i</span>) = (J(thetaPlus) - J(thetaMinus))/(<span class="number">2</span>*EPSILON);</div><div class="line"><span class="keyword">end</span>;</div></pre></td></tr></table></figure>
<p>我们把这个用在Octave里，来计算数值导数。</p>
<p>我们实现神经网络时，我们用<code>for</code>循环来计算代价函数对每个网络中的参数的偏导数<code>gradApprox</code>，然后和我们从反向传播得到的导数<code>DVec</code>进行对比，看是否相等或近似于<code>DVec</code>。</p>
<p>$$<br>Check\ that\ gradApprox≈DVec<br>$$</p>
<p>如果这两种计算导数的方法给了你相同的结果，或者非常接近的结果，那么我就非常确信我实现的反向传播是正确的。然后我把这些<code>DVec</code>向量用在梯度下降法，或者其他高级优化算法里。</p>
<h3 id="总结">总结</h3><p>最后，我想把所有的东西放在一起，然后告诉你怎样实现这个数值梯度检验。</p>
<p><strong>实现步骤:</strong></p>
<ul>
<li>实现反向传播来计算<code>DVec</code>($D^{(1)}$，$D^{(2)}$，$D^{(3)}$)。</li>
<li>用<code>gradApprox</code>实现数值梯度检验</li>
<li>然后确定<code>DVec</code>和<code>gradApprox</code>给出的结果非常相近</li>
<li>在使用你的代码去学习训练你的网络之前，重要的是要关掉梯度检验，不在使用<code>gradApprox</code>这个数值导数公式（这么做的原因是，这个梯度检验的计算量非常大，它是一个非常慢的计算近似导数的方法。而相对的反向传播算法是一个在计算导数上效率更高的方法。）</li>
</ul>
<p>再次重申一下，在为了训练分类器运行你的算法，做很多次梯度下降或高级优化算法的迭代之前，要确定你不再使用梯度检验的程序，具体来说，如果你在每次的梯度下降法迭代时，都运行数值梯度检验，你的程序会变得非常慢，因为数值检验程序比反向传播算法要慢得多。</p>
<h2 id="随机初始化$Θ$">随机初始化$Θ$</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/ND5G5/random-initialization" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这一节将介绍神经网络训练中的最后一个知识点：<strong>随机初始化$Θ$</strong></p>
</blockquote>
<p>当你运行一个算法（例如梯度下降算法，或者其他高级优化算法）时，我们需要给变量$\theta$一些初始值。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">optTheta</span> = fminunc(@costFunction, initialTheta, options)</div></pre></td></tr></table></figure>
<p>现在考虑一下梯度下降算法，同样我们需给定$\theta$一些初始值，接下来使用梯度下降方法慢慢地执行这些步骤使其下降，使$J(\theta)$下降到最小。</p>
<p>那么$\theta$的初始值该设置为多少呢？是否可以设置为一个0向量呢？</p>
<p>$$<br>Set\ \ initialTheta = zeros(n,1)\ ?<br>$$</p>
<p>虽然说在逻辑回归时，初始化所有变量为0是可行的，但在训练神经网络时，这样做是不可行的。</p>
<p>以训练下面这个神经网络为例：</p>
<p><img src="/img/16_10_13/013.png" alt=""></p>
<p>照之前所说，将所有变量初始化为0：</p>
<p>$$<br>Θ_{ij}^{(l)}=0 \ \ for\ all\ i,j,l.<br>$$</p>
<p>如果是这样的话，当初始化下面这些颜色两两相同的权重时，这些权重都被赋予相同的初始值0：</p>
<p><img src="/img/16_10_13/014.png" alt=""></p>
<p>那么这就意味着经过计算后，这两个隐藏单元$a_{1}$，$a_{2}$的值是相同的：</p>
<p>$$<br>a_{1}^{(2)}=a_{2}^{(2)}<br>$$</p>
<p>同样的原因，由于权重相同，也可以证明：</p>
<p>$$<br>δ_{1}^{(2)}=δ_{2}^{(2)}<br>$$</p>
<p>同时，如果你更深入地挖掘一下，你不难得出这些变量对参数的偏导数满足以下条件：</p>
<p>以这两条红色的权重为例：</p>
<p><img src="/img/16_10_13/015.png" alt=""></p>
<p>即代价函数的关于这两个权重的偏导数是相等的：</p>
<p>$$<br>\frac{\partial}{\partial Θ_{01}^{(1)}}J(Θ)<br>=<br>\frac{\partial}{\partial Θ_{02}^{(1)}}J(Θ)<br>$$</p>
<p>这也意味着，一旦更新梯度下降方法，第一个红色权重也会更新，等于学习率乘以这个式子:$\frac{\partial}{\partial Θ_{01}^{(1)}}J(Θ)$</p>
<p>第二条红色权重更新为学习率乘以这个式子：$\frac{\partial}{\partial Θ_{02}^{(1)}}J(Θ)$。</p>
<p>这也就意味着，一旦更新梯度下降，这两条红色权重的值，在最后将互为相等：</p>
<p>$$<br>Θ_{01}^{(1)}=Θ_{02}^{(1)}<br>$$</p>
<p>因此，即使权重现在不都为0，但参数的值最后也互为相等。</p>
<p>同样地，即使更新一个梯度下降，下面这两条红色的权重也会互为相等：</p>
<p><img src="/img/16_10_13/016.png" alt=""></p>
<p>同理，最下面的两个权重也会互为相等。</p>
<p>所以每次更新后，两个隐藏单元的输入的对应的参数将是相同的。这就意味着即使经过一次梯度下降的循环后，你会发现两个隐藏单元任然是两个完全相同的输入函数：</p>
<p>$$<br>a_{1}^{(2)}=a_{2}^{(2)}<br>$$</p>
<p>这也意味着，这个神经网络并不能计算出什么更有价值的东西。</p>
<p>想象一下，不止有两个隐藏单元，而是有很多的隐藏单元，这就是会导致所有的隐藏单元都在计算相同的特征，这是完全多余的表达，因为这意味着最后的逻辑回归单元只会得到一种特征。这样便阻止了神经网络学习出更有价值的信息。</p>
<h3 id="随机初始化$Θ$引入">随机初始化$Θ$引入</h3><p>为了解决这个神经网络变量初始化的问题，我们采用<strong>随机初始化</strong>的方法。</p>
<p>具体的说，上面我们说到的所有权重相同的问题，有时被我们也称为<strong>对称权重</strong>。所以随机初始化解决的就是如何<strong>打破这种对称性(Symmetry breaking)</strong>。</p>
<p>所以我们需要做的是对$Θ_{ij}^{(l)}$的每个值进行初始化，范围在$[-ε,ε]$之间（$-ε\le Θ_{ij}^{(l)} \le ε$）。</p>
<p>在Octave中初始化$Θ$：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Theta1 = rand(<span class="number">10</span>,<span class="number">11</span>)*(<span class="number">2</span>*INIT_EPSILON) - INIT_EPSILON;</div><div class="line">Theta2 = rand(<span class="number">1</span>,<span class="number">11</span>)*(<span class="number">2</span>*INIT_EPSILON) - INIT_EPSILON;</div></pre></td></tr></table></figure>
<p>其中<code>rand(10,11)</code>代表一个$10×11$的随机矩阵，这个<code>rand()</code>函数就是用来得到一个任意的随机矩阵的方法，并且所有的值都是介于0到1之间的实数。</p>
<p>因此，如果取0到1之间的一个数和$2ε$相乘再减去$ε$，然后得到的结果就是一个在$[-ε,ε]$之间的数。</p>
<blockquote>
<p>顺便说一句，这里的这个$ε$和在进行梯度检查中用的那个$ε$不是一回事，这也是为什么上面这段代码要使用<code>INIT_EPSILON</code>而不是<code>EPSILON</code>的原因。(在梯度检查中用到的是<code>EPSILON</code>)</p>
</blockquote>
<h3 id="总结-1">总结</h3><p>总的来说，为了训练神经网络，应该对权重进行随机初始化为$[-ε,ε]$之间的值。$ε$是接近于0的小数，然后进行反向传播，执行梯度检查，使用梯度下降或者高级的优化算法，试着使代价函数$J(Θ)$达到最小，从某个随机选取的参数$Θ$开始。通过打破对称性的过程，我们希望梯度下降或者其他高级优化算法可以找到$Θ$的最优值。</p>
<h2 id="神经网络总体回顾：所有算法合体吧！">神经网络总体回顾：所有算法合体吧！</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Wh6s3/putting-it-together" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这一节中，我们将对神经网络的所有内容进行一个整体回顾，看看这些零散的内容相互之间有怎样的联系，以及神经网络学习算法的总体实现过程。</p>
</blockquote>
<h3 id="第一步，选择一个合适的神经网络结构">第一步，选择一个合适的神经网络结构</h3><p>当我们在训练一个神经网络的时候，我们要做的第一件事就是搭建网络的大体框架，这里我说的框架意思是神经元之间的链接模式。我们可能会从以下几种结构中选择：</p>
<p><img src="/img/16_10_13/017.png" alt=""></p>
<ul>
<li>第一种网络结构包含三个输入单元、五个隐藏单元、和四个输出单元。</li>
<li>第二种包含三个输入单元，两组五个隐藏单元作为隐藏层，四个输出单元。</li>
<li>第三种组合是三个输入单元，三组五个隐藏单元作为隐藏层，四个输出单元。</li>
</ul>
<p>这些就是可能选择的结构，每一层可以选择多少个隐藏单元，以及可以选择多少个隐藏层。这些都是你构建时的选择，那么我们该如何做出选择呢？</p>
<p>首先，我们知道我们已经定义了输入单元的数量，一旦你确定了特征集$x^{(i)}$对应的输入单元数目，也就确定了特征$x^{(i)}$的维度，输入单元的数目将会由此确定。</p>
<p>其次如果你正在进行多类别分类，那么输出层的单元数目将会由你分类问题中所要区分的类别个数确定。</p>
<blockquote>
<p>值得一提的是，在多类别分类问题中，若$y$的取值范围是在${1,2,3,..,10}$之间，那么你就有10个可能的分类，别忘了把你的y重新写成向量的形式，例如：</p>
</blockquote>
<p>$$<br>y=<br>\begin{bmatrix}<br>   1 \\<br>   0 \\<br>   0 \\<br>   … \\<br>   0<br>  \end{bmatrix}<br>$$</p>
<blockquote>
<p>假设现在要表示第5个分类，也就是说$y=5$，那么在你的神经网络中，就不能直接使用数值5来表达，因为这种情况下，神经网络将有10个输出单元，你应该用一个向量来表示：</p>
</blockquote>
<p>$$<br>y=<br>\begin{bmatrix}<br>      0 \\<br>    0 \\<br>    0 \\<br>    0 \\<br>       1 \\<br>       0 \\<br>       0 \\<br>    0 \\<br>    0 \\<br>       0<br>  \end{bmatrix}<br>$$</p>
<p>所以对于输入和输出单元的数目的选择，是比较容易理解的。而对于隐藏层单元的个数，以及隐藏层的数目，我们有一个默认的规则，那就是<strong>只使用单个隐藏层</strong>，所以第一种类型的神经网络架构是最常见的：</p>
<p><img src="/img/16_10_13/018.png" alt=""></p>
<p>或者如果你使用超过一层的隐藏层的话，同样我们也有一个默认规则，那就是<strong>每一个隐藏层通常都应该拥有相同的单元数</strong>。所以后面的两种神经网络结构的隐藏层都拥有相同的单元数：</p>
<p><img src="/img/16_10_13/019.png" alt=""></p>
<p>但实际上通常来说，只使用一层隐藏层的结构是较为合理的默认结构。</p>
<p>而对于隐藏单元的个数，通常情况下<strong>隐藏单元越多越好</strong>，不过我们需要注意的是，如果有大量的隐藏单元，计算量一般会比较大。并且，一般来说，每个隐藏层所包含的单元数量还应该和输入$x$的维度相匹配，也要和特征的数目相匹配。可能隐藏单元的数目和输入特征的数量相同，或者是它的二倍或者三倍、四倍。因此，隐藏单元的数目需要和其他参数相匹配。</p>
<p>一般来说隐藏单元的数目取稍大于输入特征数目都是可以接受的。</p>
<h3 id="训练神经网络的步骤">训练神经网络的步骤</h3><p>接下来，我们就来具体介绍如何实现神经网络的训练过程，下面是训练神经网络的六个步骤：</p>
<ul>
<li>1.构建一个神经网络并且随机初始化权值（<strong>Randomly initialize Weight</strong>）</li>
</ul>
<blockquote>
<p>我们通常把权值初始化为很小的值，接近于0</p>
</blockquote>
<ul>
<li>2.执行向前传播算法，也就是对于神经网络的任意一个输入$x^{(i)}$计算出对应的$h_{Θ}(x^{(i)})$。</li>
<li>3.通过代码计算出代价函数$J(Θ)$</li>
<li>4.执行反向传播算法(<strong>Backprop</strong>)来算出这些偏导数：$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$</li>
</ul>
<blockquote>
<p>具体来说，我们要对所有训练集数据使用一个<code>for</code>循环进行遍历没一个样本（实际上有更复杂的方式来替代<code>for</code>循环来实现，但对于第一次实现神经网络的训练过程，我非常不建议使用<code>for</code>循环以为的方式，因为这种方式更有助于第一次使用时的理解）：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> = <span class="number">1</span>:m</div></pre></td></tr></table></figure>
<p>注意：下面这部分是在for循环内，但markdown的代码片段不支持显示Letex所以写在了外部：</p>
<p>—for循环内部开始—</p>
<p>对样本$(x^{(i)},y^{(i)})$使用向前传播和反向传播算法<br>     (具体来说，就是把输入项带入后，得出每个节点的激励值$a^{(l)}$和delta项$δ^{(l)}$)<br>$$<br>△^{(l)} := △^{(l)} + δ^{(l+1)}(a^{(l)})^{T}<br>$$</p>
<p>—for循环内部结束—</p>
<p>计算出这些$△$的累加值之后，我们将用别的程序来计算出偏导数项：<br>$$<br>\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)<br>$$</p>
</blockquote>
<ul>
<li>5.使用梯度检查来校验结果。用梯度检查来比较这些已经用反向传播算法得到的偏导数值$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$与用数值方法得到的估计值进行比较，来检查，确保这两种方法得到值是基本相近的。</li>
</ul>
<blockquote>
<p>通过梯度检查，我们能确保我们的反向传播算法得到的结果是正确的，但必须要说明的一点是，检查结束后我们需要去掉梯度检查的代码，因为梯度检查计算非常慢。</p>
</blockquote>
<ul>
<li>6.使用一个最优化算法（比如说梯度下降算法或者其他更加高级的优化方法，比如说BFGS算法，共轭梯度法，或者其他一些已经内置到<code>fminunc</code>函数中的方法），将所有这些优化方法和反向传播算法相结合，这样我们就能计算出这些偏导数项的值$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$。</li>
</ul>
<blockquote>
<p>到现在，我们已经知道了如何计算代价函数$J(Θ)$，我们知道了如何使用反向传播算法来计算偏导数$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$，那么我们就能使用某个最优化方法来最小化$J(Θ)$关于$Θ$的函数值。</p>
</blockquote>
<p>顺便提一下，对于神经网络代价函数$J(Θ)$是一个非凸函数，因此理论上是能够停留在局部最小值的位置。实际上，梯度下降算法和其他一些高级优化方法理论上都能收敛于局部最小值，但一般来讲这个问题其实并不是什么要紧的事，尽管我们不能保证这些优化算法一定会得到全局最优值，但通常来讲，像梯度下降这类的算法在最小化代价函数$J(Θ)$的过程中，还是表现的很不错的，通常能够得到一个很小的局部最小值，尽管这可能不一定是全局最优值。</p>
<h3 id="梯度下降法在神经网络中的直观理解">梯度下降法在神经网络中的直观理解</h3><p>最后，梯度下降算法，似乎对于神经网络来说还是比较神秘的，希望下面这幅图能让你对梯度下降法在神经网络中的应用产生一个更直观的理解：</p>
<p><img src="/img/16_10_13/020.png" alt=""></p>
<p>这实际上有点类似我们早先时候解释梯度下降时的思路：我们有一个代价函数$J(Θ)$，并且在我们的神经网络中有一系列参数值，这里我之写下了两个参数值$Θ_{12}^{(1)}$,$Θ_{11}^{(1)}$，当然实际上在神经网络里，我们可以有很多很多的参数值：$Θ^{(1)}$,$Θ^{(2)}$…。因此我们参数的维度就会很高了，也无法绘制成直观的图像。所以这里我们假设这个神经网络中只有两个参数值：$Θ_{12}^{(1)}$,$Θ_{11}^{(1)}$。</p>
<p>那么代价函数$J(Θ)$度量的就是这个神经网络对训练数据的拟合情况。</p>
<p>所以，如果你取某个参数，比如说在这样一个局部最优值：</p>
<p><img src="/img/16_10_13/021.png" alt=""></p>
<p>这一点的位置所对应的参数$Θ$的情况是对于大部分的训练数据，我的假设函数的输出会非常接近于$y^{(i)}$:</p>
<p>$$<br>h_{Θ}(x^{(i)})≈y^{(i)}<br>$$</p>
<p>那么如果是这样的话，那么我们的代价函数$J(Θ)$值就会很小。</p>
<p>而反过来，如果我们取这个值：</p>
<p><img src="/img/16_10_13/022.png" alt=""></p>
<p>我们的代价函数$J(Θ)$值就会很大。</p>
<p>因此<strong>梯度下降的原理是我们从某个随机的初始点开始，它将会不停的下降，那么反向传播算法的目的就是算出梯度下降的方向，而梯度下降的过程就是沿着这个方向一点点的下降，一直到我们希望得到的点，这一点就是我们希望找到的局部最优点</strong>。</p>
<p>所以，当你在执行反向传播算法并使用梯度下降或者更高级的优化方法时，上面的图片很好地帮你解释了基本的原理，也就是视图找到某个最优的参数值。这个值使得我们的神经网络的输出值与$y^{(i)}$的实际值（也就是训练集的输出观测值）尽可能的接近</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/09/14/斯坦福机器学习课程 第五周 (1)训练神经网络/" itemprop="url">
                斯坦福机器学习课程 第五周 (1)训练神经网络
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-09-14T01:14:00+08:00" content="2016-09-14">
            2016-09-14
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/09/14/斯坦福机器学习课程 第五周 (1)训练神经网络/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/09/14/斯坦福机器学习课程 第五周 (1)训练神经网络/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="代价函数">代价函数</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/na28E/cost-function" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在本节课中和后面的几节课中，我将开始讲述一种在给定训练集下为神经网络拟合参数的学习算法。正如我们讨论大多数学习算法一样，我们准备从拟合神经网络参数的代价函数开始讲起。</p>
</blockquote>
<p>以神经网络在分类问题中的应用为例：</p>
<p><img src="/img/16_09_18/001.png" alt=""> </p>
<p>假设我们有一个如上图所示的神经网络结构，然后假设我们有一个这样的训练集：</p>
<p>$$<br>\begin{aligned}<br>(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})<br>\end{aligned}<br>$$</p>
<p>一共有m个训练样本$(x^{(i)},y^{(i)})$。</p>
<p>用大写字母$L$表示这个神经网络结构的总层数：</p>
<p>$$<br>L = total\ no.\ of\ layers\ in\ network(神经网络总层数)<br>$$</p>
<p>所以，对于左边的网络结构，我们得到$L=4$。</p>
<p>然后，我们准备用$S_{l}$表示第$l$层的神经元的数量，<strong>这其中不包括L层的偏置单元</strong>：</p>
<p>$$<br>S_{l}= no.\ of\ units(not\ counting\ bias\ unit)\ in\ layer\ l<br>$$</p>
<p>比如说：</p>
<ul>
<li>$S_{1}=3$(也就是输入层)</li>
<li>$S_{2}=5$</li>
<li>$S_{3}=5$</li>
<li>$S_{4}=S_{L}=4$ (因为$L=4$)</li>
</ul>
<p>我们接下来讨论两种分类问题，分别是<strong>二元分类</strong>和<strong>多类别分类</strong>。</p>
<h3 id="二元分类(Binary_classification)">二元分类(Binary classification)</h3><p>在二元分类中$y$只能是0或者1：</p>
<p>$$<br>y= 0\ or\ 1<br>$$</p>
<p>在这个例子中，我们有一个输出单元（不同于上面的神经网络有4个输出单元）。神经网络的输出会是一个实数：</p>
<p>$$<br>h_{Θ}(x)\in \mathbb{R}<br>$$</p>
<p>输出单元的个数:</p>
<p>$$<br>S_{L} = 1<br>$$</p>
<blockquote>
<p>在这类问题里，为了简化记法，我会把$K$设为1，这样<strong>你可以把$K$看作输出层的单元数目</strong>。</p>
</blockquote>
<h3 id="多类别分类（Multi-class_classification）">多类别分类（Multi-class classification）</h3><p>在多类别分类问题中，会有$K$个不同的类，比如说如果我们有四类的话，我们就用下面这种表达形式来代表$y$。在这类问题里，我们就会有$K$个输出单元。</p>
<p><img src="/img/16_09_18/002.png" alt=""> </p>
<p>我们的输出假设就是一个$K$维向量：</p>
<p>$$<br>h_{Θ}(x)\in \mathbb{R}^{K}<br>$$</p>
<p>输出单元的个数就是$K$：</p>
<p>$$<br>S_{L} = K<br>$$</p>
<blockquote>
<p>通常这类问题中，我们都有$K\ge3$，因为如果我们只有两类的话，我们直接使用二元分类法就可以了。因此只有在$K\ge3$的情况下，我们才会使用这种<strong>多类别分类</strong>。</p>
</blockquote>
<h3 id="定义代价函数">定义代价函数</h3><p>我们在神经网络里，使用的代价函数，应该是逻辑回归里使用的代价函数的一般形式。</p>
<p><strong>逻辑回归的代价函数:</strong></p>
<p>$$<br>\begin{equation*}<br>J(\theta)=-\frac{1}{m}<br>[<br>\sum_{i=1}^m<br>y^{(i)}logh_{\theta}(x^{(i)})<br>+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))<br>]+<br>\frac{\lambda}{2m}<br>\sum_{j=1}^n<br>\theta^{2}_{j}<br>\end{equation*}<br>$$</p>
<p>其中$\begin{equation*}<br>\frac{\lambda}{2m}<br>\sum_{j=1}^n<br>\theta^{2}_{j}<br>\end{equation*}$这一项是个额外的正则化项，是一个$j$从1到$n$的求和形式。因为我们并没有把偏置项0正则化。</p>
<p>对于神经网络，我们使用的代价函数是这个式子的一般化形式。</p>
<p>$$<br>\begin{equation*}<br>J(Θ)=-\frac{1}{m}<br>[<br>\sum_{i=1}^m<br>\sum_{k=1}^K<br>y^{(i)}_{k}log(h_{Θ}(x^{(i)}))_{k}<br>+(1-y^{(i)})_{k})log(1-(h_{Θ}(x^{(i)}))_{k})<br>]+<br>\frac{\lambda}{2m}<br>\sum_{l=1}^{L-1}<br>\sum_{i=1}^{S_{l}}<br>\sum_{j=1}^{S_{l+1}}<br>(Θ^{(l)}_{ji})^{2}<br>\end{equation*}<br>$$</p>
<p>神经网络现在输出了在$K$维的向量$h_{Θ}(x)$：</p>
<p>$$<br>h_{Θ}(x)\in \mathbb{R}^{K}<br>$$</p>
<p>用$(h_{Θ}(x))_{i}$来表示第$i$个输出。</p>
<p>其中$\begin{equation*}\sum_{k=1}^K\end{equation*}$这个求和项是$K$个输出单元的求和，比如你有4个输出单元在神经网络的最后一层，那么这个求和项就是$k$从1到4所对应的每一个逻辑回归算法的代价函数之和。</p>
<p>最后式子中的这一项$\begin{equation*}<br>\frac{\lambda}{2m}<br>\sum_{l=1}^{L-1}<br>\sum_{i=1}^{S_{l}}<br>\sum_{j=1}^{S_{l+1}}<br>(Θ^{(l)}_{ji})^{2}<br>\end{equation*}$类似于我们在逻辑回归里所用的正则化项，这个求和项看起来确实非常复杂，它所做的就是把这些项全部加起来，也就是对所有的$Θ_{ji}^{(l)}$的值都相加。正如我们在逻辑回归里的一样，这里要除去那些对应于偏差值的项。具体来说，我们不把$Θ_{j0}^{(l)}$这些项加进去，这是因为当我们计算神经元的激励值时，我们会有这些项。这些带0的项，类似于偏置单元的项。类比于我们在做逻辑回归的时候，我们就不应该把这些项加入到正规化项里去，因为我们并不想正规化这些项，并把这些项设定为0。 （这里我表示没看懂）</p>
<p>即使我们真的把他们加进去了，也就是说$i$从0加到$S_{l}$依然成立，并且不会有大的差异，但是这个“不把偏差项正规化”的规定可能只是会更常见一些。</p>
<h2 id="反向传播(B-P)">反向传播(B-P)</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这个视频中，我们来讨论一下让代价函数最小化的算法，具体来说，我们将主要讲解<strong>反向传播算法（BP算法）</strong></p>
</blockquote>
<p>下面是我们上一节写好的代价函数：</p>
<p><img src="/img/16_09_18/003.png" alt=""> </p>
<p>我们要做的就是试图找到使得代价函数$J(Θ)$最小的$Θ$值：</p>
<p><img src="/img/16_09_18/004.png" alt=""> </p>
<p>为了使用梯度下降法，我们需要做的就是写好一个通过输入参数$Θ$，然后计算：</p>
<p><img src="/img/16_09_18/005.png" alt=""> </p>
<p>这一节的大部分内容也都是在讲解如何计算真两项的。</p>
<hr>
<p><strong>梯度下降计算</strong></p>
<p><img src="/img/16_09_18/006.png" alt=""> </p>
<p>首先，我们从只有一个训练样本的情况说起，假设我们整个训练集只包含一个训练样本：</p>
<p>$$<br>(x,y)<br>$$</p>
<p>让我们粗看一下，使用这样一个训练样本来计算的顺序。</p>
<p>首先我们用向前传播方法来计算一下在给定输入的时候，假设函数的输出结果：</p>
<p>$$<br>a^{(1)} = x<br>$$</p>
<blockquote>
<p>$a^{(1)}$就是第一层的激励值，也就是输入层</p>
</blockquote>
<p>$$<br>z^{(2)} = Θ^{(1)}a^{(1)}<br>$$</p>
<p>$$<br>a^{(2)} = g(z^{(2)})<br>$$</p>
<blockquote>
<p>这里记得添加偏差项$a_{0}^{(2)}$</p>
</blockquote>
<p>$$<br>z^{(3)} = Θ^{(2)}a^{(2)}<br>$$</p>
<p>$$<br>a^{(3)} = g(z^{(3)})<br>$$</p>
<blockquote>
<p>这里记得添加偏差项$a_{0}^{(3)}$</p>
</blockquote>
<p>$$<br>z^{(4)} = Θ^{(3)}a^{(3)}<br>$$</p>
<p>$$<br>a^{(4)} = h_{Θ}(x) = g(z^{(4)})<br>$$</p>
<p>通过上面的步骤计算，我们就可以得出假设函数的输出结果了：</p>
<p><img src="/img/16_09_18/007.png" alt=""> </p>
<p>接下来，为了计算导数项，我们将采用一种叫做<strong>反向传播(Backpropagation)</strong>的算法。</p>
<p>反向传播算法从直观上说就是对每一个节点求下面这一个误差项:</p>
<p><img src="/img/16_09_18/008.png" alt=""> </p>
<blockquote>
<p>$δ_{j}^{(l)}$这种形式代表了第$l$层的第$j$个结点的<strong>误差</strong></p>
<p>我们还记得我们使用$a_{j}^{(l)}$来表示第$l$层的第$j$个结点的<strong>激励值</strong>，所以这个$δ$项，在某种程度上就捕捉到了我们在这个神经结点的激励值的误差。所以我们可能希望这个结点的激励值稍微不一样。</p>
</blockquote>
<p>具体来讲，我们用上面的那个四层的神经网络结构做例子：</p>
<p>每一项的输出单元(layer L = 4)</p>
<p>$$<br>δ_{j}^{(4)} = a_{j}^{(4)} - y_j<br>$$</p>
<blockquote>
<p>对于每一个输出单元，我们准备计算$δ$项，所以第四层的第$j$个单元的$δ$就等于<strong>这个单元的激励值减去训练样本里的真实值</strong>。所以$a_{j}^{(4)}$这一项同样可以写成$h_{Θ}(x)_{j}$：</p>
</blockquote>
<p>$$<br>δ_{j}^{(4)} = h_{Θ}(x)_{j} - y_j<br>$$</p>
<p>顺便说一下，如果你把$δ$、$a$和$y$这三项都看作向量的话，那么上面的式子你也可以写出向量化的实现：</p>
<p>$$<br>δ^{(4)} = a^{(4)} - y<br>$$</p>
<p>这里的$δ^{(4)}$、$a^{(4)}$和$y$都是一个向量，并且向量维数等于输出单元的数目。</p>
<p>所以现在我们计算出网络结构的误差项$δ^{(4)}$，我们下一步就是计算网络中前面几层的误差项$δ$。</p>
<p>这就是$δ^{(3)}$的计算公式：</p>
<p>$$<br>δ^{(3)} = (Θ^{(3)})^{T}δ^{(4)}.*g’(z^{(3)})<br>$$</p>
<blockquote>
<p>这里的点乘$.*$是我们从MATLAB里知道的对y元素的乘法操作，指的是两个向量中元素间对应相乘。</p>
</blockquote>
<p>其中$g’(z^{(3)})$这一项其实是对激励函数$g$在输入值为$z(3)$的时候所求的导数。</p>
<p>如果你稍微会一些微积分的知识，你可以很容易的求得$g’(z^{(3)})$这一项的值是：</p>
<p>$$<br>a^{(3)}.*(1-a^{(3)})<br>$$</p>
<p>这里的$1$是元素都为1的向量。</p>
<p>接下来，你可以应用一个相似的公式来求得$δ^{(2)}$:</p>
<p>$$<br>δ^{(2)} = (Θ^{(2)})^{T}δ^{(3)}.*g’(z^{(2)})<br>$$</p>
<p>值得注意的是，这里我们没有$δ^{(1)}$项，因为第一层是输入层，不存在误差。所以这个例子中，我们的$δ$项就只有第2层和第3层。</p>
<hr>
<p>反向传播法这个名字源于我们从输出层开始计算$δ$项，然后我们返回到上一层计算第三隐藏层的$δ$项，接着我们再往前一步来计算$δ^{(2)}$。</p>
<p>所以说我们是类似于把输出层的误差反向传播给了第3层，然后再传到第二层。这就是反向传播的意思。</p>
<p>最后，这个推导过程是出奇的复杂，但是如果你按照这样几个步骤来计算，就有可能简单直接地完成复杂的数学证明。</p>
<p>如果你忽略标准化所产生的项，我们可以证明我们想要的偏导项，恰好就是下面这个表达式：</p>
<p>$$<br>\frac{\partial}{\partial Θ_{ij}^{(l)}}J(Θ)=<br>a_{j}^{(l)}δ_{i}^{(l+1)}<br>$$</p>
<p>$$<br>(ignore\ λ; if\ λ = 0)<br>$$</p>
<blockquote>
<p>这里我们忽略了$λ$，我们将在之后完善这一个关于正则化项。</p>
</blockquote>
<p>所以到现在，我们通过反向传播计算这些$δ$项，可以非常快速的计算出所有参数的偏导数项。</p>
<hr>
<p>好，现在让我们把上面所讲的所有内容整合在一起，然后说说如何实现反向传播算法：</p>
<p>当我们有一个很大的训练样本的时候，而不是像我们例子里这样的一个训练样本。我们是这样做的：</p>
<p>假设我们有m个样本的训练集：</p>
<p>$Training\ set\ {(x^{(1)},y^{(1)}),…,(x^{(m)},y^{(m)})}$</p>
<p>我们要做的第一件事就是设置这些值：</p>
<p><img src="/img/16_09_18/009.png" alt=""> </p>
<blockquote>
<p>这里的$△$其实是大写的$δ$，实际上这些$△_{ij}^{(l)}$将被用来计算偏导数项$\frac{\partial}{\partial Θ_{ij}^{(l)}}J(Θ)$</p>
<p>所以，正如我们接下来看到的，这些$△_{ij}^{(l)}$将被作为累加项，慢慢地增加，以算出这些偏导数。</p>
</blockquote>
<p>下图是我们接下来要执行的一些操作：</p>
<p><img src="/img/16_09_18/010.png" alt=""> </p>
<p>在这里我们将遍历我们的训练集。</p>
<p>我们要做的第一件事就是设定$a^{(1)}$，也就是输入层的激励函数：$a^{(1)} = x^{(i)}$</p>
<p>接下来我们运用正向传播，来计算第2，3，4，…，L层的激励值$a^{l}$。</p>
<p>接下来，我们将用$y^{(i)}$来计算$δ^{(L)}=a^{(L)}-y^{(i)}$</p>
<p>接下来，我们使用反向传播算法来计算$δ^{(L-1)},δ^{(L-2)},…,δ^{(2)}$</p>
<p>最终，我们将用$△{ij}^{(l)}$来积累我们在前面写好的偏导数项：</p>
<p>$$<br>△{ij}^{(l)}:=△{ij}^{(l)} + a_{j}^{(l)}δ_{i}^{(l+1)}<br>$$</p>
<p>如果你再看一下上面这个表达式，你可以把它写成向量形式：</p>
<p>具体来说，如果你把$△$看做一个矩阵，$ij$代表矩阵中的位置，那么上面的式子我们就可以写成：</p>
<p>$$<br>△^{(l)}:=△^{(l)} + δ^{(l+1)}(a^{(l)})^{T}<br>$$</p>
<p>最后，执行这个for循环体之后我们挑出这个for循环，然后计算下面这些式子：</p>
<p>$$<br>D_{ij}^{(l)} := \frac{1}{m}△{ij}^{(l)} + λΘ_{ij}^{(l)}<br>\ \ \ \ if\ j ≠ 0<br>$$</p>
<p>$$<br>D_{ij}^{(l)} := \frac{1}{m}△{ij}^{(l)}<br>\ \ \ \ if\ j = 0<br>$$</p>
<blockquote>
<p>这里我们对$j ≠ 0$ 和 $j = 0$分两种情况来讨论，在$j=0$的情况下对应的是偏差项，所以这也是为什么在$j=0$的情况下没有写额外的标准化项的原因。</p>
</blockquote>
<p>最后，尽管严格的证明对于你来说太复杂，你现在可以说明的是一旦你计算出来了这些，这就正好是代价函数关于每一个参数的偏导数：</p>
<p>$$<br>\frac{\partial}{\partial Θ_{ij}^{(l)}}J(Θ)<br>= D_{ij}^{(l)}<br>$$</p>
<p>所以，你可以把它用在梯度下降算法，或者其他更高级的算法中。</p>
<h2 id="反向传播算法的直观介绍">反向传播算法的直观介绍</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/du981/backpropagation-intuition" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这一节中，将要更加深入的讨论一下反向传播算法的这些复杂的步骤，并且希望给你一个更加直观的感受，理解这些步骤究竟是在做什么。也希望通过这一节，你能理解它至少还是一个合理的算法。</p>
<p>但可能你即使看了这段视频你还是觉得反向传播依然很复杂，这也没关系，其实即使是我（吴恩达）接触了反向传播这么多年了，有时候任然觉得这是一个难以理解的算法，但还是希望这段视频能有些许帮助。</p>
</blockquote>
<h3 id="距离说明神经网络计算过程">距离说明神经网络计算过程</h3><p>为了更好地理解反向传播算法，我们再来仔细研究一下向前传播的原理。</p>
<p><img src="/img/16_09_18/011.png" alt=""></p>
<p>这里有一个包含两个输入单元（不包括偏差单元）的神经网络，在第二层有两个隐藏单元（不包括偏差单元），第三层也有两个隐藏单元（不包括偏差单元），最后的输出层有一个输出单元。</p>
<h3 id="向前传播">向前传播</h3><p>为了更清楚的展示向前传播，下图展示了这个神经网络的<strong>向前传播</strong>的运算过程：</p>
<p><img src="/img/16_09_18/012.png" alt=""></p>
<p>事实上，<strong>反向传播</strong>算法的运算过程非常类似于此，只有计算的方向不同而已。</p>
<h3 id="代价函数-1">代价函数</h3><p>为了更好的理解反向传播算法的原理，我们把目光转向代价函数：</p>
<p><img src="/img/16_09_18/013.png" alt=""></p>
<p>这个代价函数对应的情况是只有一个输出单元，如果我们有不止一个输出单元的话，只需要对所有的输出单元进行一次求和运算。</p>
<p>请注意这组训练样本$x^{(i)}$,$y^{(i)}$，注意这种只有一个输出单元的情况，如果不考虑正则化即$λ=0$，因此最后的正则化项就没有了。</p>
<p><img src="/img/16_09_18/014.png" alt=""></p>
<p>这个求和运算括号里面与第i个训练样本对应的代价项，也就是说$(x^{(i)},y^{(i)})$对应的代价项，将有下面这个式子决定：</p>
<p>$$<br>cost(i) = y^{(i)}log h_{Θ}(x^{(i)}) + (1 - y^{(i)})logh_{Θ}(x^{(i)})<br>$$</p>
<p>而这个代价函数所扮演的角色可以看做是平方误差，当然，如果你愿意，你可以把$cost(i)$想象成：</p>
<p>$$<br>cost(i)≈(h_{Θ}(x^{(i)})-y^{(i)})^{2}<br>$$</p>
<p>因此，这里的$cost(i)$表征了该神经网络是否能准确地预测样本i的值，也就是输出值，和实际观测值$y^{(i)}$的接近程度。</p>
<h3 id="反向传播">反向传播</h3><p>现在我们来看看反向传播是怎么做的。</p>
<p>一种直观的理解是反向传播算法就是在计算所有这些$δ$项：</p>
<p>$$<br>δ_{j}^{(l)} = “error” \ of \ cost \ for \ a_{j}^{(l)} \ (unit \ j \ in \ layer \ l).<br>$$</p>
<p>并且我们可以把它们看作是这些激励值的“<strong>误差</strong>”(注意这些激励值是第l层中的第j项)。</p>
<p>更正式一点的说法是$δ$项实际上是关于$z_{j}^{(l)}$的偏微分，也就是cost函数关于我们计算出的输入项的加权和，也就是$z$项的偏微分:</p>
<p>$$<br>δ_{j}^{(l)} = \frac{\partial}{\partial z_{j}^{(l)}}cost(i) \ \ \ \ (for \ j \ge 0 )<br>$$</p>
<p>其中：</p>
<p>$$<br>cost(i) = y^{(i)}log h_{Θ}(x^{(i)}) + (1 - y^{(i)})logh_{Θ}(x^{(i)})<br>$$</p>
<p>如果我们观察该神经网络内部的话，把这些$z_{j}^{(l)}$项稍微改一点点，那就将影响到神经网络的输出，并且最终会改变代价函数的值。</p>
<p>因此，它们度量着我们对神经网络的权值做多少的改变，对中间的计算量影响是多少，进一步对整个神经网络的输出$h(x)$影响多少，以及对整个的代价影响多少。</p>
<p>可能刚才讲的偏微分的这种理解不太容易理解，没关系，不用偏微分的思想，我们同样也可以理解。</p>
<p>我们再深入一点，研究一下反向传播的过程，对于输入层，如果我们设置$δ$项，假设我们进行第i个训练样本，那么：</p>
<p>$$<br>δ_{1}^{(4)}=y^{(i)}-a^{(4)}_{1}<br>$$</p>
<p>接下来我们要对这些值进行反向传播，算出$δ_{1}^{(3)}$、$δ_{2}^{(3)}$，然后同样的再进行下一层的反向传播，算出$δ_{1}^{(2)}$、$δ_{2}^{(2)}$。</p>
<p>举个例子:</p>
<p>接下来，我们来看看如何计算$δ_{2}^{(2)}$。</p>
<p>我要对一些权值进行标记:</p>
<p><img src="/img/16_09_18/016.png" alt=""> </p>
<p>实际上，我们要做的是我们要用下一层的$δ$值和权值相乘，然后加上另一个$δ$值和权值相乘的结果。也就是说，它其实是$δ$值的加权和。权值是这些对应边的强度。</p>
<p><img src="/img/16_09_18/017.png" alt=""> </p>
<p>计算过程是：</p>
<p>$$<br>δ_{2}^{(2)}=Θ_{12}^{(2)}δ_{1}^{(3)} + Θ_{22}^{(2)}δ_{2}^{(3)}<br>$$</p>
<p>再看看另一个例子：</p>
<p>如果想要计算$δ_{2}^{(3)}$的值，计算过程也是类似的：</p>
<p>$$<br>δ_{2}^{(3)}=Θ_{12}^{(3)}δ_{1}^{(4)}<br>$$</p>
<p>另外顺便提一下，目前为止我写的$δ$值仅仅是隐藏层中的没有包括偏差单元:”+1”的。包不包括偏差单元取决于你如何实现这个反向传播算法，你也可以对这些偏差单元计算$δ$的值，这些偏差单元总是取为”+1”的值。</p>
<p>通常来说，我在执行反向传播的时候，我是算出了这些偏差单元的$δ$值，但我通常忽略掉它们，而不是把它们带入计算，因为它们其实并不是计算那些微积分的必要部分，</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/09/11/斯坦福机器学习课程 第四周 (3)神经网络应用实例/" itemprop="url">
                斯坦福机器学习课程 第四周 (3)神经网络应用实例
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-09-11T19:22:00+08:00" content="2016-09-11">
            2016-09-11
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/09/11/斯坦福机器学习课程 第四周 (3)神经网络应用实例/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/09/11/斯坦福机器学习课程 第四周 (3)神经网络应用实例/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="神经网络应用实例">神经网络应用实例</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/rBZmG/examples-and-intuitions-i" target="_blank" rel="external">视频地址 part1</a></p>
<p><a href="https://www.coursera.org/learn/machine-learning/lecture/solUx/examples-and-intuitions-ii" target="_blank" rel="external">视频地址 part2</a></p>
<blockquote>
<p>接下来将通过一个具体的例子来解释神经网络是如何计算关于输入的复杂的非线性函数的。</p>
</blockquote>
<h3 id="问题引入">问题引入</h3><p>考虑下面的问题：</p>
<p>我们有二进制的输入特征$x_{1}$和$x_{2}$，它们的取值要么是0，要么是1。这个例子中，我画出了两个正样本和两个负样本。</p>
<p><img src="/img/16_09_11/001.png" alt=""></p>
<p>但你可以认为这是更复杂的学习问题的简化版本。在这个复杂问题中，我们可能在右上角有一堆正样本，在右下方有一堆用圆圈来表示的负样本。</p>
<p><img src="/img/16_09_11/002.png" alt=""></p>
<p>我们想要做到的就是有一个非线性的决策边界来区分正负样本：</p>
<p><img src="/img/16_09_11/003.png" alt=""></p>
<p>那么，神经网络是如何做到的呢？为了描述方便，我继续使用上面二进制输入特征的例子。</p>
<p>具体来讲，我们要计算的目标函数:</p>
<p>$$y=x_{1} XNOR x_{2}$$</p>
<blockquote>
<p>求同或(都为真或都为假时，结果为真，否则结果为假)</p>
</blockquote>
<p>或者也可以写作：</p>
<p>$$NOT(y=x_{1} XOR x_{2})$$</p>
<blockquote>
<p>求异或(都为真或都为假时，结果为假，否则结果为真)再取反</p>
</blockquote>
<h3 id="AND、OR、NOT的实现">AND、OR、NOT的实现</h3><h4 id="AND">AND</h4><p>为了解释神经网络模型如何来拟合这种训练集。我们先讲解一个稍微简单一些的神经网络，它拟合了<strong>“且运算”(AND)</strong>：</p>
<p><img src="/img/16_09_11/004.png" alt=""></p>
<p>假设我们有二进制输入$x_{1}$和$x_{2}$，目标函数是$y=x_{1}ANDx_{2}$，那么我们怎样得到一个具有单个神经元的神经网络来计算这个<strong>逻辑与</strong>呢？为了做到这一点，我们也需要画出偏置单元（即下图中+1的单元）：</p>
<p><img src="/img/16_09_11/005.png" alt=""></p>
<p>接下来让我给这个网络分配一些权重(参数)：</p>
<p><img src="/img/16_09_11/006.png" alt=""></p>
<p>所以我的假设函数是：</p>
<p>$$<br>h_{Θ}(x)=g(-30 + 20x_{1} + 20x_{2})<br>$$</p>
<p>这里$Θ^{(1)}_{10}$就是$-30$、$Θ^{(2)}_{11}$就是$20$、$Θ^{(3)}_{12}$就是$20$。</p>
<p>接下来介绍一下这个小神经元是怎样计算的。</p>
<p>回忆一下激励函数$g(z)$看起来是这样的：</p>
<p><img src="/img/16_09_11/007.png" alt=""></p>
<p>再来看看我们的假设在各种情况下的输出：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-30)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
</tbody>
</table>
<p>这就是逻辑“与”的计算结果。</p>
<hr>
<h4 id="OR">OR</h4><p>下面的神经网络使用同样的原理实现了“或”的功能：</p>
<p><img src="/img/16_09_11/008.png" alt=""></p>
<p>假设函数为：</p>
<p>$$<br>h_{Θ}(x)=g(-10 + 20x_{1} + 20x_{2})<br>$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(30)\approx1$</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="NOT">NOT</h4><p>下面的神经网络使用同样的原理实现了“非”的功能：</p>
<p><img src="/img/16_09_11/009.png" alt=""></p>
<p>假设函数为：</p>
<p>$$<br>h_{Θ}(x)=g(10 - 20x_{1})<br>$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
</tbody>
</table>
<h3 id="一个更复杂的例子">一个更复杂的例子</h3><p>下面的神经网络使用同样的原理实现了“$(NOTx_{1})AND(NOTx_{2})$”的功能：</p>
<p><img src="/img/16_09_11/010.png" alt=""></p>
<p>假设函数为：</p>
<p>$$<br>h_{Θ}(x)=g(10 - 20x_{1} - 20x_{2})<br>$$</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-30)\approx0$</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="求解_XNOR">求解    XNOR</h3><p>接下来我们使用上面求解的以下三个神经网络，就可以来运算$x_{1}XNORx_{2}$了：</p>
<p><img src="/img/16_09_11/011.png" alt=""></p>
<p>为了拟合$x_{1}XNORx_{2}$的非线性的样本分布：</p>
<p><img src="/img/16_09_11/001.png" alt=""></p>
<p>我们可以构建以下神经网络的隐藏层：</p>
<p><img src="/img/16_09_11/012.png" alt=""></p>
<p>对应的真值表如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$a_{1}^{(2)}$</th>
<th style="text-align:center">$a_{2}^{(2)}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<p>有了$a_{1}^{(2)}$、$a_{2}^{(2)}$后，我们加入偏置单元，然后就可以得到输出层了：</p>
<p><img src="/img/16_09_11/012.png" alt=""></p>
<p>最终的真值表如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$a_{1}^{(2)}$</th>
<th style="text-align:center">$a_{2}^{(2)}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center"><strong>1</strong></td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>1</strong></td>
</tr>
</tbody>
</table>
<p>通过一个含有输入层、隐藏层、输出层的神经网络，我们最终拟合了$x_{1}XNORx_{2}$。</p>
<blockquote>
<p>更一般的理解是：在输入层中，我们有原始输入值，然后我们建立了一个隐藏层，用来计算稍微复杂一些的输入量的函数，然后通过添加另一个层我们得到了一个更复杂一点的函数，这就是神经网络可以计算较复杂函数的某种直观解释。</p>
<p>我们知道，当层数很多的时候，你有一个相对简单的输入量的函数作为第二层，而第三层可以建立在此基础上来计算更加复杂一些的函数，然后再下一层，又可以计算再复杂一些的函数：</p>
</blockquote>
<p><img src="/img/16_09_11/014.png" alt=""></p>
<h3 id="手写识别的展示">手写识别的展示</h3><p>接下来，将展示一段视频，来源于<strong>阳乐昆(Yann LeCun)</strong>，他是一名教授，供职于纽约大学，也是神经网络研究早期的奠基者之一，也是这一领域 大牛。他的很多理论和想法现在都已经被应用于各种各样的产品和应用中，遍布全世界。所以我想向大家展示一段他早期工作中的视频，这段视频中，他使用神经网络算法进行手写数字的识别。你也许记得，这门课刚开始的时候，我说过关于神经网络的一个早期成就，就是应用神经网络读取邮政编码，以帮助我们进行邮递。那么这便是其中的一项尝试。</p>
<p>视频说明：</p>
<p><img src="/img/16_09_11/015.png" alt=""></p>
<p>视频如下：</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/yxuRnBEczUU" frameborder="0" allowfullscreen></iframe>

<h2 id="神经网络的多类别分类">神经网络的多类别分类</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/gFpiW/multiclass-classification" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这一节将介绍<strong>如何使用神经网络做多类别分类</strong>。</p>
<p>在多类别分类中，通常有不止一个类别需要我们去区分，在上一节最后的视频中，我们提到了有关手写数字识别的问题，这实际上正是一个多类别分类的问题。因为识别数字从0到9，正好是10个类别。</p>
</blockquote>
<p>我们处理多类别分类的方法实际上是基于一个<strong>多神经网络算法</strong>而延伸出来的。</p>
<p>让我们来看看下面这个例子：</p>
<p>还是一个有关计算机视觉识别的例子，就像我之前介绍过的识别汽车的例子一样，但与之不同的是现在我们希望处理的是四个类别的分类问题，四个类别分别是行人、轿车、摩托车、卡车：</p>
<p><img src="/img/16_09_11/016.png" alt=""></p>
<p>任意给出一副图片，我们需要知道图片上是这四个类别中的哪一个。</p>
<p>对于这样的一个问题，我们的做法是，<strong>建立一个具有四个输出单元的神经网络</strong>：</p>
<p><img src="/img/16_09_11/017.png" alt=""></p>
<p>也就是说，此时神经网络的输出是一个思维向量。因此现在的输出需要用一个向量来表示，这个向量中有四个元素，而我们要做的是对第一个输出元素进行分辨图片上是不是一个行人(Pedestrian)，然后对第二个元素分辨它是不是一辆轿车(Car)，对第三个元素分辨它是不是摩托车(Motorcycle)，对第四个元素分辨它是不是一辆卡车(Truck)。</p>
<p>因此，如果图上是行人的话，我希望输出结果是：</p>
<p>$$<br>h_{Θ}(x)\approx<br>\begin{bmatrix}<br>1 \\<br>0 \\<br>0 \\<br>0<br>\end{bmatrix}<br>$$</p>
<p>如果图上是轿车的话，我希望输出结果是：</p>
<p>$$<br>h_{Θ}(x)\approx<br>\begin{bmatrix}<br>0 \\<br>1 \\<br>0 \\<br>0<br>\end{bmatrix}<br>$$</p>
<p>如果图上是摩托车的话，我希望输出结果是：</p>
<p>$$<br>h_{Θ}(x)\approx<br>\begin{bmatrix}<br>0 \\<br>0 \\<br>1 \\<br>0<br>\end{bmatrix}<br>$$</p>
<p>如果图上是卡车的话，我希望输出结果是：</p>
<p>$$<br>h_{Θ}(x)\approx<br>\begin{bmatrix}<br>0 \\<br>0 \\<br>0 \\<br>1<br>\end{bmatrix}<br>$$</p>
<p>所以，这和我们介绍逻辑回归时讨论过的一对多方法其实是一样的，只不过现在我们有四个逻辑回归的分类器，而我们需要对每一个分类器都分别进行识别分类。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/09/07/斯坦福机器学习课程 第四周 (2)神经网络/" itemprop="url">
                斯坦福机器学习课程 第四周 (2)神经网络
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-09-07T23:42:00+08:00" content="2016-09-07">
            2016-09-07
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/09/07/斯坦福机器学习课程 第四周 (2)神经网络/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/09/07/斯坦福机器学习课程 第四周 (2)神经网络/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="神经网络的表示I">神经网络的表示I</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/ka3jK/model-representation-i" target="_blank" rel="external">视频地址</a></p>
<p>这节课介绍神经网络如何表示我们的假设。</p>
<h3 id="神经网络起源">神经网络起源</h3><p>神经网络是在模拟大脑中的神经元时发明的，因此，要解释如何表示模型假设，我们先来看看神经元在大脑中是什么样的。我们的大脑中充满了这样的神经元：</p>
<p><img src="/img/16_09_07/001.png" alt=""></p>
<p>神经元是大脑中的细胞，其中有两点值得我们注意，一是神经元有一个细胞主体(Cell body)，二是神经元有一定数量的输入神经，这些输入神经叫做<strong>树突(Dendrite)</strong>。可以把它们想象成输入电线，它们接收来自其他神经元的信息。神经元的输出神经叫做<strong>轴突(Axon)</strong>，这些输出神经是用来给其他神经元传递信号或者传送信息的。</p>
<p>简而言之，神经元是一个计算单元，它从<strong>输入神经</strong>接受一定数目的信息，并做一些计算，然后将结果通过它的<strong>轴突</strong>传送到大脑中的其他神经元。</p>
<p>下面是一组神经元的示意图：</p>
<p><img src="/img/16_09_07/002.png" alt=""></p>
<p>神经元利用微弱的电流进行沟通，这些弱电流也称作<strong>动作电位</strong>(其实就是一些微弱的电流)。</p>
<p>所以如果神经元想要传递一个消息，它就会通过它的轴突发送一段微弱的电流，给其他神经元。这就是<strong>轴突</strong>连接到输入神经元(另一个神经元的<strong>树突</strong>)。接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己收到的消息进行计算，并向其他神经元传递消息。</p>
<blockquote>
<p>顺便说一下，这也是我们感觉和肌肉运转的原理。如果你想要活动一块肌肉，就会触发一个神经元给你的肌肉，发送脉冲，并引起你的肌肉收缩。如果一些感官，比如说眼睛，想要向大脑传递一个消息，那么它就是像这样发送电脉冲给大脑的。</p>
</blockquote>
<h3 id="神经网络逻辑单元">神经网络逻辑单元</h3><p>在一个计算机的神经网络里，我们将使用一个非常简单的模型来模拟神经元的工作：</p>
<p><img src="/img/16_09_07/003.png" alt=""></p>
<p>我们将神经元模拟成一个逻辑单元。上图中，黄色的圆圈，你可以理解为类似神经元的东西，然后我们通过它的树突(或者说是它的输入神经)传递给它一些信息。然后神经元做一些计算，并通过它的输出神经(即它的轴突)输出计算结果。</p>
<p>这里的$h_{\theta}(x)$通常值的是：</p>
<p>$$<br>\begin{align*}<br>h_{\theta}(x)<br>= \frac{1}{1+e^{-\theta^{T}x}}<br>\end{align*}<br>$$</p>
<p>其中$x$和$\theta$指的是我们的参数向量：</p>
<p><img src="/img/16_09_07/004.png" alt=""></p>
<p>这就是一个简单的模拟神经元的模型。</p>
<hr>
<p>当我绘制一个神经网络时，有时会额外增加一个$x_{0}$的输入节点，这个$x_{0}$节点有时也被称作<strong>偏置单位(或偏置神经元)</strong>。但由于$x_{0}=1$，有时我是不会画出它的，这取决于它是否对例子有利。</p>
<h3 id="激励函数">激励函数</h3><p>在神经网络中，有时我们会听到“一个有Sigmoid函数或者Logistic函数作为<strong>激励函数(activation function)</strong>的人工神经元”这样的话。其实这里所指的<strong>激励函数(activation function)</strong>只是对类似非线性函数$g(z)$的另一个术语称呼：</p>
<p>$$<br>\begin{align*}<br>g(z)<br>= \frac{1}{1+e^{-z}}<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>z<br>= \theta^{T}x<br>\end{align*}<br>$$</p>
<blockquote>
<p>在之前我们一值称$\theta$为模型的参数，但在神经网络的文献里，有时你可能会看到人们谈论一个模型的<strong>权重(weight)</strong>，这个<strong>权重(weight)</strong>其实和模型的参数$\theta$是一个东西。</p>
</blockquote>
<hr>
<h3 id="解读神经网络">解读神经网络</h3><h4 id="输入层，输出层，隐藏层">输入层，输出层，隐藏层</h4><p>神经网络其实就是这些不同的神经元组合在一起的集合：</p>
<p><img src="/img/16_09_07/005.png" alt=""></p>
<p>具体来说：</p>
<ul>
<li><p>上图中有三个输入单元：$x_{1}$,$x_{2}$和$x_{3}$，当然我们也可以加入值为1的$x_{0}$。</p>
</li>
<li><p>中间有三个神经元：$a_{1}^{(2)}$，$a_{2}^{(2)}$和$a_{3}^{(2)}$，同理，你可以可以加上值永远为1的偏置单元$a_{0}^{(2)}$。</p>
</li>
<li><p>然后，我们在最右侧有第三层，第三层的这个节点输出了假设函数$h(x)$的计算结果$h_{Θ}(x)$。</p>
</li>
</ul>
<p>用神经网络的术语来说，第一层也被称为<strong>输入层</strong>，因为我们在这一层输入我们的特征项$x_{1}$，$x_{2}$和$x_{3}$。</p>
<p>最后一层，也被称为<strong>输出层</strong>，因为这一层的神经元会输出假设函数的最终计算结果$h_{Θ}(x)$。</p>
<p>中间层，也被称为<strong>隐藏层</strong>，在监督学习中，你能看到输入，也能看到输出，而隐藏层的值在你的训练过程中是看不到的，它的值不是$x$也不是$y$，所以我们叫它隐藏层。神经网络可以有不止一个的隐藏层。<strong>在神经网络中，任何一个非输入层且非输出层，就被称为隐藏层</strong>。</p>
<h4 id="神经网络运行原理">神经网络运行原理</h4><p><img src="/img/16_09_07/005.png" alt=""></p>
<p>接下来，让我们来逐步分析，上图所呈现的神经网络的计算步骤。</p>
<p>首先需要说明以下两个符号的含义：</p>
<ul>
<li>$a_{i}^{(j)}$ 表示第j层的第i个神经元。</li>
</ul>
<blockquote>
<p>具体来说$a_{1}^{2}$表示的是第2层的第1个激励，即隐藏层的第一个激励。</p>
<p>所谓<strong>激励(activation)</strong>是指由一个具体神经元读入计算并输出的值。</p>
</blockquote>
<ul>
<li>$Θ^{(j)}$ 表示层与层之间权重的<strong>参数矩阵</strong>(或者叫<strong>权重矩阵</strong>)(比如说从第一层到第二层、或者从第二层到第三层的作用)。</li>
</ul>
<p>具体来说，$a_{1}^{(2)}$的值的计算是这样的：</p>
<p>$$<br>a_{1}^{(2)} = g(Θ_{10}^{(1)}x_{0} +<br>Θ_{11}^{(1)}x_{1} +<br>Θ_{12}^{(1)}x_{2} +<br>Θ_{13}^{(1)}x_{3}<br>)<br>$$</p>
<blockquote>
<p>不要忘记这里的$g$函数是S型函数(或者说是S激励函数，也叫作逻辑激励函数)。</p>
</blockquote>
<p>我们可以把隐藏层的三个神经元的计算结果都写出来：</p>
<p>$$<br>a_{1}^{(2)} = g(Θ_{10}^{(1)}x_{0} +<br>Θ_{11}^{(1)}x_{1} +<br>Θ_{12}^{(1)}x_{2} +<br>Θ_{13}^{(1)}x_{3}<br>)<br>$$</p>
<p>$$<br>a_{2}^{(2)} = g(Θ_{20}^{(1)}x_{0} +<br>Θ_{21}^{(1)}x_{1} +<br>Θ_{22}^{(1)}x_{2} +<br>Θ_{23}^{(1)}x_{3}<br>)<br>$$</p>
<p>$$<br>a_{3}^{(2)} = g(Θ_{30}^{(1)}x_{0} +<br>Θ_{31}^{(1)}x_{1} +<br>Θ_{32}^{(1)}x_{2} +<br>Θ_{33}^{(1)}x_{3}<br>)<br>$$</p>
<p>这里我们有三个输入单元和三个隐藏单元，这样一来，参数矩阵$Θ^{(1)}$控制了我们来自三个输入单元到三个隐藏单元的映射。因此$Θ^{(1)}$的维数是$R^{3×4}$的(考虑$x_{0}$的情况下)矩阵。</p>
<p>更一般的，如果一个神经网络在第$j$层有$s_{j}$个单元，在$j+1$层有$s_{j+1}$个单元，那么第$j$层的参数矩阵$Θ^{(j)}$的维度就是$s_{j+1}×(s_{j} + 1)$</p>
<p>以上我们讨论了三个隐藏单位是怎么计算它们的值的，最后，在输出层，我们还有一个单元，它用来计算$h_{Θ}(x)$：</p>
<p>$$<br>h_{Θ}(x) = a_{1}^{(3)}=g(<br>Θ_{10}^{(2)}a_{0}^{(2)} +<br>Θ_{11}^{(2)}a_{1}^{(2)} +<br>Θ_{12}^{(2)}a_{2}^{(2)} +<br>Θ_{13}^{(2)}a_{3}^{(2)}<br>)<br>$$</p>
<p>以上就是从数学上对一个人工神经网络的定义。</p>
<h2 id="神经网络的表示II">神经网络的表示II</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/Hw3VK/model-representation-ii" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这节课，将介绍如何高效地进行计算，并展示一个向量化的实现方法。更重要的是让你们明白神经网络的好处所在，并且介绍它是如何帮助我们学习复杂的非线性假设的。</p>
</blockquote>
<h3 id="前向传播(forward_propagation)的向量化实现">前向传播(forward propagation)的向量化实现</h3><p>以这个神经网络为例：</p>
<p><img src="/img/16_09_07/006.png" alt=""></p>
<p>上一节我们介绍过假设函数的计算方式，以及上面这些方程的含义。接下来，我要定义一些额外的项，来简化上面的式子的表示，比如我要把上面第一行(用来表示第一个隐藏层输出的值)：</p>
<p>$$<br>a_{1}^{(2)} = g(Θ_{10}^{(1)}x_{0} +<br>Θ_{11}^{(1)}x_{1} +<br>Θ_{12}^{(1)}x_{2} +<br>Θ_{13}^{(1)}x_{3}<br>)<br>$$</p>
<p>改写为：</p>
<p>$$<br>a_{1}^{(2)} = g(z^{(2)}_{1})<br>$$</p>
<p>可见：</p>
<p>$$<br>z^{(2)}_{1} = Θ_{10}^{(1)}x_{0} +<br>Θ_{11}^{(1)}x_{1} +<br>Θ_{12}^{(1)}x_{2} +<br>Θ_{13}^{(1)}x_{3}<br>$$</p>
<p>其中$z^{(2)}_{1}$的上标$^{(2)}$表示<strong>神经网络的第二层(即这里的隐藏层)</strong>，下标$_{1}$表示<strong>当前层的第一个元素</strong>。</p>
<p>我们通过同样的方式定义$a_{2}^{(2)}$和$a_{3}^{(2)}$。</p>
<p>$$<br>a_{2}^{(2)} = g(z^{(2)}_{2})<br>$$</p>
<p>$$<br>a_{3}^{(2)} = g(z^{(2)}_{3})<br>$$</p>
<p>因此，这些$z$值都是$x_{0}$、$x_{1}$、$x_{2}$和$x_{3}$的加权线性组合，然后带入一个特定的神经元，</p>
<p>如果你仔细观察表达式中红色方框内的区域：</p>
<p><img src="/img/16_09_07/007.png" alt=""></p>
<p>你会发现，这里其实是一个矩阵的乘法运算：</p>
<p>$$<br>Θ^{(1)}x<br>$$</p>
<p>这样一来，我们就能将神经网络的计算，向量化了。</p>
<p>具体而言，我们定义特征向量$x$：</p>
<p>$$<br>x=<br>\begin{bmatrix}<br>x_{0} \\<br>x_{1} \\<br>x_{2} \\<br>x_{3}<br>\end{bmatrix}<br>$$</p>
<blockquote>
<p>其中$x_{0}=1$</p>
</blockquote>
<p>并定义$z^{(2)}$：</p>
<p>$$<br>z^{(2)}=<br>\begin{bmatrix}<br>z^{(2)}_{1} \\<br>z^{(2)}_{2} \\<br>z^{(2)}_{3}<br>\end{bmatrix}<br>$$</p>
<blockquote>
<p>注意，这里的$z^{(2)}$是一个三维向量。</p>
</blockquote>
<p>我们只需要两个步骤，就可以计算出$a^{(2)}$向量了：</p>
<p>$$<br>z^{(2)}=Θ^{(1)}x<br>$$</p>
<p>$$<br>a^{(2)}=g(z^{(2)})<br>$$</p>
<blockquote>
<p>注意，这里的$a^{(2)}$也是一个三维向量，这里的激励函数$g()$将对$z^{(2)}$中的每个元素进行计算。<br>说明一下，在输入层虽然我们有$x$输入，但我们可以把这些输入想象成是第一层的激励。所以可以定义$a^{(1)}=x$，这样就有了向量$a^{(1)}$。</p>
</blockquote>
<p>将向量$x$替换为向量$a^{(1)}$:</p>
<p>$$<br>z^{(2)}=Θ^{(1)}a^{(1)}<br>$$</p>
<hr>
<p>现在，就我目前所写的，我得到了$a^{(2)}_{1}$、$a^{(2)}_{2}$和$a^{(2)}_{3}$的值，但是我同样还需要隐藏层的偏置单元$a^{(2)}_{0}$，这个额外的偏执单元值为1：</p>
<p>$$<br>a^{(2)}_{0}=1<br>$$</p>
<p>加上偏置单元后，向量$a^{(2)}$的长度变成了4：</p>
<p>$$<br>a^{(2)}\in \mathbb{R}^{4}<br>$$</p>
<p>最后，为了计算实际输出值：</p>
<p>$$<br>h_{Θ}(x) = a_{1}^{(3)}=g(<br>Θ_{10}^{(2)}a_{0}^{(2)} +<br>Θ_{11}^{(2)}a_{1}^{(2)} +<br>Θ_{12}^{(2)}a_{2}^{(2)} +<br>Θ_{13}^{(2)}a_{3}^{(2)}<br>)<br>$$</p>
<p>我们计算出代表$Θ_{10}^{(2)}a_{0}^{(2)} +Θ_{11}^{(2)}a_{1}^{(2)} + Θ_{12}^{(2)}a_{2}^{(2)} + Θ_{13}^{(2)}a_{3}^{(2)}$的$z^{(3)}$，并将其带入激励函数，最后就能得出$h(x)$的值了：</p>
<p>$$<br>z^{(3)}=Θ^{(2)}a^{(2)}<br>$$</p>
<p>$$<br>h_{Θ}(x)=a^{(3)}=g(z^{(3)})<br>$$</p>
<p>这就是计算$h_{Θ}(x)$的过程，也称为<strong>前向传播(forward propagation)</strong>。</p>
<blockquote>
<p>这样的命名是因为我们从输入层的激励开始，然后进行前向传播给隐藏层，并计算隐藏层的激励，然后我们继续向前传播，并计算出层的激励，这个从输入层，到隐藏层，再到输出层，依次计算激励的过程，叫<strong>前向传播</strong>。</p>
</blockquote>
<h3 id="神经网络与逻辑回归的对比，以及学习非线性假设的原理">神经网络与逻辑回归的对比，以及学习非线性假设的原理</h3><p>这种前向传播也可以帮助我们了解神经网络的原理，以及解释为什么神经网络可以帮助我们学习非线性假设。</p>
<p>例如下面这个神经网络：</p>
<p><img src="/img/16_09_07/008.png" alt=""></p>
<p>我们暂时盖住左边部分，看右侧这一部分：</p>
<p><img src="/img/16_09_07/009.png" alt=""></p>
<p>这看起来很像逻辑回归，在逻辑回归中，我们就用这一个逻辑回归单元来预测$h(x)$的值。具体来说：</p>
<p>$$<br>h_{Θ}(x)=g(Θ_{10}^{(2)}a_{0}^{(2)} + Θ_{11}^{(2)}a_{1}^{(2)} + Θ_{12}^{(2)}a_{2}^{(2)} + Θ_{13}^{(2)}a_{3}^{(2)})<br>$$</p>
<p>这很像一个很标准的逻辑回归模型，不同之处在于这里使用的是大写的$Θ$而不是小写的$\theta$，神经网络的输入特征值是通过隐藏层计算的。即，神经网络所做的工作看起来就像是逻辑回归，但是它不是使用$x_{1}$、$x_{2}$、$x_{3}$作为输入特征，而是使用$a_{1}^{(2)}$、$a_{2}^{(2)}$、$a_{3}^{(2)}$。</p>
<hr>
<p><img src="/img/16_09_07/010.png" alt=""></p>
<p>然而有趣的是，特征项$a_{1}^{(2)}$、$a_{2}^{(2)}$、$a_{3}^{(2)}$是通过输入的函数来学习的。具体来说，就是从第一层(Layer1)映射到第二层(Layer2)的函数。这个函数由其他一组参数$Θ^{(1)}$决定。所以，在神经网络中，它没有用输入特征$x_{1}$、$x_{2}$、$x_{3}$来训练逻辑回归，而是训练逻辑回归的输入:$a_{1}^{(2)}$、$a_{2}^{(2)}$、$a_{3}^{(2)}$。</p>
<p>可以想象，如果在$Θ^{(1)}$中选择不同的参数有时可以学习到一些很有趣的，和很复杂的特征，就可以得到一个比使用原始输入$x_{1}$、$x_{2}$、$x_{3}$时得到的假设更好的假设。</p>
<p>你也可以使用多项式作为输入，例如：$x_{1}x_{2}$、$x_{2}x_{3}$等作为输入项。这个算法都可以灵活的快速学习任意的特征项，并把最后的结果通过最后一个单元的逻辑回归输出出来。</p>
<h3 id="神经网络的架构">神经网络的架构</h3><p>你还可以用其他类型的图来表示神经网络。神经网络中神经元相连接的方式，称为<strong>神经网络的架构(Architecture)</strong>。</p>
<p>下面是另外一个神经网络架构的例子：</p>
<p><img src="/img/16_09_07/011.png" alt=""></p>
<p>在这个神经网络中，第一层被称为输入层，第四层任然是我们的输出层。其中，第二层和第三层都是隐藏层。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/09/05/斯坦福机器学习课程 第四周 (1)神经网络引入/" itemprop="url">
                斯坦福机器学习课程 第四周 (1)神经网络引入
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-09-05T00:03:00+08:00" content="2016-09-05">
            2016-09-05
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/09/05/斯坦福机器学习课程 第四周 (1)神经网络引入/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/09/05/斯坦福机器学习课程 第四周 (1)神经网络引入/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="非线性假设">非线性假设</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/OAOhO/non-linear-hypotheses" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>接下来的课程中，我将为大家介绍一种叫做<strong>“神经网络”(Neural Network)</strong>的机器学习算法。我们首先来讨论神经网络的表层结构，在后来的课程中再来具体讨论学习算法。 </p>
</blockquote>
<p>神经网络算法实际上是一个相对古老的算法，并且沉寂了一段时间。不过到了现在，它又称为许多机器学习问题的首选技术。</p>
<p>那么我们已经有了线性回归，和逻辑回归，为什么还需要学习<strong>神经网络</strong>这个算法呢？为了阐述研究神经网络的目的，我们首先来看几个机器学习问题作为例子。这几个例子都依赖于研究复杂的非线性分类器。</p>
<h3 id="线性回归对于非线性假设的不可行性分析">线性回归对于非线性假设的不可行性分析</h3><p>考虑这个监督学习分类的问题：</p>
<p><img src="/img/16_09_04/008.png" alt=""></p>
<p>我们已经有了对应的训练集，如果利用逻辑回归算法来解决这个问题，我们首先要构造一个包含很多非线性项的逻辑回归函数：</p>
<p>$$<br>g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{1}x_{2} + \theta_{4}x_{1}^{2}x_{2} + \theta_{5}x_{1}^{3}x_{2} + \theta_{6}x_{1}x_{2}^{2} + …)<br>$$</p>
<blockquote>
<p>这里$g()$函数依然是一个S型函数，即Logistic函数。</p>
</blockquote>
<p>当多项式数足够多时，你可能可以得到一个分开正样本和负样本的分界线：</p>
<p><img src="/img/16_09_04/009.png" alt=""></p>
<p>当你只有两项时，这种方法确实能得到不错的结果，因为你完全可以把$x_{1}$和$x_{2}$的所有组合都包含到多项式中。但对于许多复杂的机器学习问题，涉及的项往往多于两项。</p>
<p><img src="/img/16_09_04/010.png" alt=""></p>
<p>我们之前讨论过房价预测的问题，假设你现在处理的是关于住房的分类问题，而不是一个回归问题。假设你对一栋房子的多方面特点都有所了解，你想预测房子在未来半年内能被卖出去的概率，这是一个分类问题。我们可以想出多达上百个特征。对于这类问题，如果要包含所有的二次项，最终的多项式也可能有很多项：</p>
<p>$$<br>x_{1}^{2},x_{1}x_{2},x_{1}x_{3},x_{1}x_{4}…x_{1}x_{100},x_{2}^{2},x_{2}x_{3}…<br>$$</p>
<blockquote>
<p>对于100个特征的情况下，会有5050个二次项组合。而且随着特征数量的增加，二次项的个数大约以$O(n^{2})$的量级增长，其中$n$是原始项(特征)的个数。</p>
</blockquote>
<p>因此，包含所有的二次项是困难的，所以这可能不是一个好方法，而且由于项数过多，最后的结果很有可能是过拟合的。此外，这里也存在运算量过大的问题。</p>
<blockquote>
<p>当然，你也可以只考虑上面这些二次项的子集，比如$x_{1}^{2},x_{2}^{2},x_{3}^{2}…x_{100}^{2}$，这样就能将二次项的数量大幅减少了，但是却忽略了太多的相关项，在处理问题的时候，不可能得到理想的结果。</p>
</blockquote>
<p>5000多个二次项已经很多了，如果我们还要引入三次项，那么我们的项的个数将以$O(n^{3})$的量级增长，当有100个特征的情况下，大约有170000个三次项组合。所以，当特征个数增大时，这些高阶多项式项数将以几何级数递增，特征空间也随之急剧膨胀。因此，当特征数量较多时，通过这种方式来构建分类器，并不是一个好办法。</p>
<h3 id="图像识别中使用线性回归的不可行性举例">图像识别中使用线性回归的不可行性举例</h3><p>对于许多实际的机器学习问题，特征个数$n$是很大的，举个关于计算机视觉中的一个例子：</p>
<p>假设你想要使用机器学习算法来训练一个分类器，使它检测一个图像来判断图像是否为一辆汽车，很多人可能会很好奇，这对计算机视觉来说有什么难的？当你们看到下面这幅图时，下面是一辆车，这是一目了然的事情，你肯定会觉得很奇怪，为什么学习算法竟可能会不知道图像是什么：</p>
<p><img src="/img/16_09_04/011.png" alt=""></p>
<p>为了解答这个问题，我们取出图像的一小部分，将其放大，结果表明，当人眼看到一辆汽车时，计算机实际上看到的却是这样一个数据矩阵：</p>
<p><img src="/img/16_09_04/012.png" alt=""></p>
<p>这个矩阵中的每个元素表示了像素强度值，告诉我们图像中每个像素的亮度值。因此，对于计算机视觉来说，问题就变成了根据这个像素点亮度矩阵，来告诉我们这些数值代表一个汽车门把手。</p>
<p><strong>具体而言，当使用机器学习算法构造一个汽车识别器时，我们要想出一个带标签的样本集，其中一些样本是各类汽车，另一部分样本是其他任何东西。将这个样本集输入给学习算法以训练出一个分类器，训练完毕后，我们输入一幅新的图片，让分类器来判定“这是什么东西？”</strong>理想情况下，分类器能识别出这是一辆汽车。</p>
<p><img src="/img/16_09_04/013.png" alt=""></p>
<p>为了理解引入非线性分类器的必要性，我们从学习算法的训练样本中挑出一些汽车的图片，和一些非汽车的图片，让我们从其中每幅图片中挑出一组像素点，在坐标系中标出这幅汽车的位置：</p>
<p><img src="/img/16_09_04/014.png" alt=""></p>
<p>汽车的位置取决于像素点1和像素点2的亮度，让我们用同样的方法标出其他图片中汽车的位置：</p>
<p><img src="/img/16_09_04/015.png" alt=""></p>
<p>观察这张图片两个相同的像素位置的像素亮度，这两点都有着不同的像素亮度，所以在这幅图中，它们处于不同的位置。</p>
<p>我们继续画上两个非汽车样本，并且用”-“表示非汽车：</p>
<p><img src="/img/16_09_04/016.png" alt=""></p>
<p>随着样本数量的增多，我们将发现这两类数据分布在坐标系中的不同区域：</p>
<p><img src="/img/16_09_04/017.png" alt=""></p>
<p>因此我们现在需要一个非线性分类器，来尽量分开这两类样本：</p>
<p><img src="/img/16_09_04/018.png" alt=""></p>
<p>这个分类问题中，特征空间的维度是多少呢？假设我们使用$50*50$像素的图片，虽然这个图片尺寸很小，但依然有2500个像素点。因此我们的特征数量$n$是2500个，特征向量$X$包含了所有像素点的亮度值。如果存储的是每个像素点的灰度值（典型的计算机图片表示方法），那么每个元素值应该介于0到255之间。如果图片存储形式是RGB模式，每个像素点包含红、绿、蓝三个子像素，那么$n=7500$。</p>
<p><img src="/img/16_09_04/019.png" alt=""></p>
<p>因此，如果我们非要通过包含所有的二次项来解决这个非线性问题，那么，这个式子的所有条件的个数大约是300万个：</p>
<p><img src="/img/16_09_04/020.png" alt=""></p>
<p>显然，这数字大的有些离谱了，计算成本太高了。</p>
<h2 id="神经网络和大脑">神经网络和大脑</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/IPmzw/neurons-and-the-brain" target="_blank" rel="external">视频地址</a></p>
<p>神经网络是一种很古老的算法，它最初产生的目的是制造能模拟大脑的机器，在这门课中，我将向你们介绍神经网络。因为它能很好的解决不同的机器学习问题而不只因为它们在逻辑上行得通。这节课中，主要介绍一些神经网络的背景知识，由此我们能知道可以用它们来做什么。</p>
<p>神经网络产生的原因是人们想尝试设计出模仿大脑的算法。从某种意义上说，想要建立学习系统，那么为什么不去模仿我们所认识的，最神奇的学习机器–人类的大脑呢？</p>
<p>神经网络逐渐兴起于二十世纪八九十年代应用得非常广泛，但由于各种原因，在90年代的后期应用减少了，但是最近，神经网络又东山再起了，其中一个原因是，神经网络是计算量有些偏大的算法。然而，大概是由于近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络，正是由于这个原因，和其他一些我们后面会讨论到的技术因素，使得如今的神经网络对于许多应用来说是最先进的技术。</p>
<p>当你想模拟大脑时，是指想制造出与人类大脑作用效果相同的机器，对吧？大脑可以学会去以看而不是听的方式处理图像，学会处理我们的触觉，我们能学习数学，学着做微积分。而且大脑能处理各种不同的令人惊奇的事情，似乎如果你想要模仿它，你得写很多不同的软件来模拟所有大脑告诉我们的这些五花八门的奇妙的事情。不过能不能假设大脑做所有这些不同事情的方法不需要用上千个不同的程序去实现，相反的，大脑处理的方法只需要一个单一的学习算法就可以了？尽管这只是一个假设，不过我可以和你分享一些这方面的证据：</p>
<p><img src="/img/16_09_04/021.png" alt=""></p>
<p>大脑的这一小片红色区域，是你的听觉皮层。你现在正在听到的声音，靠的是耳朵接收到声音信号，并把声音信号传递给你的听觉皮层，正因如此你才能明白我的话。</p>
<p>神经系统科学家做了下面这个有趣的实验：</p>
<p><img src="/img/16_09_04/022.png" alt=""></p>
<p>把耳朵到听觉皮层的神经切断，在这种情况下将其重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层。如果这样做了，那么结果表明听觉皮层将会学会“看”，这里“看”代表了我们所知道的每层含义。所以如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。</p>
<p>来看另一个例子：</p>
<p><img src="/img/16_09_04/023.png" alt=""></p>
<p>这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的。如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能学会”看“。这个实验和其它一些类似的实验被称为<strong>神经重接实验</strong>。</p>
<p>从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉、听觉和触觉，而不是需要运行上千个不同的程序，或者上千个不同的算法来做这些大脑所完成的成千上万的美好事情。也许我们需要做的就是找出一些近似的，或实际的大脑学习算法，然后实现它。大脑通过自学掌握如何处理这些不同类型的数据，在很大的程度上可以猜想如果我们把几乎任何一种传感器接入到大脑的几乎任何一个部位的话，大脑就会学会处理它。</p>
<p>下面再举几个例子：</p>
<p><img src="/img/16_09_04/024.png" alt=""></p>
<p>这张图是用舌头学会“看”的一个例子，这实际上是一个名为BrainPort的系统，它现在正在FDA(美国食品和药物管理局)的临床试验阶段，它能帮助失明人士看见事物。它的原理是：你在前额上带一个灰度摄像头，面朝前它就能获取你面前事物的低分辨率的灰度图像，你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应一个暗像素，电压值低的点对应于亮像素，即使依靠它现在的功能，使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。</p>
<p>这是第二个例子：</p>
<p><img src="/img/16_09_04/025.png" alt=""></p>
<p>关于人体回声定位(或者说人体声纳)，你可以通过弹响指或者咂舌头来实现，现在有失明人士确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。如果你搜索YouTube之后就会发现，有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球但是通过打响指，他可以四处走动而不撞到任何东西。他能滑滑板，他可以将篮球投入篮框中，注意这是一个没有眼球的孩子。</p>
<p>第三个例子是触觉皮带：</p>
<p><img src="/img/16_09_04/026.png" alt=""></p>
<p>如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感。用类似于鸟类感知方向的方式。</p>
<p>还有一些离奇的例子：</p>
<p><img src="/img/16_09_04/027.png" alt=""></p>
<p>如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。</p>
<p>因此，如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法，并处理这些数据。</p>
<p>从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执行 大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。<strong>人工智能的梦想就是有一天能制造出真正的智能机器</strong>，当然我不是教神经网络的，介绍它只因为它可能为我们打开一扇进入遥远的人工智能梦的窗户。对于我个人来说，它也是我研究生涯中致力于的一个项目，但我在这节课中讲授神经网络的原因，主要是对于现代机器学习应用，它是最有效的技术方法。因此在接下来的一些课程中，我们将开始深入到神经网络的技术细节。那么你就可以将它们应用到现代机器学习的应用中，并利用它们很好地解决问题。但对我来说，使我兴奋的原因之一，就是它或许能给我们一些启示，让我们知道当我们在思考未来有什么样的算法能以与人类相似的方式学习时，我们能做些什么。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/09/04/斯坦福机器学习课程 第三周 (4)正则化：解决过拟合问题/" itemprop="url">
                斯坦福机器学习课程 第三周 (4)正则化：解决过拟合问题
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-09-04T23:10:00+08:00" content="2016-09-04">
            2016-09-04
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/09/04/斯坦福机器学习课程 第三周 (4)正则化：解决过拟合问题/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/09/04/斯坦福机器学习课程 第三周 (4)正则化：解决过拟合问题/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><blockquote>
<p>机器学习模型需要拥有很好地泛化能力来适应训练集中没有出现过的新样本。</p>
<p>在这个节中，我们将介绍正则化方面的知识，这有助于预防过拟合的问题的产生。</p>
</blockquote>
<h2 id="解决过拟合问题">解决过拟合问题</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/ACpTQ/the-problem-of-overfitting" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>到现在为止，你已经见识了几种不同的学习算法。包括线性回归和逻辑回归，它们能够有效地解决许多问题，但是当将它们应用到某些特定的机器学习应用时，会遇到<strong>过度拟合(over-fitting)</strong>的问题，可能会导致它们效果很差。</p>
<p>在这一节中，我将为你解释什么是过度拟合问题，并且在此之后接下来的几节中，我们将谈论一种称为<strong>正则化(regularization)</strong>的技术，它可以改善或者减少过度拟合问题，以使学习算法更好实现。</p>
</blockquote>
<h3 id="梯度下降中的过拟合问题">梯度下降中的过拟合问题</h3><p>让我们继续使用那个线性回归来预测房价的例子。</p>
<h4 id="欠拟合">欠拟合</h4><p>我们通过建立以住房面积为自变量的函数来预测房价，我们可以对该数据做线性回归：</p>
<p><img src="/img/16_09_04/028.png" width="300" height="200" align="center"></p>
<p>但是这不是一个很好的模型，我们来看看这些数据，很明显，随着房子面积增大，住房价格的变化趋势趋于稳定，或者往右越平缓。因此这个算法没有很好地拟合训练数据。</p>
<p>我们把这个问题称为<strong>欠拟合(under fitting)</strong>，这个问题的另一个术语叫做<strong>高偏差(High bias)</strong>。</p>
<p>这两种说法大致相似，意思是它只是没有很好地拟合训练数据。</p>
<blockquote>
<p><strong>bias</strong>这个词是过去传下来的一个专业名词，它的意思是如果拟合一条直线，到训练数据，就好像算法有一个很强的偏见，或者说非常大的偏差。</p>
<p>因为该算法认为房子价格与面积仅仅线性相关，尽管与该数据的事实相反，它还是接近于拟合一条直线，而此法最终导致拟合数据效果很差。</p>
</blockquote>
<h4 id="刚好合适">刚好合适</h4><p>当我们加入一个二次项，在这组数据中，我们用二次函数来拟合它：</p>
<p><img src="/img/16_09_04/029.png" width="300" height="200" align="center"></p>
<p>事实证明这个拟合效果很好。</p>
<h4 id="过拟合">过拟合</h4><p>另一个极端情况是，如果我们拟合一个四次多项式，因此在这里我们用五个参数来拟合这五个训练样本，你可以得到看上去像这样的一条曲线：</p>
<p><img src="/img/16_09_04/030.png" width="300" height="200" align="center"></p>
<p>一方面似乎对训练数据做了一个很好的拟合，因为这条曲线通过了所有的训练实例。但是，这任然是一条扭曲的曲线。它不停上下波动，因此事实上，我们并不认为它是一个预测房价的好模型。</p>
<p>这个问题我们把他叫做<strong>过度拟合(over fitting)</strong>，另一个描述该问题的术语是<strong>高方差(variance)</strong>。</p>
<p>介于<strong>高偏差</strong>和<strong>高方差</strong>这两者之间的情况叫做<strong>“刚好合适”</strong>。</p>
<p>概括地说，过拟合的问题将会在变量过多的时候发生，这种时候训练出的方程总能很好的拟合训练数据，所以你的代价函数实际上可能非常接近于0，甚至就是0。但是这会导致这种模型无法<strong>泛化</strong>到新的数据样本中，以至于无法预测新样本价格。</p>
<p>在这里术语<strong>“泛化(generalize)”</strong>指的是一个假设模型能够应用到新样本的能力。</p>
<h3 id="逻辑回归中的过拟合问题">逻辑回归中的过拟合问题</h3><p>类似的方法同样可以应用到<strong>逻辑回归</strong>。</p>
<p>这里是一个以$x_{1}$，$x_{2}$为变量的逻辑回归。</p>
<p><img src="/img/16_09_04/031.png" width="300" height="200" align="center"></p>
<h4 id="欠拟合-1">欠拟合</h4><p>我们先用下面这个假设函数来拟合：</p>
<p>$$<br>h_{\theta}(x) = g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2})<br>$$</p>
<p>$$<br>(g=sigmoid\ function)<br>$$</p>
<p>拟合的结果如下：</p>
<p><img src="/img/16_09_04/032.png" width="300" height="200" align="center"></p>
<p>它直接分开了正样本和负样本，但这个模型并不能够很好地拟合数据。因此这是一个<strong>欠拟合</strong>的例子。或者说假设模型具有高偏差。</p>
<h4 id="刚好适合">刚好适合</h4><p>相比之下，如果我们再加入一些变量，改用这样一个多项式：</p>
<p>$$<br>h_{\theta}(x) = g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{1}^{2} + \theta_{4}x_{2}^{2} + \theta_{5}x_{1}x_{2})<br>$$</p>
<p>那么你可以得到一个能比较好的拟合数据的判定边界：</p>
<p><img src="/img/16_09_04/033.png" width="300" height="200" align="center"></p>
<h4 id="过拟合-1">过拟合</h4><p>在另一种极端情况下，如果你用高阶多项式来拟合数据：</p>
<p>$$<br>h_{\theta}(x) = g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{1}^{2} + \theta_{3}x_{1}^{2}x_{2} + \theta_{4}x_{1}^{2}x_{2}^{2} + \theta_{5}x_{1}^{2} x_{2}^{3} + \theta_{6}x_{1}^{3}x_{2} + …)<br>$$</p>
<p>那么逻辑回归可能发生自身扭曲：</p>
<p><img src="/img/16_09_04/034.png" width="300" height="200" align="center"></p>
<p>它会千方百计的来形成扭曲以至于成为一条能尽量拟合每一个样本的拟合曲线。</p>
<p>这是一个<strong>过拟合</strong>的例子，是一个高方差假设模型，它不能够很好泛化到新样本。</p>
<h4 id="解决过拟合问题-1">解决过拟合问题</h4><p>当我们使用一维或二维数据时，我们可以通过绘制出假设模型的图像来研究问题所在，再选择合适的多项式来拟合数据。因此在以之前的房屋价格为例，我们可以绘制假设模型的图像就能看到模型的曲线非常扭曲，并通过所有的样本房价：</p>
<p><img src="/img/16_09_04/035.png" width="300" height="200" align="center"></p>
<p>我们可以通过绘制这样的图形来选择合适的多项式阶次，但当我们遇到有很多变量的假设模型时，我们就无法绘制出图像了：</p>
<p><img src="/img/16_09_04/036.png" width="300" height="200" align="center"></p>
<p>当我们有过多变量，同时只有非常少的训练数据时，就会出现过度拟合的问题。</p>
<p>为了解决过度拟合问题，有两种方法：</p>
<ul>
<li><p>1.要尽量减少选取的变量的数量。</p>
<ul>
<li>具体而言我们可以人工检测变量的条目，保留重要的特征，舍弃无用的特征</li>
<li>模型选择算法（后面的课程会提到）</li>
</ul>
</li>
<li><p>2.正则化</p>
<ul>
<li>保留所有的特征变量，但是通过减少参数$\theta_{j}$的数量级或值的大小。</li>
</ul>
</li>
</ul>
<p><strong>正则化</strong>相关的知识点将在接下来的内容中介绍。</p>
<h2 id="代价函数">代价函数</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/B1MnL/cost-function" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在这一节中，我将传达给你一个关于正则化的直观的感受，并且我们还要写出使用正则化时需要使用的代价函数。</p>
</blockquote>
<p>在上一节中，我们看到了如果说我们要用一个二次函数来拟合这些数据，拟合的效果会很好。</p>
<p><img src="/img/16_09_04/037.png" width="300" height="200" align="center"></p>
<p>然而我们用一个更高次的多项式去拟合，我们最终可能得到一个曲线，能非常好的拟合训练集，但是这真的不是一个好的结果。</p>
<p><img src="/img/16_09_04/038.png" width="300" height="200" align="center"></p>
<p>它过度拟合了数据，因此泛化性并不是很好。</p>
<p>让我们考虑下面的假设：</p>
<p>我们想要加上惩罚项，从而使参数$\theta_{3}$和$\theta_{4}$足够的小。这里我的意思就是，尽量减少代价函数的均方误差。</p>
<p><img src="/img/16_09_04/039.png" width="300" height="200" align="center"></p>
<p>对于上面这个函数，我们对它添加一些项：</p>
<p><img src="/img/16_09_04/040.png" width="300" height="200" align="center"></p>
<p>现在，如果想最小化这个函数，我们需要尽量减少$\theta_{3}$和$\theta_{4}$的值。</p>
<p>如果我们减少了这两项的值，就像是在上面的式子中忽略了后面的两项：</p>
<p><img src="/img/16_09_04/041.png" width="300" height="200" align="center"></p>
<p>最终得到一个近似的二次函数，从而达到恰当地拟合数据的表达式。</p>
<hr>
<p>在这个具体的例子中，我们看到了，惩罚$\theta_{3}$和$\theta_{4}$这两个大的参数值的效果。</p>
<p>所以推广到更通用的情况下，这里给出了正规化背后的思路。这种思路就是如果我们的参数值对应一个较小的值的话，那么往往我们会得到一个形式更简单的假设函数。</p>
<blockquote>
<p>所以我们最后一个例子中我们惩罚的只是$\theta_{3}$和$\theta_{4}$，使这两个数值均接近于零。我们从而得到了一个更简单的假设。</p>
<p>通常情况下来讲，参数值减少的越多，函数越光滑，因此就不易发生过拟合的问题。</p>
</blockquote>
<hr>
<h3 id="具体例子">具体例子</h3><p>来让我们看看具体的例子：</p>
<p>对于房屋价格预测，我们可能有上百种特征，我们谈到了一些可能的特征，比如说:</p>
<ul>
<li>$x_{1}$是房屋的尺寸</li>
<li>$x_{2}$是卧室的数目</li>
<li>$x_{3}$是房屋的层数</li>
<li>…</li>
</ul>
<p>我们可能有一百个特征。</p>
<p>跟前面的多项式例子不同，我们是不知道$\theta_{3}$,$\theta_{4}$是高阶多项式的项，即如果我们有一百个特征，我们是很难提前选出那些关联度更小的特征的。我们不知道如何选择参数来缩小参数的数目。</p>
<p>因此在正规化中，我们要做的事情，就是通过修改我们的代价函数（这里就是线性回归的代价函数）</p>
<p><img src="/img/16_09_04/042.png" width="300" height="200" align="center"></p>
<p>从而缩小我所有的参数值，修改方式就是在这后面添加一项：</p>
<p><img src="/img/16_09_04/043.png" width="300" height="200" align="center"></p>
<p>当我在最后添加一个额外的正则化项的时候，我们收缩了每个参数，并且因此 我们会使我们所有的参数$\theta_{1}.\theta_{2}.\theta_{3}.\theta_{4}…\theta_{100}$的值变小。</p>
<p>顺便说一下，按照惯例来讲，我们是从$\theta_{1}$开始的：</p>
<p><img src="/img/16_09_04/044.png" width="300" height="200" align="center"></p>
<p>所以我实际上没有去惩罚$\theta_{0}$，因此$\theta_{0}$的值是大的。</p>
<blockquote>
<p>在实际情况中，是否包含$\theta_{0}$只会有非常小的差异。</p>
</blockquote>
<h3 id="正规化优化的目标">正规化优化的目标</h3><p>我们正规化之后的代价函数如下：</p>
<p><img src="/img/16_09_04/045.png" width="300" height="200" align="center"></p>
<p>在后面的正规化项中的$λ$我们称为<strong>正规化参数</strong>。</p>
<p>$λ$要做的就是控制在两个不同的目标中的一个平衡关系：</p>
<ul>
<li>第一个目标就是：我们想要训练使假设函数更好的拟合训练数据。</li>
<li>第二个目标就是：我们想要保持参数值较小。</li>
</ul>
<p>参数$λ$就是用来控制这两者之间的平衡，目标就是平衡<strong>拟合训练</strong>的目的和<strong>保持参数值较小</strong>的目的。（即欠拟合和过拟合的平衡）</p>
<p>对于我们房屋价格预测的例子来说，尽管我们之前已经用非常高的高阶多项式来拟合，我们将会得到一个非常弯曲和复杂的曲线函数，就像下面这样：</p>
<p><img src="/img/16_09_04/046.png" width="300" height="200" align="center"></p>
<p>但现在我们不这样了，你只需要确保使用了正规化目标的方法，那么你就可以得到一个类似于二次函数但并不是真正的二次函数的一条曲线：</p>
<p><img src="/img/16_09_04/047.png" width="300" height="200" align="center"></p>
<p>这样我们就得到了对于这个数据来说更好的假设函数。</p>
<hr>
<p>在正规化线性回归中，如果正规化参数值被设定为非常大，那么会发生什么呢？</p>
<p>我们将会非常大地惩罚我们的参数，会使得所有的参数接近于0。如果我们这么做，那么就相当于我们从假设函数中去掉了这些项，并且使得我们只是留下了一个简单的假设：</p>
<p><img src="/img/16_09_04/048.png" width="300" height="200" align="center"></p>
<p>$$<br>h_{\theta}(x) = \theta_{0}<br>$$</p>
<p>那就是类似于拟合了一条水平直线：</p>
<p><img src="/img/16_09_04/049.png" width="300" height="200" align="center"></p>
<p>对于数据来说这就是<strong>欠拟合(underfitting)</strong>。</p>
<p>换句话说，就是这种假设有过于强烈的“偏见”或者过高的偏差（bais），认为预测的价格只是等于$\theta_{0}$。</p>
<p>因此为了使正则化运作的良好，我们应该去选择一个不错的正则化参数$λ$，并且当我们讲到多重选择时，我们将会介绍一种方法来<strong>自动选择正则化参数$λ$</strong>。这就是高度正则化的思路。</p>
<h2 id="正则化线性回归">正则化线性回归</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/QrMXd/regularized-linear-regression" target="_blank" rel="external">视频地址</a></p>
<p>对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降、一种基于正规方程。</p>
<p>在这段视频中，我们将继续学习这两个算法，并把它们推广到正则化线性回归中去。</p>
<h3 id="基于梯度下降">基于梯度下降</h3><p>这是我们上节课推导出的正则化线性回归的优化目标：</p>
<p><img src="/img/16_09_04/050.png" width="300" height="200" align="center"></p>
<p>前面第一部分是一般线性回归的目标函数，后半部分是正则化项。其中$\lambda$是正则化参数。我们想找到能最小化代价函数的参数$\theta$。</p>
<p>之前，我们使用梯度下降求解原来没有正则项的代价函数：</p>
<p><img src="/img/16_09_04/051.png" width="300" height="200" align="center"></p>
<p>我们会反复更新参数$\theta_{j}$，其中$j=0,1,2,3,…,n$。</p>
<p>让我们把$j=0$的情况单独写出来：</p>
<p><img src="/img/16_09_04/052.png" width="300" height="200" align="center"></p>
<blockquote>
<p>我只是把$\theta_{0}$的更新分离出来，剩下的这些参数($\theta_{j}$其中$j=1,2,3,…,n$)更新作为另一部分，所以这样做其实没有什么变化。仅仅是把$\theta_{0}$的更新从其他参数中分离出来。</p>
<p>我这样做的原因是因为上节课提到的，对于正则化的线性回归，我们惩罚参数从$\theta_{1}$开始，我们不惩罚$\theta_{0}$。所以当我们修改这个正则化线性回归的算法时，我们将对$\theta_{0}$有所不同。</p>
</blockquote>
<p>具体地说，如果我们要对这个算法进行修改，并用它求解正则化的目标函数，我们需要做的是把下面的这一项加上正则化项：</p>
<p><img src="/img/16_09_04/053.png" width="500" height="200" align="center"></p>
<p>如果这样做的话，那么你就有了用于最小化正则化代价函数$J(\theta)$的梯度下降算法。</p>
<p>对于上面式子中第二项：</p>
<p><img src="/img/16_09_04/054.png" width="500" height="200" align="center"></p>
<p>可以改写为这种形式：</p>
<p><img src="/img/16_09_04/055.png" width="500" height="200" align="center"></p>
<p>具体来讲，这一项的值:</p>
<p><img src="/img/16_09_04/056.png" width="100" height="200" align="center"></p>
<p>通常是一个<strong>小于1的具体的实数</strong>（通常情况下是一个比1小一点点的值），所以我们可以把它想成一个像$0.99$一样的数字。</p>
<p>所以对$\theta_{j}$更新的结果我们可以看作是被替换为$\theta_{j}$的0.99倍（把$\theta_{j}$向0压缩了一点点）。</p>
<p>另外后面的这一项：</p>
<p><img src="/img/16_09_04/057.png" width="300" height="200" align="center"></p>
<p>实际上与我们原来的梯度下降更新完全一样。</p>
<hr>
<p>当我们使用正则化线性回归时，我们需要做的就是在每一个被正则化的参数$\theta_{j}$上乘以了一个比1小一点点的数字，然后，我们执行跟以前一样的更新。</p>
<h3 id="基于正规方程">基于正规方程</h3><p>梯度下降只是拟合线性回归模型的两种算法之一，另一种算法是基于正规方程。</p>
<p>我们的做法是建立这个设计矩阵X:</p>
<p><img src="/img/16_09_04/058.png" width="300" height="200" align="center"></p>
<p>其中每一行对应于一个单独的训练样本。</p>
<p>然后创建一个m维度的向量y，包含了所有训练集里的标签：</p>
<p><img src="/img/16_09_04/059.png" width="300" height="200" align="center"></p>
<p>所以X是一个$m \times (n+1)$维矩阵。</p>
<p>为了最小化代价函数$J$:</p>
<p><img src="/img/16_09_04/060.png" width="150" height="200" align="center"></p>
<p>我们发现一个办法就是让$\theta$等于这个式子：</p>
<p>$$<br>\theta=(X^{T}X)^{-1}X^{T}y<br>$$</p>
<p>这是没有使用正则化的表达式。</p>
<p>如果我们使用了正则化，我们想要得到最小值，那么我们的推导方式如下：</p>
<p>取$J$关于各个参数的偏导数，并令他们等于0：</p>
<p><img src="/img/16_09_04/061.png" width="150" height="200" align="center"></p>
<p>然后通过一些数学推导，你可以得到这样的式子：</p>
<p>$$<br>\theta=(X^{T}X + \lambda<br>\begin{bmatrix}<br>     0 &amp; 0 &amp; … &amp; 0 \\<br>     0 &amp; 1 \\<br>     … &amp; … &amp; … \\<br>     0 &amp; 0 &amp; … &amp; 1 \\<br>\end{bmatrix}<br> )^{-1}X^{T}y<br>$$</p>
<p>其中正则化矩阵左上角的元素是0，其余对角线元素都是1，剩下的元素也都是0。矩阵的维度是$(n+1)\times(n+1)$。</p>
<h4 id="举个具体的例子来说明：">举个具体的例子来说明：</h4><p>假设$m=2$，那么这里我们的矩阵是一个$3\times3$维的矩阵。这个矩阵看起来就是下面这种形式：</p>
<p>$$<br>\begin{bmatrix}<br>     0 &amp; 0 &amp; 0 \\<br>     0 &amp; 1 &amp; 0 \\<br>     0 &amp; 0 &amp; 1 \\<br>\end{bmatrix}<br>$$</p>
<p>如果你采用新定义包含正则项的$J(\theta)$，那么这个计算$\theta$的式子，能使你的$J(\theta)$达到全局最小值。</p>
<h3 id="不可逆问题(*选学)">不可逆问题(*选学)</h3><p>最后，我想快速地谈一下不可逆性的问题，这部分是比较高阶的内容，所以这一部分还是作为选学。</p>
<p>之前当我讲正规方程的时候，我们也有一段讲不可逆问题的选学视频，这节视频是作为那一节的补充。</p>
<p>现在考虑m（即样本总数）小于等于特征数量n：</p>
<p>$$<br>m \le n<br>$$ </p>
<p>如果你的样本数量比特征数量小的话，那么这个矩阵$X^{T}X$将是<strong>不可逆</strong>的或<strong>奇异(singluar)</strong>的。</p>
<p>幸运的是，正规化也为我们解决了这个问题，具体地说，只要正则参数是严格大于0的：</p>
<p>$$<br>\lambda \gt 0<br>$$</p>
<p>实际上是可以证明：</p>
<p>$$<br>\theta=(X^{T}X + \lambda<br>\begin{bmatrix}<br>     0 &amp; 0 &amp; … &amp; 0 \\<br>     0 &amp; 1 \\<br>     … &amp; … &amp; … \\<br>     0 &amp; 0 &amp; … &amp; 1 \\<br>\end{bmatrix}<br> )^{-1}X^{T}y<br>$$</p>
<p>中</p>
<p>$$<br>X^{T}X + \lambda<br>\begin{bmatrix}<br>     0 &amp; 0 &amp; … &amp; 0 \\<br>     0 &amp; 1 \\<br>     … &amp; … &amp; … \\<br>     0 &amp; 0 &amp; … &amp; 1 \\<br>\end{bmatrix}<br>$$</p>
<p>这个矩阵是可逆的。</p>
<p>因此，使用正则化还可以照顾一些$X^{T}X$不可逆的问题</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/09/04/斯坦福机器学习课程 第三周 (3)多类别分类问题：一对多/" itemprop="url">
                斯坦福机器学习课程 第三周 (3)多类别分类问题：一对多
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-09-04T23:00:00+08:00" content="2016-09-04">
            2016-09-04
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/09/04/斯坦福机器学习课程 第三周 (3)多类别分类问题：一对多/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/09/04/斯坦福机器学习课程 第三周 (3)多类别分类问题：一对多/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="一对多分类问题">一对多分类问题</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/68Pol/multiclass-classification-one-vs-all" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在本节视频中，我们将谈到如何使用<strong>逻辑回归来解决多类别分类问题</strong>。具体来说，我想通过一个叫做<strong>“一对多”(one-vs-all)</strong>的分类算法，让你了解什么是多类别分类问题。</p>
</blockquote>
<p>先看这样一些例子：</p>
<ul>
<li>邮件分类器自动将邮件按照以下标签分类：工作，朋友，家庭，兴趣</li>
<li>医学根据病人的得病类型分类：没有病，感冒，流感</li>
<li>天气预报根据天气类型分类：晴天，多云，雨天，雪天</li>
</ul>
<p>我们可以分别对这些类别打上数值标签，比如天气分类中：晴天代表<code>y=1</code>，多云代表<code>y=2</code>，雨天代表<code>y=3</code>，雪天代表<code>y=4</code>。</p>
<p>以上这些例子都属于多类别分类问题。</p>
<hr>
<p>二元分类问题和多类别分类问题的比较：</p>
<p><img src="/img/16_09_04/001.png" alt=""></p>
<p>二元分类问题中，我们只有两个类别需要处理，而在多类别分类问题中，我们有多种类别需要处理。这里我们用三种不同的符号代表三种类别。</p>
<p>在二元分类中，我们知道我们可以使用逻辑回归来将两种类别的数据集一分为二：</p>
<p><img src="/img/16_09_04/002.png" alt=""></p>
<p>对于一对多的分类问题，我们也可以借鉴二元分类的思想。下面就来介绍一些如何进行一对多分类工作(有时也被称为“一对余”方法)。</p>
<p><img src="/img/16_09_04/003.png" alt=""></p>
<p>如图，我们现在有一组由三个类别元素组成的训练集。接下来我们要做的就是使用一个训练集将其分成三个二元分类问题。我们先从三角形开始，将其他两种类型视为同一种类型：</p>
<p><img src="/img/16_09_04/004.png" alt=""></p>
<p>实际上这是一个新的“伪类型集”，类型2和类型3被定义为负类(值为0)，类型1定义为正类(值为1)。我们要拟合出一个合适的分类器，来区分正类和负类。我们称这个分类器为$h_{\theta}^{(1)}(x)$</p>
<p><img src="/img/16_09_04/005.png" alt=""></p>
<p>同理，对类型2也做同样的处理，得到分类器$h_{\theta}^{(2)}(x)$：</p>
<p><img src="/img/16_09_04/006.png" alt=""></p>
<p>最后，对类型三也做同样处理，得到分类器$h_{\theta}^{(3)}(x)$：</p>
<p><img src="/img/16_09_04/007.png" alt=""></p>
<p>这样我们就得到了三个分类器，通过这些分类器来估算出给出$x$和先验$\theta$时，$y=i$的概率：</p>
<p>$$<br>h_{\theta}^{(i)}(x) = P(y=i|x;\theta)    \qquad  (i=1,2,3)<br>$$</p>
<p>总之，现在我们要做的就是训练这个逻辑回归分类器$h_{\theta}^{(i)}(x)$，其中$i$对应每一个可能的$y=i$。</p>
<p>最后，为了做出预测，我们给定一个新的输入值$x$，用来做预测，我们要做的就是在我们的三个分类器里输入$x$，然后我们选择一个让$h_{\theta}^{(i)}(x)$最大的$i$，这就是预测出的类别。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/" itemprop="url">
                斯坦福机器学习课程 第三周 (2)Logistic回归模型
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-08-25T23:54:00+08:00" content="2016-08-25">
            2016-08-25
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/08/25/斯坦福机器学习课程 第三周 (2)Logistic回归模型/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="逻辑回归的代价函数">逻辑回归的代价函数</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function" target="_blank" rel="external">视频地址</a></p>
<p>这节课将讲解逻辑回归中，如何定义用来拟合参数的<strong>代价函数</strong>。</p>
<p><img src="/img/16_08_25/001.png" alt=""></p>
<p>如上图所示，我们现在有一组训练集，其中有$m$个训练样本，由于是一个分类问题，我们的$y$取值范围就是0和1。并且我们给出了假设函数$h_{\theta}(x)$。现在的问题就是，我们如何确定我们的参数$\theta$的值呢？也就是说如何<strong>拟合参数$\theta$</strong>呢？</p>
<p><img src="/img/16_08_25/002.png" alt=""></p>
<p>上图是之前求线性回归问题时的代价函数，现在我将上图的公式进行改写：</p>
<p>$$<br>Cost(h_{\theta}(x^{(i)}),y^{(i)})=<br>\frac{1}{2}(h_{θ}(x^{(i)})-y^{(i)})^{2}<br>$$</p>
<p>为了简写方便，去掉上标之后的代价函数如下：</p>
<p>$$<br>Cost(h_{\theta}(x),y)=<br>\frac{1}{2}(h_{θ}(x)-y)^{2}<br>$$</p>
<p>在线性回归问题中，代价函数会被定义为以上这种形式，但在逻辑回归里，如果我们最小化代价函数后，使用这个最小代价的值也是可以正常工作的，但是我们的这个代价函数会变成参数$\theta$的非凸函数。</p>
<p>因为这里：</p>
<p>$$<br>\begin{align*}<br>h_{\theta}(x)<br>= \frac{1}{1+e^{-\theta^{T}x}}<br>\end{align*}<br>$$ </p>
<p>$h_{\theta}(x)$是非线性的，如果将这个值带入Cost函数，然后在求代价函数$J(\theta)$，我们会得到如下的有多个局部最优的函数：</p>
<p><img src="/img/16_08_25/003.png" alt=""></p>
<p>这种函数的官方名称叫做：<strong>非凸函数</strong>。</p>
<p>你大概可以感觉到如果把梯度下降算法用在这样一个函数上，不能保证它会收敛到全局最小值。</p>
<p>相应的，我们期望我们的代价函数$J(\theta)$是一个凸函数：</p>
<p><img src="/img/16_08_25/004.png" alt=""></p>
<p>对于这样的函数，我们使用梯度下降算法，才能够保证顺利的收敛到最小值。</p>
<p>这里导致我们的代价函数是非凸函数的根本原因就在于使用逻辑回归的那一步：</p>
<p>$$<br>\begin{align*}<br>h_{\theta}(x)<br>= \frac{1}{1+e^{-\theta^{T}x}}<br>\end{align*}<br>$$ </p>
<p>所以我们想做的就是，<strong>另找一个是凸函数的代价函数</strong>使得我们能够方便的找到代价函数的全局最小值。</p>
<p>因此对于<strong>逻辑回归的代价函数</strong>，形式如下：</p>
<p><img src="/img/16_08_25/005.png" alt=""></p>
<p>对应的函数的图形如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$y=1$时</th>
<th style="text-align:center">$y=0$时</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/16_08_25/006.png" alt=""></td>
<td style="text-align:center"><img src="/img/16_08_25/007.png" alt=""></td>
</tr>
</tbody>
</table>
<p>现在这种形式的代价函数，有着很多很有趣的性质：</p>
<ul>
<li>在$y=1$的情况下：<ul>
<li>如果假设函数$h_{\theta}(x) = 1$，那么代价函数为0:$Cost=0$。</li>
<li>如果假设函数$h_{\theta}(x) \rightarrow 0$，那么代价函数趋近于∞:$Cost \rightarrow ∞$</li>
</ul>
</li>
<li>在$y=0$的情况下：<ul>
<li>如果假设函数$h_{\theta}(x) \rightarrow 1$，那么代价函数为0:$Cost \rightarrow ∞ $。</li>
<li>如果假设函数$h_{\theta}(x) = 0$，那么代价函数等于0:$Cost = 0$</li>
</ul>
</li>
</ul>
<p>这就是我们所期望的效果：<strong>当假设函数和期望值越接近时，代价函数值越小，反之越大。</strong></p>
<h2 id="简化代价函数以及梯度下降">简化代价函数以及梯度下降</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent" target="_blank" rel="external">视频地址</a></p>
<p>这节课我们来讲解一种<strong>稍微简单一些的方式来写代价函数</strong>，来替代我们现有的方法。同时也要讲清楚，<strong>如何运用梯度下降法来拟合出逻辑回归的参数$\theta$</strong>。因此，听了这节课，你应该就能够实现一个完整的逻辑回归算法了。</p>
<h3 id="简化单个样本的代价函数$Cost(h_{\theta}(x),y)$">简化单个样本的代价函数$Cost(h_{\theta}(x),y)$</h3><p>之前我们讲解了逻辑回归中的代价函数：</p>
<p><img src="/img/16_08_25/008.png" alt=""></p>
<p>上图中有：</p>
<ul>
<li>我们定义的逻辑回归的整体的代价函数$J(\theta)$</li>
<li>针对于单个样本的代价函数$Cost(h_{\theta}(x),y)$</li>
<li>并且我们知道，作为分类问题，我们的$y$的取值永远都是0或者1。</li>
</ul>
<p>其中对于单个样本的代价函数$Cost(h_{\theta}(x),y)$，我们可以把$y=0$和$y=1$的两种情况合二为一，写成一个统一的式子：</p>
<p>$$<br>Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))<br>$$</p>
<p>这个表达式和我们之前写的代价函数的表达式，效果是完全一样的，但后者更加紧凑。</p>
<blockquote>
<p>我们知道在上面的式子中，$y$的取值非0即1，如果$y=1$那么$Cost(h_{\theta}(x),y)=-log(h_{\theta}(x))$，如果$y=0$那么$Cost(h_{\theta}(x),y)=-log(1-h_{\theta}(x))$。效果和分开写是完全一样的。</p>
</blockquote>
<h3 id="拟合逻辑回归的参数$\theta$">拟合逻辑回归的参数$\theta$</h3><p>这样一来，我们就可以写出完整的逻辑回归的代价函数了：</p>
<p><img src="/img/16_08_25/009.png" alt=""></p>
<blockquote>
<p>似乎我们有其他方式来表示单个样本的代价函数$Cost(h_{\theta}(x),y)$，但在这里不做详细的说明，但是我可以告诉你，现在我们使用的这种形式的代价函数的式子，是从统计学中的<strong>极大似然法</strong>得来的。不过如果你不理解什么是<strong>极大似然法</strong>，以及逻辑回归中的代价函数为什么是这种形式，也没关系，也不会影响你后面的学习。</p>
</blockquote>
<p>接下来的问题就是找到使得代价函数$j(\theta)$最小的参数$\theta$了：</p>
<p><img src="/img/16_08_25/010.png" alt=""></p>
<p>一旦我们得到了这个合适的$\theta$那么我们就可以对一个给定的样本进行预测了：</p>
<p><img src="/img/16_08_25/011.png" alt=""></p>
<blockquote>
<p>注意：$p(y=1|x;\theta)$就是指关于$x$以$\theta$为参数，$y=1$的概率。</p>
</blockquote>
<p>由此可见，我们问题的核心在于求解最小化代价函数：</p>
<p><img src="/img/16_08_25/012.png" alt=""></p>
<p>上图就是逻辑回归问题中最小化代价函数的方法，是通过梯度下降算法来实现。</p>
<p>对上图中的偏导数项展开后就是下面的形式：</p>
<p><img src="/img/16_08_25/013.png" alt=""></p>
<p>如果你把上面的式子放到线性回归问题中进行对比的话，<strong>你会发现这个式子正是用来做线性回归梯度下降的。</strong></p>
<p>那么我们不禁要问一下自己，线性回归算法和逻辑回归算法是一样的吗？</p>
<p>要回答这个问题，就要看从线性回归，到逻辑回归，到底发生了哪些变化。</p>
<p>实际上对于假设函数$h_{\theta}(x)$的定义发生了变化：</p>
<ul>
<li>线性回归中，假设函数定义为$h_{\theta}(x)=\theta^{T}x$</li>
<li>在逻辑回归中，假设函数的定义为$h_{\theta}(x)= \frac{1}{1+e^{-\theta^{T}x}}$</li>
</ul>
<p>由于假设函数的不同，实际上线性回归和逻辑回归的梯度下降是两个完全不同的东西。</p>
<p>当我们在讨论线性回归的梯度下降算法时，我们谈到了如何监控梯度下降法以确保其收敛。其实我们通常也把这种方法运用在逻辑回归中，来检测梯度下降，以确保其正常收敛。这里就不多做介绍，可以参考<strong>斯坦福机器学习课程 第一周 (5)参数学习-梯度下降算法</strong>来实现。</p>
<h2 id="高级优化">高级优化</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/licwf/advanced-optimization" target="_blank" rel="external">视频地址</a></p>
<p>这节课中会介绍一些高级优化概念和技巧。利用这些技巧，我们就使通过梯度下降进行逻辑回归的速度大大提高，这也将使得算法更加适合解决大型的机器学习问题。</p>
<h3 id="优化算法概念引入">优化算法概念引入</h3><p>现在我们换个角度来看看什么是梯度下降：</p>
<p>我们有个代价函数$J(\theta)$，并且我们希望最小化代价函数$min_{\theta}J(\theta)$。</p>
<p>为了得到最小化代价函数$min_{\theta}J(\theta)$，我们需要计算出$J(\theta)$关于每一项$\theta$的偏导：</p>
<p><img src="/img/16_08_25/014.png" alt=""></p>
<p>有了上面的$J(\theta)$关于每一项$\theta$的偏导，为了求得合适的$\theta$，我们就可以利用梯度下降算法来递减$\theta$，使其接近全局最小值了：</p>
<p><img src="/img/16_08_25/015.png" alt=""></p>
<p>但事实上，<strong>梯度下降并不是我们可以使用的唯一算法</strong>，下面是我们常用的几种算法：</p>
<ul>
<li><strong>梯度下降算法(Gradient descent)</strong></li>
<li><strong>共轭梯度法(Conjugate gradient)</strong></li>
<li><strong>变尺度法(BFGS)</strong></li>
<li><strong>限制变尺度法(L-BFGS)</strong></li>
</ul>
<p>除了我们常说的梯度下降算法外的其他三种算法，都有着更复杂的方式来求最小化代价函数。但是这三种算法的细节，超出了本门课程的范畴(实际上如果你想要研究这些算法，可能你需要花费几周的时间，你可以专门学一门课程来提高数值计算能力)。不过我会告诉你他们的一些特性。</p>
<h3 id="优缺点">优缺点</h3><p>这三种算法有许多优点：</p>
<p><strong>优点</strong>：</p>
<ul>
<li>不需要手动选择学习率$a$<ul>
<li>所以对这些算法的一种思路是 给出计算导数项和代价函数$J(\theta)$的方法，你可以认为算法有一个智能的内部循环，这种智能的内部循环被称为<strong>线性搜索(line search)</strong>算法，它可以自动尝试不同的学习速率$a$，并自动选择一个好的学习速率$a$。因此它甚至可以在每次迭代时都选择不同的学习速率，那么你就不需要自己选择$a$了。</li>
</ul>
</li>
<li>通常情况下收敛速度比梯度下降算法更快<ul>
<li>这些算法实际上在做更复杂的事情，而不仅仅是选择一个好的学习速率$a$，所以他们往往最终收敛的远远快于梯度下降。不过关于它们是如何做到的细节，已经超过了本门课程的范畴。</li>
</ul>
</li>
</ul>
<blockquote>
<p>实际上，我过去十年我都在相当频繁的使用这几种算法。但直到前几年，我才真正搞清楚共轭梯度法、BFGS和L-BFGS的细节。因此，实际上你完全可以在不了解这些算法的内环间在做什么的的情况下使用这些算法，并应用于许多不同的学习问题。</p>
</blockquote>
<p>如果说这些算法优缺点的话，那么它的缺点就是：</p>
<p><strong>缺点</strong></p>
<ul>
<li>比梯度下降法复杂多了<ul>
<li>如果你不是数值计算方面的专家，不建议你亲自去写共轭梯度法、BFGS和L-BFGS，而是去使用一些现成的库。实际上Octave和Matlab都有很理想的库来实现这些先进的优化算法。值得一提的是，这些算法在不同语言上的实现，是有差别的，有的语言上的库可能会表现的不太好。所以建议对比选择合适的实现。</li>
</ul>
</li>
</ul>
<h3 id="使用实例">使用实例</h3><p>接下来来举例说明如何使用这些算法。</p>
<p>比如你有一个包含两个参数的问题：</p>
<p><img src="/img/16_08_25/016.png" alt=""></p>
<p>对于上图中的代价函数，我们可以求得当$\theta_{1}=5$,$\theta_{2}=5$时，$J(\theta)$的值最小。</p>
<p>下面就看看怎么使用Octave程序来计算$\theta_{1}$和$\theta_{2}$吧。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">function [jVal, gradient] = costFunction(theta)</div><div class="line"></div><div class="line">jVal = (theta(<span class="number">1</span>)<span class="number">-5</span>)^<span class="number">2</span> + (theta(<span class="number">2</span>)<span class="number">-5</span>)^<span class="number">2</span>;</div><div class="line"></div><div class="line">gradient = zeros(<span class="number">2</span>,<span class="number">1</span>);</div><div class="line">gradient(<span class="number">1</span>) = <span class="number">2</span>*(theta(<span class="number">1</span>)<span class="number">-5</span>);</div><div class="line">gradient(<span class="number">2</span>) = <span class="number">2</span>*(theta(<span class="number">2</span>)<span class="number">-5</span>);</div></pre></td></tr></table></figure>
<p>上面这段代码定义了一个函数<code>costFunction(theta)</code>，参数是一个<code>theta</code>向量，包含$\theta_{1}$和$\theta_{2}$，返回值是<code>jVal</code>和<code>gradient</code>。通过函数的定义，我们可以看出来<code>jVal</code>表示的是代价函数$J(\theta)$：</p>
<p><img src="/img/16_08_25/018.png" alt=""></p>
<p><code>gradient</code>是一个列量，其中两个值分别代表对$\theta_{1}$和$\theta_{2}$的偏导：</p>
<p><img src="/img/16_08_25/019.png" alt=""></p>
<p>接下来，调用高级优化函数<code>fminunc</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="string">'100'</span>);</div><div class="line">initialTheta = zeros(2,1);</div><div class="line">[optTheta, <span class="keyword">function</span>Val, <span class="built_in">exit</span>Flag] = fminunc(@costFunction, initialTheta, options);</div></pre></td></tr></table></figure>
<p><code>fminunc</code>表示Octave里无约束最小化函数，调用这个函数时，需要传入一个存有配置信息的变量<code>options</code>。上面的代码中，我们的设置项中<code>&#39;GradObj&#39;, &#39;on&#39;,</code>代表设置梯度目标参数为打开状态(on)，这也意味着你现在确实要给这个算法提供一个梯度。<code>&#39;MaxIter&#39;, &#39;100&#39;</code>代表设置最大迭代次数为100次。<code>initialTheta</code>代表我们给出的一个$\theta$的猜测初始值。</p>
<p>然后我们调用<code>fminunc</code>这个函数，传入三个参数，其中第一个参数<code>@costFunction</code>这里的<code>@</code>符号代表指向之前我们定义的<code>costFunction</code>函数的指针。后面两个参数分别是我们定义的$theta$初始值和配置信息<code>options</code>。</p>
<p>当我们调用这个<code>fminunc</code>函数时，它会自动的从众多高级优化算法中挑选一个来使用(你也可以把它当做一个可以自动选择合适的学习速率$a$的梯度下降算法)。</p>
<p>最终我们会得到三个返回值，分别是满足最小化代价函数$J(\theta)$的$\theta$值<code>optTheta</code>，<code>costFunction</code>中定义的<code>jVal</code>的值<code>functionVal</code>，以及标记是否已经收敛的状态值<code>exitFlag</code>，如果已收敛，标记为<code>1</code>，否则为<code>0</code>。</p>
<p>下面我们看看在Octave中具体怎么操作：</p>
<p>首先在当前工作目录下创建costFunction.m脚本，写入costFunction函数:</p>
<p><img src="/img/16_08_25/020.png" alt=""></p>
<p>然后在Octave终端下带入<code>fminunc</code>函数：</p>
<p><img src="/img/16_08_25/021.png" alt=""></p>
<h3 id="在逻辑回归中使用优化算法">在逻辑回归中使用优化算法</h3><p>上面的例子是一个简单的二次函数中使用优化算法的例子，下面我们介绍一些在逻辑回归中使用这些算法：</p>
<p><img src="/img/16_08_25/022.png" alt=""></p>
<p>上图就是逻辑回归问题的代价函数的求解代码。可以看到大体流程和之前的例子是一致的，你只需要把相应的计算<code>jVal</code>以及各个<code>gradient</code>的代码，按照逻辑回归的规则填写完整即可。</p>
<blockquote>
<p>使用了这些复杂的优化算法之后，虽然它将使得调试变得更为困难，但是这些算法的速度远远大于普通的梯度下降算法的速度，因此可以用来处理很大的机器学习问题的时候，可以使用这些高级算法，而不是梯度下降算法。</p>
</blockquote>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2016/08/21/斯坦福机器学习课程 第三周 (1)分类和表达式/" itemprop="url">
                斯坦福机器学习课程 第三周 (1)分类和表达式
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2016-08-21T18:00:00+08:00" content="2016-08-21">
            2016-08-21
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2016/08/21/斯坦福机器学习课程 第三周 (1)分类和表达式/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2016/08/21/斯坦福机器学习课程 第三周 (1)分类和表达式/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="分类">分类</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification" target="_blank" rel="external">视频地址</a></p>
<p>在接下来的几节课中，将介绍分类问题。我们将学习一种叫做<strong>逻辑回归(Logistic Regression)</strong>的算法，这也是目前最流行，使用最广泛的一种学习算法。</p>
<p>下面是几个使用分类算法的例子：</p>
<ul>
<li>Email的垃圾邮件分类问题：区分邮件是否为垃圾邮件</li>
<li>网上交易分类问题：是否为欺诈交易</li>
<li>肿瘤分类问题：区分肿瘤是恶性还是良性</li>
</ul>
<p>在所有的这些问题中，我们想要预测的变量是$y$，我们可以认为它可以取两个值：<code>0</code>或者<code>1</code>。</p>
<p>$$<br>y\in {0, 1}<br>$$</p>
<blockquote>
<p>0：“负类”（例如：良性肿瘤）<br>1：“正类”（例如：恶性肿瘤）</p>
</blockquote>
<p>现在我们要开始研究只有两类（0和1）的分类问题。之后我们也会讨论更多类别的问题：$y\in {0,1,2,3,…}$，这就是所谓的<strong>多类分类问题</strong>。但接下来，我们将开始介绍只有两种类别的分类问题，或者叫做<strong>二元分类</strong>问题。</p>
<h3 id="二元分类">二元分类</h3><p>那么我们要怎样开发一个分类算法呢？下面是一个用来给肿瘤分类的训练集的例子(恶性或良性)。</p>
<p><img src="/img/16_08_22/001.png" alt=""></p>
<blockquote>
<p>注意这里的恶性值(Malignant)只取两个值：0也就是良性，1也就是恶性。</p>
</blockquote>
<p>所以拿到这样的数据，我们通常的做法是用线性回归算法尝试使用一条直线来拟合这些数据：</p>
<p>$$<br>h_{\theta}(x)=\theta^{T}x<br>$$</p>
<p>如果你试着用一条直线去拟合这些数据，你也许会得到这样一条直线：</p>
<p><img src="/img/16_08_22/002.png" alt=""></p>
<p>如果你想要尝试分类，可以试着将分类器的阈值设为0.5：</p>
<ul>
<li>当$h_{\theta}(x)\ge0.5$时，预测$y=1$。</li>
<li>当$h_{\theta}(x)\lt0.5$时，预测$y=0$。</li>
</ul>
<p>因此，如图这里就是阈值的所在点：</p>
<p><img src="/img/16_08_22/003.png" alt=""></p>
<p>这个点右侧的点，我们会将它们全部预测为正，因为它们的输出值在纵轴上都是大于0.5的。同理，在这一点的左侧的点，我们会将它们全部预测为负。</p>
<p>在这个例子中，看起来好像线性回归所做的实际上是合理的，现在让我们稍做一些改动，延长一下横轴，假设在很远的右侧有一个训练样本。</p>
<p><img src="/img/16_08_22/004.png" alt=""></p>
<p>此时，得到的拟合数据的直线就会是上图那样，而且阈值所在的位置也会向右侧偏移。此时我们预测阈值点右侧的值都为正，左侧都为负。这看起来线性回归的效果并不好。因为我们可以很明显的看出来，新增的这个点其实并没有给我们带来什么新的信息，这里的意思是说，按照原本的预测，这一个位置出现的点就应该是正。而且我们根据这个新增的点得到的阈值所在点并没有很好的把数据进行划分。</p>
<p>因此<strong>使用线性函数进行分类并不是一个好办法</strong>。在第一个例子中，线性函数能够正常的预测数据只不过是巧合罢了。</p>
<p>另外一个不使用线性函数进行分类的原因是：对于分类问题，我们通常只需要取得$y=0$或$y=1$。如果使用线性回归，那么假设输出函数很有可能远大于1或者远小于0。</p>
<p><img src="/img/16_08_22/005.png" alt=""></p>
<p>因此我们接下来的视频中将要介<strong>逻辑回归算法(Logistic Regression)</strong>，这个算法的性质就是适用于$y$值为离散值的情况，并且它的输出值保证在0到1之间:<br>$$<br>0\le h_{\theta}(x)\le1<br>$$</p>
<blockquote>
<p>顺便提一句，<strong>逻辑回归算法</strong>是一个<strong>分类算法</strong>，即使它的名字里有<strong>回归</strong>，但我们也把它当成分类算法来使用。这里只是因为历史原因，才被这样称呼。</p>
</blockquote>
<h2 id="假设函数表达式">假设函数表达式</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/RJXfB/hypothesis-representation" target="_blank" rel="external">视频地址</a> </p>
<p>之前我们说过，我们希望分类器的输出值的范围能控制在0和1之间：</p>
<p>$$<br>0\le h_{\theta}(x)\le1<br>$$</p>
<p>因此我们渴望找到一个满足这种预测值性质的函数。</p>
<p>当我们使用线性回归时，我们用下面的式子来作为假设函数表达式：</p>
<p>$$<br>h_{\theta}(x) = \theta^{T}x<br>$$</p>
<p>对于逻辑回归来说，我们需要稍微改动一下：</p>
<p>$$<br>h_{\theta}(x) = g(\theta^{T}x)<br>$$</p>
<p>其中$g$函数的表达式为：</p>
<p>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$</p>
<p>这个函数被称为<strong>S型函数(Sigmoid function)</strong>，或者<strong>逻辑函数(Logistic function)</strong>。</p>
<blockquote>
<p><strong>S型函数(Sigmoid function)</strong>和<strong>逻辑函数(Logistic function)</strong>基本上是同义词。因此这两个术语是可以互换的。</p>
</blockquote>
<p><strong>S型函数(Sigmoid function)</strong>的图形是这样的，顶部是随着$z$的增大而逐渐趋近于1，底部随着$z$的减小而逐渐趋近于0：</p>
<p><img src="/img/16_08_22/006.png" alt=""></p>
<p>因此我们可以得到：</p>
<p>$$<br>\begin{align*}<br>h_{\theta}(x) &amp;= g(\theta^{T}x) \\<br>&amp;= \frac{1}{1+e^{-\theta^{T}x}}<br>\end{align*}<br>$$ </p>
<p>接下来我们要做的就是用参数$\theta$来拟合我们的数据。</p>
<h3 id="对假设函数的进一步理解">对假设函数的进一步理解</h3><p><strong>假设函数的解释</strong>：</p>
<p>$h_{\theta}(x)$是对于新注入样本$x$的满足$y=1$的概率估计。</p>
<p>举个肿瘤分类的例子来说：</p>
<p>如果：</p>
<p>$$<br>x=<br>\begin{bmatrix}<br>x_{0} \\<br>x_{1}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>1 \\<br>tumorSizs<br>\end{bmatrix}<br>$$</p>
<p>$$<br>h_{\theta}(x) = 0.7<br>$$</p>
<p>和之前一样，特征向量$x_{0}=1$，特征向量$x_{1}=tumorSize$代表肿瘤大小。</p>
<p>假设有一个病人，我们已知他肿瘤的大小，将他的特征向量$x$带入到$h_{\theta}(x)$中，得到的结果是$0.7$，那么我的解释就是$y=1$的概率是$0.7$，即这个患者的肿瘤是恶性的概率是$70\%$。</p>
<p>用更加正式的式子来表示就是：</p>
<p>$$<br>h_{\theta}(x) = P(y=1|x;\theta)<br>$$</p>
<blockquote>
<p>熟悉概率的同学可能对这个式子不陌生，这个式子代表着<strong>在给定的特征$x$的情况下，$y=1$的概率，其中参数为$\theta$</strong>。<br>在这里给定的$x$就是代表我的病人的肿瘤大小的特征$x$。</p>
</blockquote>
<p>由于我们知道，$y$的取值不是0就是1，因此当我们得到了$y=1$的概率时，$y=0$的概率就很容易得出：</p>
<p>$$<br>P(y=0|x;\theta) + P(y=1|x;\theta) = 1 \\<br>P(y=0|x;\theta) = 1 - P(y=1|x;\theta)<br>$$</p>
<h2 id="决策边界">决策边界</h2><p><a href="https://www.coursera.org/learn/machine-learning/discussions/weeks/3" target="_blank" rel="external">视频地址</a></p>
<p>这一节向大家介绍一个<strong>决策边界(Decision Boundary)</strong>的概念，这个概念能更好的帮助大家理解逻辑回归中假设函数在计算什么。</p>
<p>在上一节中，我们说假设函数可以表示为如下形式：</p>
<p>$$<br>h_{\theta}(x) = g(\theta^{T}x)<br>$$</p>
<p>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$</p>
<p>对应的图像是这样的：</p>
<p><img src="/img/16_08_22/007.png" alt=""></p>
<p>更进一步来理解：</p>
<ul>
<li>如果$h_{\theta}(x)\ge0.5$ 我们认为$y=1$</li>
<li>如果$h_{\theta}(x)\lt0.5$ 我们认为$y=0$</li>
</ul>
<p>根据图片，我们可以得出:</p>
<ul>
<li>当$z\ge0$时，$h_{\theta}(x)\ge0.5$，满足$y=1$；</li>
<li>当$z\lt0$时，$h_{\theta}(x)\lt0.5$，满足$y=0$。</li>
</ul>
<p>即：</p>
<ul>
<li>当$\theta^{T}x\ge0$时，$y=1$；</li>
<li>当$\theta^{T}x\lt0$时，$y=0$；</li>
</ul>
<p>通过上面这些，我们可以更好的理解如何利用逻辑回归的假设函数来进行预测。</p>
<p>接下来我们看一个例子：</p>
<p><img src="/img/16_08_22/008.png" alt=""></p>
<p>假设我们有一个训练集，我们的假设函数为：</p>
<p>$$<br>h_{\theta}(x)=g(\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2})<br>$$</p>
<blockquote>
<p>目前我们还没有讲解如何拟合此模型中的参数，这个知识点将在下一节中讲解。这里我们假设数据已经是拟合好的了。</p>
</blockquote>
<p>假设我们已经拟合好了数据，并且得到了$\theta_{0}=-3$、$\theta_{1}=1$、$\theta_{2}=1$。这意味着：</p>
<p>$$<br>\theta=<br>\begin{bmatrix}<br>-3 \\<br>1 \\<br>1<br>\end{bmatrix}<br>$$</p>
<p>根据上面我们的讨论，我们可以得到：</p>
<ul>
<li>当$-3 + x_{1} + x_{2} \ge 0$时，满足$y=1$；</li>
<li>当$-3 + x_{1} + x_{2} \lt 0$时，满足$y=0$。</li>
</ul>
<p>因此，当$x_{1} + x_{2} = 3$时，我们可以得到一条直线：</p>
<p><img src="/img/16_08_22/009.png" alt=""></p>
<p>在这条直线右上方的点，代表$y=1$的点，这片区域被称为$y=1$区域。于此相对的是，在这条直线左下方的点，代表$y=0$的点，这片区域被称为$y=0$区域。</p>
<p>上面我们绘制出来的$x_{1} + x_{2} = 3$这条直线被称为<strong>决策边界(decision boundary)</strong>。</p>
<blockquote>
<p>值得说明的是，决策边界其实是假设函数$h_{\theta}(x)$的属性，它取决于其参数$\theta$。目前我们还没有说明如何对这些参数$\theta$进行取值，后面我们会学习如何使用训练集来确定这些参数的取值。但是，我们一旦能确定$\theta$的值时，我们就能确定出决策边界了。</p>
</blockquote>
<p>接下来我们来看一个更复杂的例子：</p>
<p><img src="/img/16_08_22/010.png" alt=""></p>
<p>和之前一样，圆圈表示负样本，红色的叉号表示正样本。那么我们怎样使用逻辑回归来拟合这些数据呢？</p>
<p>之前我们讲过多项式回归时，我们谈论到可以通过添加额外的高阶多项式项，这里我们同样可以对逻辑回归使用相同的方法。</p>
<p>具体的说，假设我的假设函数如下：</p>
<p>$$<br>h_{\theta}(x) =<br>g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{2}^{2})<br>$$</p>
<p>之前我们说到，我们将在后面的学习讨论如何得到正确的$\theta$，这里假设我们已经得到了正确的$\theta$，值如下：</p>
<p>$$<br>\begin{align*}<br>\theta_{0} &amp;= -1 \\<br>\theta_{1} &amp;= 0 \\<br>\theta_{2} &amp;= 0 \\<br>\theta_{3} &amp;= 1 \\<br>\theta_{4} &amp;= 1<br>\end{align*}<br>$$</p>
<p>在这个参数的选择下，我们的参数向量将是：</p>
<p>$$<br>\theta=<br>\begin{bmatrix}<br>-1 \\<br>0 \\<br>0 \\<br>1 \\<br>1<br>\end{bmatrix}<br>$$</p>
<p>这也说明，若我们期望$y$满足$y=1$，那么$x_{1}$,$x_{2}$需满足：</p>
<p>$$<br>-1 + x_{1}^{2} + x_{2}^{2} \ge 0<br>$$</p>
<p>对应的决策边界就是这样：</p>
<p><img src="/img/16_08_22/011.png" alt=""></p>
<hr>
<p>其实对于更复杂的假设函数：</p>
<p>$$<br>h_{\theta}(x) =<br>g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^{2}+\theta_{4}x_{1}^{2}x_{2}+\theta_{5}x_{1}^{2}x_{2}^{2}+\theta_{6}x_{1}^{3}x_{2}+…)<br>$$</p>
<p>它的决策边界可能会是一些有趣的形状：</p>
<p><img src="/img/16_08_22/012.png" alt=""></p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/5/">&laquo;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/7/">&raquo;</a>
  </nav>

 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://dannylee1991.github.io/images/avatar.jpg" alt="DannyLee佳楠" itemprop="image"/>
          <p class="site-author-name" itemprop="name">DannyLee佳楠</p>
        </div>
        <p class="site-description motion-element" itemprop="description">一只在迈向机器学习道路上狂奔的程序猿.</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">113</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">16</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">19</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DannyLee1991" target="_blank">GitHub</a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee佳楠</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dannylee1991"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
