<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿." />



  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta property="og:type" content="website">
<meta property="og:title" content="DannyLee">
<meta property="og:url" content="http://dannylee1991.github.io/page/2/index.html">
<meta property="og:site_name" content="DannyLee">
<meta property="og:description" content="一只在迈向机器学习道路上狂奔的程序猿.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DannyLee">
<meta name="twitter:description" content="一只在迈向机器学习道路上狂奔的程序猿.">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>

<!--baidu统计-->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?2f967e5ec4f276411160d27aeace7722";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

  <title> DannyLee </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">DannyLee</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu menu-left">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      

      
      
        <li class="menu-item menu-item-search">
          <a href="#" class="st-search-show-outputs">
            <i class="menu-item-icon icon-next-search"></i> <br />
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'ss9-_Hsd4DyhyGw4m99P','2.0.0');
</script>



    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/03/06/【Tensorflow r1.0 文档翻译】通过tf.contrib.learn来构建输入函数/" itemprop="url">
                【Tensorflow r1.0 文档翻译】通过tf.contrib.learn来构建输入函数
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-03-06T22:24:58+08:00" content="2017-03-06">
            2017-03-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/03/06/【Tensorflow r1.0 文档翻译】通过tf.contrib.learn来构建输入函数/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/06/【Tensorflow r1.0 文档翻译】通过tf.contrib.learn来构建输入函数/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>本教程将介绍如何在tf.contrib.learn中创建输入函数。您将对如何构造一个用于预处理并将数据反馈到你的模型的<code>input_fn</code>操作有一个大致的了解。然后，您将实现一个<code>input_fn</code>，它将训练，评估和预测数据提供给神经网络回归，并用于预测房屋数据的中位数值。</p>
<h2 id="使用input_fn的自定义输入管道">使用input_fn的自定义输入管道</h2><p>当通过使用tf.contrib.learn来训练一个神经网络时，可以将您的特征和目标数据直接传递到你的<code>fit</code>(拟合)、<code>evaluate</code>(评估)或<code>predict</code>(预测)操作中。下面是从<a href="/2017/03/05/【Tensorflow%20r1.0%20文档翻译】【tf.contrib.learn快速入门】/">tf.contrib.learn快速入门教程</a>中获取的示例：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">training_set = tf<span class="selector-class">.contrib</span><span class="selector-class">.learn</span><span class="selector-class">.datasets</span><span class="selector-class">.base</span><span class="selector-class">.load_csv_with_header</span>(</div><div class="line">    filename=IRIS_TRAINING, target_dtype=np<span class="selector-class">.int</span>, features_dtype=np.float32)</div><div class="line">test_set = tf<span class="selector-class">.contrib</span><span class="selector-class">.learn</span><span class="selector-class">.datasets</span><span class="selector-class">.base</span><span class="selector-class">.load_csv_with_header</span>(</div><div class="line">    filename=IRIS_TEST, target_dtype=np<span class="selector-class">.int</span>, features_dtype=np.float32)</div><div class="line">...</div><div class="line"></div><div class="line">classifier.fit(x=training_set<span class="selector-class">.data</span>,</div><div class="line">               y=training_set<span class="selector-class">.target</span>,</div><div class="line">               steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>这种数据量不大的情况下，我们即使不处理数据源，也可好获得良好的效果。但是在需要更多特征工程的情况下，<code>tf.contrib.learn</code>支持使用自定义输入函数（<code>input_fn</code>），它可以将预处理和管道数据的逻辑封装到模型中。</p>
<h3 id="input_fn的剖析">input_fn的剖析</h3><p>以下代码说明了输入函数的基本框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">()</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># Preprocess your data here...</span></div><div class="line"></div><div class="line">    <span class="comment"># ...then return 1) a mapping of feature columns to Tensors with</span></div><div class="line">    <span class="comment"># the corresponding feature data, and 2) a Tensor containing labels</span></div><div class="line">    <span class="keyword">return</span> feature_cols, labels</div></pre></td></tr></table></figure>
<p>输入函数的主体包含用于预处理输入数据的特定逻辑，例如<strong>擦除不良样本</strong>或<strong><a href="https://en.wikipedia.org/wiki/Feature_scaling" target="_blank" rel="external">特征缩放</a></strong>。</p>
<p>输入函数必须返回以下两个值，这两个值包含要输入到模型中的最终特征和标签数据（如上面的代码框架中所示）：</p>
<p><code>feature_cols</code></p>
<pre><code>包含将特征列名称映射到包含相应特征数据的<span class="escape">`T</span>ensor<span class="escape">`（</span>或<span class="escape">`S</span>parseTensor<span class="escape">`）</span>的键/值对的字典。
</code></pre><p><code>labels</code></p>
<pre><code>包含您的标签（目标）值的<span class="escape">`T</span>ensor<span class="escape">`：</span>你的模型的值的目的是用于预测。
</code></pre><h3 id="将特征数据转换为Tensor">将特征数据转换为Tensor</h3><p>如果你的特征/标签数据储存在<a href="http://pandas.pydata.org/" target="_blank" rel="external">pandas</a>数据帧中或<a href="http://www.numpy.org/" target="_blank" rel="external">numpy</a>数组中，那么你需要将其转换为<code>Tensor</code>，然后从您的<code>input_fn</code>中返回它。</p>
<p>对于连续数据，可以使用<code>tf.constant</code>创建和填充<code>Tensor</code>：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feature_column_data = [<span class="number">1</span>, <span class="number">2.4</span>, <span class="number">0</span>, <span class="number">9.9</span>, <span class="number">3</span>, <span class="number">120</span>]</div><div class="line">feature_tensor = tf.constant(feature_column_data)</div></pre></td></tr></table></figure>
<p>对于<a href="https://en.wikipedia.org/wiki/Sparse_matrix" target="_blank" rel="external">稀疏分类数据</a>（大多数值为0的数据），您应该替换为填充一个<code>SparseTensor</code>，它使用三个参数来实例化：</p>
<p><code>dense_shape</code></p>
<pre><code>tensor的形状。获取一个列表，指示每个维度中元素的数量。例如：`dense_shape=[<span class="number">3</span>,<span class="number">6</span>]`指定了一个二维的<span class="number">3</span>x6的tensor，`dense_shape=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]`指定了一个三维的<span class="number">2</span>x3x4的tensor，`dense_shape=[<span class="number">9</span>]`指定了一个拥有<span class="number">9</span>个元素的一维tensor。
</code></pre><p><code>indices</code></p>
<pre><code>您的tensor中包含非零元素的索引。值为一个列表，其中每一项本身是包含非零元素的索引的列表。（元素是零索引的 - 即，`[<span class="number">0</span>,<span class="number">0</span>]`是二维张量中第一行的第一列中的元素的索引值）。例如：`indices=[[<span class="number">1</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>]]`指定了索引为`[<span class="number">1</span>,<span class="number">3</span>]`和`[<span class="number">2</span>,<span class="number">4</span>]`的元素具有非零值。
</code></pre><p><code>values</code></p>
<pre><code>值为一维tensor。<span class="escape">`v</span>alues<span class="escape">`的</span>项<span class="escape">`i</span><span class="escape">`对</span>应于<span class="escape">`i</span>ndices<span class="escape">`中</span>的项<span class="escape">`i</span><span class="escape">`，</span>并且指定了它的值。例如，给定了<span class="escape">`i</span>ndices=[[<span class="number">1</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>]]<span class="escape">`，</span>那么参数<span class="escape">`v</span>alues=[<span class="number">18</span>, <span class="number">3.6</span>]<span class="escape">`就</span>指定了tensor的元素<span class="escape">`[</span><span class="number">1</span>,<span class="number">3</span>]<span class="escape">`的</span>值为<span class="number">18</span>，元素<span class="escape">`[</span><span class="number">2</span>,<span class="number">4</span>]<span class="escape">`的</span>值为<span class="number">3.6</span>。
</code></pre><p>以下代码定义了一个具有3行和5列的二维<code>SparseTensor</code>。具有索引<code>[0,1]</code>的元素的值为6，并且索引为<code>[2,4]</code>的元素值为0.5（所有其他值为0）：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sparse_tensor = tf.SparseTensor(indices=[[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">4</span>]],</div><div class="line">                                values=[<span class="number">6</span>, <span class="number">0.5</span>],</div><div class="line">                                dense_shape=[<span class="number">3</span>, <span class="number">5</span>])</div></pre></td></tr></table></figure>
<p>这对应了下面的稠密tensor(dense tensor)：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[[<span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"> [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0.5</span>]]</div></pre></td></tr></table></figure>
<p>更多关于<code>SparseTensor</code>的内容，请见<a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensor" target="_blank" rel="external"><code>tf.SparseTensor</code></a></p>
<h3 id="将input_fn数据传递给您的模型">将input_fn数据传递给您的模型</h3><p>要将数据馈送到您的模型进行训练，您只需将创建的输入函数作为<code>input_fn</code>参数的值传递到<code>fit</code>运算即可，例如：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(<span class="attr">input_fn=my_input_fn,</span> <span class="attr">steps=2000)</span></div></pre></td></tr></table></figure>
<p>请注意，<code>input_fn</code>负责将特征和标签数据提供给模型，并在<code>fit</code>(拟合)中替换<code>x</code>和<code>y</code>参数。如果为<code>fit</code>提供了一个不为空的<code>input_fn</code>值与不为<code>None</code>的<code>x</code>或<code>y</code>结合，它将抛出一个<code>ValueError</code>。</p>
<p>还要注意一点，<code>input_fn</code>参数必须接收一个函数对象（例如<code>input_fn = my_input_fn</code>），而不是函数调用的返回值（<code>input_fn = my_input_fn()</code>）。这意味着，如果您尝试在<code>fit</code>的调用中按照下面的方式，将参数传递给输入函数，则会导致TypeError：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.fit(<span class="attr">input_fn=my_input_fn(training_set),</span> <span class="attr">steps=2000)</span></div></pre></td></tr></table></figure>
<p>但是，如果你想要参数化你的输入函数，有一些其他的方法可以做到。您可以使用不带参数的包装函数作为<code>input_fn</code>，并使用它来调用具有所需参数的输入函数。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_function_training_set</span><span class="params">()</span>:</span></div><div class="line">  <span class="keyword">return</span> my_input_function(training_set)</div><div class="line"></div><div class="line">classifier.fit(input_fn=my_input_fn_training_set, steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>或者，你可以使用Python的<a href="https://docs.python.org/2/library/functools.html#functools.partial" target="_blank" rel="external"><code>functools.partial</code></a>方法来构造一个新的所有参数值固定的方法对象：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">classifier.fit(input_fn=functools.partial(my_input_<span class="keyword">function</span>,</div><div class="line">                                          data_<span class="built_in">set</span>=training_<span class="built_in">set</span>), steps=2000)</div></pre></td></tr></table></figure>
<p>第三种方式是将<code>input_fn</code>调用包装在<a href="https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions" target="_blank" rel="external"><code>lambda</code></a>中，并将其传递给<code>input_fn</code>参数：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier<span class="selector-class">.fit</span>(input_fn=<span class="attribute">lambda</span>: my_input_fn(training_set), steps=<span class="number">2000</span>)</div></pre></td></tr></table></figure>
<p>构建您的输入管道的一个很大的优势如上所示 – 可以接受数据集的参数 – 是你只需修改数据集的参数，就可以传递相同的<code>input_fn</code>到<code>evaluate</code>和<code>predict</code>操作上。例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">classifier.evaluate(input_fn=lambda: my_input_fn(<span class="built_in">test</span>_<span class="built_in">set</span>), steps=2000)</div></pre></td></tr></table></figure>
<p>这种方法增强了代码的可维护性：不需要针对每种类型的操作捕获单独变量（例如，<code>x_train</code>，<code>x_test</code>，<code>y_train</code>，<code>y_test</code>）中的<code>x</code>和<code>y</code>值。</p>
<h3 id="一个用于波士顿房屋数据的神经网络">一个用于波士顿房屋数据的神经网络</h3><p>在本教程的剩余部分，您将编写一个输入函数，用于预处理从<a href="https://archive.ics.uci.edu/ml/datasets/Housing" target="_blank" rel="external">UCI住宅数据集</a>中提取的一组波士顿房屋数据，并使用它来将数据馈送到神经网络回归器，以预测房屋中值。</p>
<p>您将用于训练神经网络的<a href="https://www.tensorflow.org/get_started/input_fn#setup" target="_blank" rel="external">波士顿CSV数据集</a>包含以下波士顿郊区的<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names" target="_blank" rel="external">特征数据</a>：</p>
<table>
<thead>
<tr>
<th style="text-align:left">特征</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">CRIM</td>
<td style="text-align:left">人均犯罪率</td>
</tr>
<tr>
<td style="text-align:left">ZN</td>
<td style="text-align:left">超过25,000+平方呎地段的住宅用地的部分</td>
</tr>
<tr>
<td style="text-align:left">INDUS</td>
<td style="text-align:left">非零售业的土地部分</td>
</tr>
<tr>
<td style="text-align:left">NOX</td>
<td style="text-align:left">一氧化氮浓度 以百万分之一为单位</td>
</tr>
<tr>
<td style="text-align:left">RM</td>
<td style="text-align:left">每个住宅平均房间数</td>
</tr>
<tr>
<td style="text-align:left">AGE</td>
<td style="text-align:left">在1940年之前建造的自用住宅的部分</td>
</tr>
<tr>
<td style="text-align:left">DIS</td>
<td style="text-align:left">到波士顿地区就业中心的距离</td>
</tr>
<tr>
<td style="text-align:left">TAX</td>
<td style="text-align:left">每$10,000的房产税税率</td>
</tr>
<tr>
<td style="text-align:left">PTRATIO</td>
<td style="text-align:left">学生 - 教师比例</td>
</tr>
</tbody>
</table>
<p>你的模型预测的标签是MEDV，自用住宅的价格中值，以千美元计。</p>
<h2 id="构建">构建</h2><p>下载以下数据集：<a href="http://download.tensorflow.org/data/boston_train.csv" target="_blank" rel="external">boston_train.csv</a>, <a href="http://download.tensorflow.org/data/boston_test.csv" target="_blank" rel="external">boston_test.csv</a>, 和 <a href="http://download.tensorflow.org/data/boston_predict.csv" target="_blank" rel="external">boston_predict.csv</a>。</p>
<p>以下部分提供了如何创建输入函数的手把手的步骤，将这些数据集送入神经网络回归，训练和评估模型，并进行房屋价值预测。完整的最终代码在<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/input_fn/boston.py" target="_blank" rel="external">这里</a>。</p>
<h3 id="输入房屋数据">输入房屋数据</h3><p>要开始，请设置导入所需的库（包括<code>pandas</code>和<code>tensorflow</code>），并将<a href="https://www.tensorflow.org/get_started/monitors#enabling_logging_with_tensorflow" target="_blank" rel="external">日志级别设置</a>为<code>INFO</code>以获取更详细的日志输出：</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="title">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="title">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="title">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="title">tf</span>.logging.set_verbosity(tf.logging.<span class="type">INFO</span>)</div></pre></td></tr></table></figure>
<p>在<code>COLUMNS</code>中定义数据集的列名称。要区分特征和标签，还需要定义<code>FEATURES</code>和<code>LABEL</code>。然后将三个CSV（<code>tf.train</code>，<code>tf.test</code>和<code>predict</code>）读入pandas <code>DataFrame</code>s：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="attr">COLUMNS</span> = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>, <span class="string">"age"</span>,</div><div class="line">           <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>, <span class="string">"medv"</span>]</div><div class="line"><span class="attr">FEATURES</span> = [<span class="string">"crim"</span>, <span class="string">"zn"</span>, <span class="string">"indus"</span>, <span class="string">"nox"</span>, <span class="string">"rm"</span>,</div><div class="line">            <span class="string">"age"</span>, <span class="string">"dis"</span>, <span class="string">"tax"</span>, <span class="string">"ptratio"</span>]</div><div class="line"><span class="attr">LABEL</span> = <span class="string">"medv"</span></div><div class="line"></div><div class="line"><span class="attr">training_set</span> = pd.read_csv(<span class="string">"boston_train.csv"</span>, <span class="attr">skipinitialspace=True,</span></div><div class="line">                           <span class="attr">skiprows=1,</span> <span class="attr">names=COLUMNS)</span></div><div class="line"><span class="attr">test_set</span> = pd.read_csv(<span class="string">"boston_test.csv"</span>, <span class="attr">skipinitialspace=True,</span></div><div class="line">                       <span class="attr">skiprows=1,</span> <span class="attr">names=COLUMNS)</span></div><div class="line"><span class="attr">prediction_set</span> = pd.read_csv(<span class="string">"boston_predict.csv"</span>, <span class="attr">skipinitialspace=True,</span></div><div class="line">                             <span class="attr">skiprows=1,</span> <span class="attr">names=COLUMNS)</span></div></pre></td></tr></table></figure>
<h3 id="定义特征列并创建回归">定义特征列并创建回归</h3><p>接下来，为输入数据创建<code>FeatureColumn</code>list，正式指定要用于训练的特征集。由于房屋数据集中的所有特征都包含连续的值，因此可以使用<code>tf.contrib.layers.real_valued_column()</code>函数创建其<code>FeatureColumn</code>s：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feature_cols = [tf<span class="selector-class">.contrib</span><span class="selector-class">.layers</span><span class="selector-class">.real_valued_column</span>(k)</div><div class="line">                  <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES]</div></pre></td></tr></table></figure>
<p>注意：有关特征列的更深入的内容，请参阅此<a href="https://www.tensorflow.org/tutorials/linear#feature_columns_and_transformations" target="_blank" rel="external">简介</a>，以及说明如何为分类数据定义<code>FeatureColumns</code>的示例，请参阅线<a href="https://www.tensorflow.org/tutorials/wide" target="_blank" rel="external">性模型教程</a>。</p>
<p>现在，为神经网络回归模型实例化一个<code>DNNRegressor</code>。这里你需要提供两个参数：<code>hidden_units</code>，指定每个隐藏层中的节点数量的超参数(hyperparameter)（这里，有两个隐藏层，每个隐藏层都具有10个节点），以及<code>feature_columns</code>，包含您刚定义的<code>FeatureColumns</code>list：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">regressor = tf<span class="selector-class">.contrib</span><span class="selector-class">.learn</span><span class="selector-class">.DNNRegressor</span>(feature_columns=feature_cols,</div><div class="line">                                          hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</div><div class="line">                                          model_dir=<span class="string">"/tmp/boston_model"</span>)</div></pre></td></tr></table></figure>
<h3 id="构建input_fn">构建input_fn</h3><p>要将输入数据传递到<code>regressor</code>，请创建一个输入函数，它将接受一个pandas <code>Dataframe</code>并返回特征列和标签值作为<code>Tensor</code>s：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(data_set)</span>:</span></div><div class="line">  feature_cols = &#123;k: tf.constant(data_set[k].values)</div><div class="line">                  <span class="keyword">for</span> k <span class="keyword">in</span> FEATURES&#125;</div><div class="line">  labels = tf.constant(data_set[LABEL].values)</div><div class="line">  <span class="keyword">return</span> feature_cols, labels</div></pre></td></tr></table></figure>
<p>请注意，输入数据被传递到<code>data_set</code>参数中的<code>input_fn</code>中，这意味着该函数可以处理您导入的任何<code>DataFrames</code>：<code>training_set</code>，<code>test_set</code>和<code>prediction_set</code>。</p>
<h3 id="训练回归">训练回归</h3><p>要训​​练神经网络回归，运行指定了包含有<code>training_set</code>的<code>input_fn</code>的<code>fit</code>函数，如下：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">regressor<span class="selector-class">.fit</span>(input_fn=<span class="attribute">lambda</span>: input_fn(training_set), steps=<span class="number">5000</span>)</div></pre></td></tr></table></figure>
<p>您应该能看到类似于以下内容的日志输出，它会报告每100步的训练loss值：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">1</span>: loss = <span class="number">483.179</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">101</span>: loss = <span class="number">81.2072</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">201</span>: loss = <span class="number">72.4354</span></div><div class="line">...</div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">1801</span>: loss = <span class="number">33.4454</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">1901</span>: loss = <span class="number">32.3397</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">2001</span>: loss = <span class="number">32.0053</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">4801</span>: loss = <span class="number">27.2791</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Step <span class="number">4901</span>: loss = <span class="number">27.2251</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Saving checkpoints <span class="keyword">for</span> <span class="number">5000</span> into <span class="regexp">/tmp/</span>boston_model/model.ckpt.</div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Loss <span class="keyword">for</span> <span class="keyword">final</span> <span class="string">step:</span> <span class="number">27.1674</span>.</div></pre></td></tr></table></figure>
<h3 id="评估模型">评估模型</h3><p>接下来，看看训练模型如何针对测试数据集执行。运行<code>evaluate</code>，这次将<code>test_set</code>传递给<code>input_fn</code>：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">ev</span> = regressor.evaluate(<span class="attr">input_fn=lambda:</span> input_fn(test_set), <span class="attr">steps=1)</span></div></pre></td></tr></table></figure>
<p>从<code>ev</code>的结果中检索损失并将其打印到输出：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">loss_score = ev[<span class="string">"loss"</span>]</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"Loss: &#123;0:f&#125;"</span>.format(loss_score)</span></span>)</div></pre></td></tr></table></figure>
<p>您应该会看到类似以下的结果：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Eval steps [<span class="number">0</span>,<span class="number">1</span>) <span class="keyword">for</span> training step <span class="number">5000.</span></div><div class="line"><span class="string">INFO:</span><span class="string">tensorflow:</span>Saving evaluation summary <span class="keyword">for</span> <span class="number">5000</span> <span class="string">step:</span> loss = <span class="number">11.9221</span></div><div class="line"><span class="string">Loss:</span> <span class="number">11.922098</span></div></pre></td></tr></table></figure>
<h3 id="进行预测">进行预测</h3><p>最后，您可以使用模型预测<code>prediction_set</code>中的房屋中值，其中包含特征数据，但没有六个样本的标签：</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">y = regressor.<span class="keyword">predict</span>(input_fn=lambda: input_fn(prediction_set))</div><div class="line"># .<span class="keyword">predict</span>() returns <span class="keyword">an</span> iterator; convert to a <span class="keyword">list</span> and <span class="keyword">print</span> predictions</div><div class="line">predictions = <span class="keyword">list</span>(itertools.islice(y, 6))</div><div class="line"><span class="keyword">print</span> (<span class="string">"Predictions: &#123;&#125;"</span>.<span class="keyword">format</span>(str(predictions)))</div></pre></td></tr></table></figure>
<p>您的结果应包含以 $1000 计的六次房价预测，例如：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Predictions: [ <span class="number">33.30348587</span>  <span class="number">17.04452896</span>  <span class="number">22.56370163</span>  <span class="number">34.74345398</span>  <span class="number">14.55953979</span></div><div class="line">  <span class="number">19.58005714</span>]</div></pre></td></tr></table></figure>
<h2 id="其他资源">其他资源</h2><p>本教程专注于为神经网络回归创建一个<code>input_fn</code>。要了解更多有关对其他类型模型使用<code>input_fn</code>的信息，请查看以下资源：</p>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/linear" target="_blank" rel="external">TensorFlow的大尺寸线性模型</a>：这种对TensorFlow中的线性模型的介绍提供了用于变换输入数据的特征列和技术的高级概述。</li>
<li><a href="https://www.tensorflow.org/tutorials/wide" target="_blank" rel="external">TensorFlow线性模型教程</a>：本教程包括为线性分类模型创建<code>FeatureColumn</code>s和<code>input_fn</code>，该模型根据人口普查数据预测收入范围。</li>
<li><a href="https://www.tensorflow.org/tutorials/wide_and_deep" target="_blank" rel="external">TensorFlow宽＆深学习教程</a>：基于<a href="https://www.tensorflow.org/tutorials/wide" target="_blank" rel="external">TensorFlow线性模型教程</a>，本教程涵盖了使用<code>DNNLinearCombinedClassifier</code>组合线性模型和神经网络的“宽和深”模型的<code>FeatureColumn</code>和<code>input_fn</code>创建。</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/03/05/关于机器学习的一些思考/" itemprop="url">
                关于机器学习的一些思考
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-03-05T16:40:58+08:00" content="2017-03-05">
            2017-03-05
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/03/05/关于机器学习的一些思考/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/05/关于机器学习的一些思考/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>我大概是在两年前开始正式关注机器学习领域的，当时一心想做一个基于机器学习的五子棋程序，希望能达到机器自动理解五子棋游戏规则的效果，但没能成功，于是我开始找一些机器学习领域的课程和书籍开始啃。</p>
<p>当时机器学习在国内还是一个很少听到的名词，但当我学习这方面的东西的时候，我深深的感受到这个领域的东西和我之前学到的技术不是一个维度的东西，并且它将深远的影响未来的发展。直到后来AlphaGo的出现，以及Google开源了TensorFlow，这一系列的大事件的发生，悄然在业内刮起了一股人工智能风。</p>
<p>TensorFlow在开源之初，国内的<strong>极客学院</strong>发起了文档的翻译工作，还记的在其翻译文档的首页这样写到：</p>
<blockquote>
<p>你正在阅读的项目可能会比 Android 系统更加深远地影响着世界！</p>
</blockquote>
<p>不管这句话说得是否过于夸大，但TensorFlow在github上开源一个月之内就收到了10000+的star，这是github上机器学习领域也是python领域star增长最快的项目了。截止目前，TensorFlow的star为49227，已经超过了linux的42490。可见，机器学习的发展速度之迅猛，是不容小视的。</p>
<p>本文是我对机器学习领域的一些见解和思考，主要涉及<strong>机器学习的定义</strong>，<strong>机器学习的学习方式</strong>以及<strong>相关概念的理解</strong>。你会看到很多教科书上看不到的解读，虽然不是很严谨，但有助于你对一些概念建立起直觉上的理解，从而帮助你更好的了解这一领域的知识。</p>
<h2 id="思考1：机器学习是什么">思考1：机器学习是什么</h2><h3 id="机器学习定义">机器学习定义</h3><blockquote>
<p>机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p>
</blockquote>
<p>这是百度百科给出的定义，更加通俗的解释一下，机器学习是人工智能领域下的子领域，通过对大量现有数据的分析运算来对未知数据进行预测的一种学科。（虽然说法不严谨，但这种解释有助于理解）</p>
<p>将机器学习的通用模型类比于养一个小孩：</p>
<ul>
<li>训练过程就像是养一个小孩子</li>
<li>如果小孩子小的时候接触到了正确的教育（这里正确的教育就是<strong>训练数据集</strong>）</li>
<li>如果小孩子本身的悟性很高（悟性很高类比于有很好的<strong>学习算法</strong>）</li>
<li>那么这个孩子经过一段时间的成长学习后（类比于机器学习的<strong>训练</strong>阶段）</li>
<li>会成为一个有用之才（得到和很好的<strong>假设函数</strong>，即训练模型）</li>
<li>当他遇到新的人和事的时候（接受<strong>测试数据</strong>）</li>
<li>就能够处理的很好（<strong>预测结果</strong>）</li>
</ul>
<p>这就是机器学习的通用模型，虽然不是严谨的学术定义，但相信这能使你建立一种直觉上的认识。</p>
<h3 id="这是一项新技术吗？">这是一项新技术吗？</h3><p>机器学习目前处于学术界迈向工业界的一个过程，其核心算法几十年前就有了，理念也绝非新鲜事物，达到工业级别是一个时间问题，而现在<strong>我们就处在这一学术界迈向工业界的关键阶段</strong>。</p>
<p>其实，机器学习的核心概念早在第一台计算机制造之前就已经产生了。这里也不做过多介绍，大家可以自行去搜索。</p>
<h2 id="思考2：如何学习机器学习">思考2：如何学习机器学习</h2><h3 id="学习框架_or_学习算法?">学习框架 or 学习算法?</h3><p>完全没有学过这一领域的东西，是否应该直接上手TensorFlow之类的框架呢？</p>
<p>框架只是工具，不管是TensorFlow还是Caffe还是Torch，都是对算法的封装，很多之前做其他方面开发的程序员，在接触一个框架或者工具时，都倾向于追求能达到“直接去调用一下就得到结果”这样的效果，但机器学习领域的框架并不是如此，它要求你对机器学习领域的算法有一定了解，要明白自己在做什么。</p>
<p>所以先对这个领域的算法知识有个掌握之后，再去学习和使用框架来实践操作可能是效率更高的一种学习模式。相反，直接上手来学习框架，期望直接调用一些API就得出结果的想法会让你有种寸步难行的感觉。</p>
<p>这里安利一下<a href="https://www.coursera.org/" target="_blank" rel="external">coursera</a>上的吴恩达老师的<a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external">机器学习</a>课程（可能会需要翻墙才能访问，自行购买vpn）。这门课深入浅出的讲解了机器学习领域的流行的算法的实现原理。</p>
<blockquote>
<p>介绍一下吴恩达老师，在最强大脑第四季开播之前，可能知道他的人很少。吴恩达是华裔美国人，是斯坦福大学计算机科学系和电子工程系副教授，人工智能实验室主任，是Coursera的联合创始人。之前是google负责google大脑项目，后来在14年5月16日加入百度，担任百度首席科学家，负责百度研究院的领导工作，尤其是Baidu Brain计划。这是中国互联网公司迄今为止引进的最重量级人物。</p>
</blockquote>
<h3 id="深入底层_or_关注上层？">深入底层 or 关注上层？</h3><p>在技术领域一直流传着这样一种理念：底层的就是牛X的。上层技术变化无常，但底层技术万变不离其宗。再加上底层技术的学习成本远高于上层技术，而且吃透了底层技术能够对上层技术有更深刻的理解，导致程序员对底层技术有一种神圣的向往情节。</p>
<p>技术领域，专注细节不是坏事，但对于机器学习这个领域呢？恐怕我们需要重新考量一下这种思维模式了。</p>
<p>其实在机器学习领域，也是提倡对底层实现算法有一定的了解的，但并不代表我们要去亲自实现一些算法，比如神经网络中的反向传播算法的实现，很多框架都已经写的很完善了，而且都是数值计算领域的专家来实现的高质量高性能的代码，我们没必要花时间和精力来重新造轮子，也没必要去研究他们是如何造的这个轮子。我们更应该把宝贵的时间留在理解这些算法的原理上，以及学会使用这些算法来应用到实际中的问题，这才是把好钢用到了刀刃上。</p>
<p>实际上，正确的学习方式是，首先学习并理解算法原理，在直觉层面建立起对算法的认识，然后快速动手实践，将学到的算法应用到小项目里，不用太过在意写的东西是不是很挫，重要的是开始写！然后再不断的完善你的项目。带着一些问题去继续学习，你会有更多的收获。</p>
<h3 id="不要想太多有的没的">不要想太多有的没的</h3><p>可能很多人跟我一样，刚开始了解这一领域的东西的时候，很是兴奋，感觉造出一个通用人工智能指日可待了。再加上近些年随着人工智能概念的兴起，出来了一批相关影视作品和科幻小说，更让我们对人工智能产生了一系列不切实际的想法。</p>
<p>对于这一点，我想说明的是，不要过分执迷于AI能创建出智能大脑的想法，除非你是业内出类拔萃的专家。科幻小说与电影只是一种艺术表现手法，并不代表着未来。通常，人们看不懂看不透的地方就容易形成骗局，通用智能虽然是人工智能一直以来在追求的一个目标，但目前主流学术发展方向并没有朝着这方面走。机器学习、深度学习的确在飞速的进步，未来也会波及到很多行业，但这个学科归根结底还是一门基于数据的学科，并没有达到拥有情感、独立思考等这方面的能力，实际上，绝大多数业内专家对这方面的想法是持有厌恶态度的，倒是相关的社科类软文在这个时代被吹上了天。</p>
<p>因此，与其花时间想这些有的没的，还不如花时间脚踏实地的去学习一下算法、写写代码，除非你想成为一个科幻小说作家。</p>
<h2 id="思考3：机器学习中一些概念的解读">思考3：机器学习中一些概念的解读</h2><p>机器学习领域中有很多很有趣、很耐人寻味的原理值得细细品味，下面是我对其中部分概念的直觉上的理解，可能你在教科书或者教学视频上看不到这种解读，但这些概念所折射出来的现象也许就是我所描述的那样。当然，如果你没有看过这个领域，甚至没有写过代码也没关系，我保证能让你能读得懂。</p>
<h3 id="神经网络">神经网络</h3><p><strong>神经网络</strong>是机器学习领域出镜率很高的一个词汇，很多人对它的理解一直是停留在“<strong>很牛X，很复杂</strong>”的状态。下面用一个例子来解释神经网络的使用原理：</p>
<p>假设你有一个下周六是否要去电影院看电影的决定。这个决定的结果无非只有两种：<strong>去</strong>或者<strong>不去</strong>。</p>
<p>影响你去或不去的因素有很多：</p>
<ul>
<li>是否有人陪</li>
<li>是否有想看的电影</li>
<li>是否有时间</li>
<li>天气是否足够好</li>
<li>…</li>
</ul>
<p>我相信你可以列出足够多的理由来拒绝周末去看电影，但为了方便描述起见，我们先用三个条件：<strong>是否有人陪</strong>、<strong>是否有想看的电影</strong>、<strong>是否有时间</strong>。</p>
<p>好，现在我们来用三个圈来表示这三个条件。</p>
<p><img src="/img/17_03_05/002.png" alt=""></p>
<p>圆圈中间的数值代表对做出去看电影的决策的影响程度，可以看到这里<strong>是否有时间</strong>的影响程度是最大的（这里的每个小圈，其实就是神经网络中的<strong>神经元</strong>，上面的数值就是<strong>权值</strong>）。</p>
<p>我们接下来把我们的神经网络补充完整：</p>
<p><img src="/img/17_03_05/003.png" alt=""></p>
<p>我们又加了两个圈和一些箭头。好，对应图中，如果我们现在<strong>有想看的电影，可是没有人陪，但有时间</strong>，那么我们的计算方式就是:</p>
<p><img src="/img/17_03_05/004.png" alt=""></p>
<p>$$<br>(-1)×0.3 + 1×0.6 + 1×1.0 = 1.3<br>$$</p>
<p>输出结果是一个大于0的值：1.3，代表我们会做出去看电影的决定。</p>
<p>类似的，假如我们<strong>有人陪，有想看的电影，但是没时间</strong>：</p>
<p>$$<br>1×0.3 + 1×0.6 + (-1)×1.0 = -0.1<br>$$</p>
<p>是一个负数，代表我们不会去看电影。</p>
<p>这就是一个训练好的神经网络的使用方式，这幅图就是一个典型的三层神经网络，从左到右依次是<strong>输出层</strong>、<strong>隐藏层</strong>、<strong>输出层</strong>，其中0.3，0.6，1.0是通过<strong>训练</strong>得出的<strong>权值</strong>。</p>
<p><img src="/img/17_03_05/005.png" alt=""></p>
<p>我相信你对神经网络还是有很多疑问，比如0.3，0.6和1.0是怎么得来的（实际上是通过<strong>反向传播算法</strong>得来的），但神经网络运作的大体模式就是这样，希望你能对它产生一种宏观层面的认识。</p>
<h3 id="偏拟合_和_过拟合">偏拟合 和 过拟合</h3><p><strong>偏拟合</strong>和<strong>过拟合</strong>具体是什么意思呢？不要被陌生的名词吓到。</p>
<p>首先要说明的是，这两个词都不是褒义词，都是我们不想看到的一种状态。</p>
<p>其实所谓<strong>偏拟合</strong>就是相当于某一领域经验不足的人，由于经历的事情太少，容易做出一些错误的判断，这种现象就是<strong>偏拟合</strong>。</p>
<p>所谓<strong>过拟合</strong>，恰恰相反，是指某一领域经验非常丰富的人，由于经历的事情太多，反而容易对新的事物产生偏见（因为既往的经验会告诉他这是不对的），从而产生错误的决定。</p>
<p>教科书上不会这么解释<strong>偏拟合</strong>和<strong>过拟合</strong>的概念，但事实上这个概念描述的就是这样的现象。是不是我们身边随处可见这两种现象呢？</p>
<h3 id="查准率_和_召回率">查准率 和 召回率</h3><p>假设我们现在写了个用于预测病人是否患有肺癌的程序，我们出入了100个病人的体征数据，然后来告知这一批病人有谁不幸得了肺癌。</p>
<p>假设我们的预测准确率为98%，这个结果乍一看是不是很高呢？但我告诉你另一个事实，那就是我们只有两个病人是真正患有肺癌的，然而我们的算法正确的识别了两者中的一位，并且还错误的认为在98名没有患有肺癌的患者里有一位癌症患者。那这还是一个好结果吗？</p>
<p>很明显，100个人里只有两个人患有癌症，其中一个还预测错误，这是一个很差的结果，但我们的准确率为98%，因为我们预测对了100个人中的98个人，所以从准确率上来看并不差，但实际结果却很差。</p>
<p>这就是典型的<strong>偏斜类</strong>问题。</p>
<p>也许有人会说，这表明了数据会说谎，但实际情况是，数据并没有说谎，只不过我们看待数据的方式不够科学。</p>
<p>科学的方式就是引入<strong>查准率</strong>和<strong>召回率</strong>。</p>
<p>在这里:</p>
<p>$$<br>查准率 = \frac{在预测患有肺癌的病人中，预测正确的数量} {预测患有肺癌的病人数量}<br>$$</p>
<p>$$<br>召回率 = \frac{在预测患有肺癌的病人中，预测正确的数量} {实际患有肺癌病人的数量}<br>$$</p>
<p>可见，<strong>查准率</strong>和<strong>召回率</strong>都是越高越好的。</p>
<p>那么我们的例子中<strong>查准率</strong>和<strong>召回率</strong>的真实值分别为：</p>
<p>$$<br>查准率 = \frac{在预测患有肺癌的病人中，预测正确的数量} {预测患有肺癌的病人数量} = \frac{1} {2} = 0.5<br>$$</p>
<p>$$<br>召回率 = \frac{在预测患有肺癌的病人中，预测正确的数量} {实际患有肺癌病人的数量} = \frac{1} {2} = 0.5<br>$$</p>
<p>我们的查准率和召回率都很低。所以换个角度来看待数据，会有新的发现。</p>
<p>查准率和召回率是两个维度的数据，有的时候我们为了追求高查准率，会得到一个低召回率的结果；有的时候我们为了追求高召回率，却会得到一个低查准率的结果。这种顾此失彼的状态，也不是我们想要。</p>
<p>所以我们引入了一个叫做<strong>Fscore</strong>的方法来对两种进行一个整体的衡量，其表达式为：</p>
<p>$$<br>Fscore = \frac{查准率×召回率} {查准率+召回率}<br>$$</p>
<p>这里我们的$Fscore=0.25$。</p>
<p>我们生活中也处处充满着<strong>准确率高但查准率和召回率低的例子</strong>：</p>
<p>我们往往会有这种体验，一些成功的名人说的话似乎都很有道理，但其实他们说的有道理的话似乎都比较“大”；而有些人，专注于某一特定领域的专家，在发表一些观点的时候，用词都非常谨慎，因为他们在描述一件具体的事物。</p>
<p>所以，我们会发现，<strong>越“大”的话，越容易获得较高的准确率；越“专”的话，越不容易获得较高的准确率</strong>。</p>
<p>所以带着这种角度，我们去审视一下：</p>
<ul>
<li>失败是成功之母。</li>
<li>天才是99%的汗水加1%的灵感。</li>
<li>…</li>
</ul>
<p>类似这类的<strong>鸡汤名言</strong>，我们会有新的思考。我们会发现，真正的牛人说的话，不仅仅准确率高，而且查准率和召回率也很高。</p>
<h3 id="无监督学习">无监督学习</h3><p><strong>无监督学习</strong>是相对于<strong>监督学习</strong>而言的。那么什么是<strong>监督学习</strong>呢？</p>
<p><strong>监督学习</strong>通俗的将就是我们告诉机器一堆格式为：</p>
<p><strong>bulabulabula的东西，是xxx</strong></p>
<p>来预测未知类别的数据：</p>
<p><strong>bulabulabula的东西，是？</strong></p>
<p>例如通过体征来预测性别：</p>
<p>已有100个数据：</p>
<ul>
<li>身高175cm，短发，70kg的人是男性</li>
<li>身高165cm，长发，48kg的人是女性</li>
<li>…</li>
</ul>
<p>那么：</p>
<ul>
<li>身高170cm，长发，55kg的人是？</li>
</ul>
<p>这种预测类别已知的机器学习，就是<strong>监督学习</strong>。典型的<strong>监督学习</strong>的案例有<strong>语音识别</strong>、<strong>图像识别</strong>、<strong>人脸识别</strong>等等。</p>
<p>所谓<strong>无监督学习</strong>就是我们并不能知道数据所属的类别，通过算法使数据自动地按照相似的类别聚合起来，即所谓的<strong>聚类算法</strong>。</p>
<p>我们俗话所说的<strong>物以类聚，人以群分</strong>就是无监督学习的体现。</p>
<p>其实，仔细想想，人类社会的发展演化过程，是不是就是一个大型的<strong>无监督学习</strong>的过程呢？我们都是这个学习过程中的一环，是这个大型的神经网络中的一个神经元，我们社会的发展形态和目标在各个阶段都是不一样的，即使我们无法过远地预测到未来进化的方向，但我们一直在自我学习的过程中不断进化。也许这正是生命的本质。</p>
<hr>
<p>以上观点可能不是很严谨，也不是正统的机器学习理论，是本人对机器学习的一些思考。如有概念上的错误，欢迎斧正。也欢迎在评论区和我一起讨论<strong>机器学习</strong>的相关问题。</p>
<p>非常感谢您能读完我的文章，最后安利一下我的一个实验项目<a href="http://118.190.96.169:3389/iw/help/" target="_blank" rel="external">智能背词算法</a>，目前处于数据采集阶段，具体使用方式见帮助页面。谢谢~</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/03/05/【Tensorflow r1.0 文档翻译】【tf.contrib.learn快速入门】/" itemprop="url">
                【Tensorflow r1.0 文档翻译】【tf.contrib.learn快速入门】
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-03-05T15:19:58+08:00" content="2017-03-05">
            2017-03-05
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/03/05/【Tensorflow r1.0 文档翻译】【tf.contrib.learn快速入门】/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/05/【Tensorflow r1.0 文档翻译】【tf.contrib.learn快速入门】/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>TensorFlow的高级机器学习API(tf.contrib.learn)使得各种机器学习模型的配置、训练和评估都变得简单。在本教程中，你将使用tf.contrib.learn来构建一个<a href="https://en.wikipedia.org/wiki/Artificial_neural_network" target="_blank" rel="external">神经网络</a>分类器，并且在<a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" target="_blank" rel="external">Iris数据集</a>上进行训练，以达到通过萼片/花瓣几何来预测花的种类。您将编写代码以执行以下五个步骤：</p>
<ul>
<li>1.加载格包含Iris的训练和测试数据的CSV到TensorFlow数据集中。</li>
<li>2.构建一个<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier" target="_blank" rel="external">神经网络分类器</a>。</li>
<li>3.使用训练数据拟合模型</li>
<li>4.评估模型的准确性</li>
<li>5.分类新样品</li>
</ul>
<blockquote>
<p><strong>注意：</strong>在开始本教程之前，请确认在你的机器上已经<a href="https://www.tensorflow.org/install/index" target="_blank" rel="external">安装了TensorFlow</a>。</p>
</blockquote>
<h2 id="完整的神经网络源代码">完整的神经网络源代码</h2><p>这里是神经网络分类器的完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Data sets</span></div><div class="line">IRIS_TRAINING = <span class="string">"iris_training.csv"</span></div><div class="line">IRIS_TEST = <span class="string">"iris_test.csv"</span></div><div class="line"></div><div class="line"><span class="comment"># Load datasets.</span></div><div class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TRAINING,</div><div class="line">    target_dtype=np.int,</div><div class="line">    features_dtype=np.float32)</div><div class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    filename=IRIS_TEST,</div><div class="line">    target_dtype=np.int,</div><div class="line">    features_dtype=np.float32)</div><div class="line"></div><div class="line"><span class="comment"># Specify that all features have real-value data</span></div><div class="line">feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</div><div class="line"></div><div class="line"><span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></div><div class="line">classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</div><div class="line">                                            hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">                                            n_classes=<span class="number">3</span>,</div><div class="line">                                            model_dir=<span class="string">"/tmp/iris_model"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Fit model.</span></div><div class="line">classifier.fit(x=training_set.data,</div><div class="line">               y=training_set.target,</div><div class="line">               steps=<span class="number">2000</span>)</div><div class="line"></div><div class="line"><span class="comment"># Evaluate accuracy.</span></div><div class="line">accuracy_score = classifier.evaluate(x=test_set.data,</div><div class="line">                                     y=test_set.target)[<span class="string">"accuracy"</span>]</div><div class="line">print(<span class="string">'Accuracy: &#123;0:f&#125;'</span>.format(accuracy_score))</div><div class="line"></div><div class="line"><span class="comment"># Classify two new flower samples.</span></div><div class="line">new_samples = np.array(</div><div class="line">    [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>], [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=float)</div><div class="line">y = list(classifier.predict(new_samples, as_iterable=<span class="keyword">True</span>))</div><div class="line">print(<span class="string">'Predictions: &#123;&#125;'</span>.format(str(y)))</div></pre></td></tr></table></figure>
<p>接下来，我们将详细介绍这部分代码的细节。</p>
<h2 id="将Iris_CSV数据加载到TensorFlow">将Iris CSV数据加载到TensorFlow</h2><p><a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" target="_blank" rel="external">Iris数据集</a>包含150行数据，包括来自三个相关鸢尾花物种，其中每个物种包含50个样本：山鸢尾，杂色鸢尾和维吉尼亚鸢尾。</p>
<p><img src="/img/17_03_05/001.jpg" alt=""></p>
<p><strong>从左到右依次是：<a href="https://commons.wikimedia.org/w/index.php?curid=170298" target="_blank" rel="external">山鸢尾</a>(by <a href="https://commons.wikimedia.org/wiki/User:Radomil" target="_blank" rel="external">Radomil</a>, CC BY-SA 3.0),<a href="https://commons.wikimedia.org/w/index.php?curid=248095" target="_blank" rel="external">杂色鸢尾</a>(by <a href="https://commons.wikimedia.org/wiki/User:Dlanglois" target="_blank" rel="external">Dlanglois</a>, CC BY-SA 3.0)和<a href="https://www.flickr.com/photos/33397993@N05/3352169862" target="_blank" rel="external">维吉尼亚鸢尾</a>(by <a href="https://www.flickr.com/photos/33397993@N05" target="_blank" rel="external">Frank Mayfield</a>, CC BY-SA 2.0)</strong></p>
<p>每行包含每个花样品的以下数据：<a href="https://en.wikipedia.org/wiki/Sepal" target="_blank" rel="external">萼片</a>长度，萼片宽度，<a href="https://en.wikipedia.org/wiki/Petal" target="_blank" rel="external">花瓣</a>长度，花瓣宽度以及花的品种。花的品种用整数表示，0表示山鸢尾，1表示杂色鸢尾，2表示维吉尼亚鸢尾。</p>
<table>
<thead>
<tr>
<th style="text-align:left">萼片长度(Sepal Length)</th>
<th style="text-align:left">萼片宽度(Sepal Width)</th>
<th style="text-align:left">花瓣长度(Petal Length)</th>
<th style="text-align:left">花瓣宽度(Petal Width)</th>
<th style="text-align:left">品种(Species)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">5.1</td>
<td style="text-align:left">3.5</td>
<td style="text-align:left">1.4</td>
<td style="text-align:left">0.2</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">4.9</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">1.4</td>
<td style="text-align:left">0.2</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">4.7</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">1.3</td>
<td style="text-align:left">0.2</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
</tr>
<tr>
<td style="text-align:left">7.0</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">4.7</td>
<td style="text-align:left">1.4</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">6.4</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">4.5</td>
<td style="text-align:left">1.5</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">6.9</td>
<td style="text-align:left">3.1</td>
<td style="text-align:left">4.9</td>
<td style="text-align:left">1.5</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
</tr>
<tr>
<td style="text-align:left">6.5</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">5.2</td>
<td style="text-align:left">2.0</td>
<td style="text-align:left">2</td>
</tr>
<tr>
<td style="text-align:left">6.2</td>
<td style="text-align:left">3.4</td>
<td style="text-align:left">5.4</td>
<td style="text-align:left">2.3</td>
<td style="text-align:left">2</td>
</tr>
<tr>
<td style="text-align:left">5.9</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">5.1</td>
<td style="text-align:left">1.8</td>
<td style="text-align:left">2</td>
</tr>
</tbody>
</table>
<p>在本教程中，Iris数据已随机分到两个单独的CSV中：</p>
<ul>
<li>一个包含了120个样本的训练集(<a href="http://download.tensorflow.org/data/iris_training.csv" target="_blank" rel="external">iris_training.csv</a>)</li>
<li>一个包含了30个样本的测试集(<a href="http://download.tensorflow.org/data/iris_test.csv" target="_blank" rel="external">iris_test.csv</a>)</li>
</ul>
<p>将这些文件放在与Python代码相同的目录中。</p>
<p>首先导入TensorFlow和numpy：</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="title">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><span class="title">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><span class="title">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<p>接下来，使用<code>learn.datasets.base</code>中的<a href="https://www.tensorflow.org/code/tensorflow/contrib/learn/python/learn/datasets/base.py" target="_blank" rel="external"><code>load_csv_with_header()</code></a>方法将训练和测试集装入数据集。<code>load_csv_with_header()</code>方法需要三个必需的参数：</p>
<ul>
<li><code>filename</code>，CSV文件的路径</li>
<li><code>target_dtype</code>，接受数据集的目标值的<a href="http://docs.scipy.org/doc/numpy/user/basics.types.html" target="_blank" rel="external"><code>numpy</code>数据类型</a>。</li>
<li><code>features_dtype</code>，接受数据集的特征值的<a href="http://docs.scipy.org/doc/numpy/user/basics.types.html" target="_blank" rel="external"><code>numpy</code>数据类型</a>。</li>
</ul>
<p>在这里，target（你训练模型预测的值）是花种，它是一个从0-2的整数，所以对应的适当的<code>numpy</code>数据类型是<code>np.int</code>：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Data sets</span></div><div class="line"><span class="attr">IRIS_TRAINING</span> = <span class="string">"iris_training.csv"</span></div><div class="line"><span class="attr">IRIS_TEST</span> = <span class="string">"iris_test.csv"</span></div><div class="line"></div><div class="line"><span class="comment"># Load datasets.</span></div><div class="line"><span class="attr">training_set</span> = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    <span class="attr">filename=IRIS_TRAINING,</span></div><div class="line">    <span class="attr">target_dtype=np.int,</span></div><div class="line">    <span class="attr">features_dtype=np.float32)</span></div><div class="line"><span class="attr">test_set</span> = tf.contrib.learn.datasets.base.load_csv_with_header(</div><div class="line">    <span class="attr">filename=IRIS_TEST,</span></div><div class="line">    <span class="attr">target_dtype=np.int,</span></div><div class="line">    <span class="attr">features_dtype=np.float32)</span></div></pre></td></tr></table></figure>
<p>tf.contrib.learn中的<code>Dataset</code>是<a href="https://docs.python.org/2/library/collections.html#collections.namedtuple" target="_blank" rel="external">命名元组</a>；您可以通过<code>data</code>和<code>target</code>字段访问特征数据和目标值。这里<code>training_set.data</code>和<code>training_set.target</code>分别包含训练集的特征数据和目标值；<code>test_set.data</code>和<code>test_set.target</code>分别包含测试集的特征数据和目标值。</p>
<p>在后面的<a href="#将DNN分类器用于Iris训练数据">“将DNN分类器用于Iris训练数据”</a>中，你将使用到<code>training_set.data</code>和<code>training_set.target</code>来训练你的模型，在<a href="#评估模型精度">“评估模型精度”</a>中，你将使用<code>test_set.data</code>和<code>test_set.target</code>。但首先，你需要在下一节中构建你的模型。</p>
<h2 id="构建一个深度神经网络分类器">构建一个深度神经网络分类器</h2><p>tf.contrib.learn提供了一系列预定义的模型，叫做<a href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators" target="_blank" rel="external">Estimator</a>s。通过Estimator，您可以对您的数据很方便的进行训练和评估操作，达到“开箱即用”的效果。在这里，您将配置一个深层神经网络分类器模型以适应Iris数据。通过使用tf.contrib.learn，你可以仅仅使用一行代码就实例化一个<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier" target="_blank" rel="external"><code>tf.contrib.learn.DNNClassifier</code></a>。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># Specify that all features have real-value data</div><div class="line">feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">""</span>, dimension=<span class="number">4</span>)]</div><div class="line"></div><div class="line"># Build <span class="number">3</span> layer DNN with <span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span> units respectively.</div><div class="line">classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</div><div class="line">                                            hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</div><div class="line">                                            n_classes=<span class="number">3</span>,</div><div class="line">                                            model_dir=<span class="string">"/tmp/iris_model"</span>)</div></pre></td></tr></table></figure>
<p>上面的代码首先定义了模型的特征列，它指定了数据集中特征的数据类型。所有的特征数据都是连续的，因此<code>tf.contrib.layers.real_valued_column</code>是用于构造特征列的适当函数。数据集中有四个特征（萼片宽度，萼片高度，花瓣宽度和花瓣高度），因此相应的尺寸必须设置为4以保存所有数据。</p>
<p>然后，代码使用以下参数创建<code>DNNClassifier</code>模型：</p>
<ul>
<li><code>feature_columns=feature_columns</code>。上面定义的一组特征</li>
<li><code>hidden_units=[10, 20, 10]</code>。三个<a href="http://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw" target="_blank" rel="external">隐藏层</a>分别包含10，20，10个神经元。</li>
<li><code>n_classes=3</code>。三个目标类，代表三个鸢尾物种。</li>
<li><code>model_dir=/tmp/iris_model</code>。TensorFlow在模型训练期间将保存检查点数据的目录。有关使用TensorFlow进行日志记录和监视的更多信息，请见<a href="">使用tf.contrib.learn记录和监视的基本知识</a>。</li>
</ul>
<h2 id="将DNN分类器用于Iris训练数据">将DNN分类器用于Iris训练数据</h2><p>现在，你已经配置好了你的DNN<code>classifier</code>模型，你可以使用<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/BaseEstimator#fit" target="_blank" rel="external"><code>fit</code></a>方法来将Iris训练数据应用到分类器上。将特征数据（<code>training_set.data</code>），目标值（<code>training_set.target</code>）和要训练的步数（这里是<code>2000</code>）作为参数传递：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Fit model</span></div><div class="line">classifier.fit(<span class="attr">x=training_set.data,</span> <span class="attr">y=training_set.target,</span> <span class="attr">steps=2000)</span></div></pre></td></tr></table></figure>
<p>模型的状态保存在<code>classifier</code>(分类器)中，这意味着如果你喜欢，你可以迭代地训练。上面的代码执行效果等同于下面这两行代码：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">classifier.fit(x=training_set<span class="selector-class">.data</span>, y=training_set<span class="selector-class">.target</span>, steps=<span class="number">1000</span>)</div><div class="line">classifier.fit(x=training_set<span class="selector-class">.data</span>, y=training_set<span class="selector-class">.target</span>, steps=<span class="number">1000</span>)</div></pre></td></tr></table></figure>
<p>但是，如果您希望在训练时跟踪模型，则可能需要使用TensorFlow<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/monitors" target="_blank" rel="external">monitor</a>(监视器)来执行日志操作。关于这个主题更多的内容，请见教程<a href="">使用tf.contrib.learn记录和监视的基本知识</a>。</p>
<h2 id="评估模型精度">评估模型精度</h2><p>你已经将Iris的训练数据适配到了<code>DNNClassifier</code>模型上；现在，您可以使用<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/BaseEstimator#evaluate" target="_blank" rel="external"><code>evaluate</code></a>方法在Iris测试数据上检查其准确性。像<code>fit</code>（拟合）一样，<code>evaluate</code>（评估操作）将特征数据和目标值作为参数，并返回带有评估结果的<code>dict</code>（字典）。以下代码通过了Iris测试数据-<code>test_set.data</code>和<code>test_set.target</code>来评估和打印结果的准确性：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">accuracy_score = classifier.evaluate(x=test_set<span class="selector-class">.data</span>, y=test_set.target)[<span class="string">"accuracy"</span>]</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">'Accuracy: &#123;0:f&#125;'</span>.format(accuracy_score)</span></span>)</div></pre></td></tr></table></figure>
<p>运行全部的脚本，并检查结果的准确度：</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">Accuracy:</span> <span class="number">0.966667</span></div></pre></td></tr></table></figure>
<p>您的准确度结果可能有所不同，但应该是高于90％的。这对于相对较小的数据集是一个不错的结果了！</p>
<h2 id="分类新样品">分类新样品</h2><p>使用评估器的<code>predict()</code>方法来分类一个新的样本。例如，说你有这两个新的花样本：</p>
<table>
<thead>
<tr>
<th style="text-align:left">萼片长度(Sepal Length)</th>
<th style="text-align:left">萼片宽度(Sepal Width)</th>
<th style="text-align:left">花瓣长度(Petal Length)</th>
<th style="text-align:left">花瓣宽度(Petal Width)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">6.4</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">4.5</td>
<td style="text-align:left">1.5</td>
</tr>
<tr>
<td style="text-align:left">5.8</td>
<td style="text-align:left">3.1</td>
<td style="text-align:left">5.0</td>
<td style="text-align:left">1.7</td>
</tr>
</tbody>
</table>
<p>你可以用以下代码预测他们的物种：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># Classify two new flower samples.</div><div class="line">new_samples = np.array(</div><div class="line">    <span class="string">[[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]]</span>, dtype=float)</div><div class="line">y = list(classifier.predict(new_samples, as_iterable=True))</div><div class="line"><span class="built_in">print</span>(<span class="string">'Predictions: &#123;&#125;'</span>.format(str(y)))</div></pre></td></tr></table></figure>
<p><code>predict()</code>方法返回了一个预测数组，每个样本对应其中的一个结果：</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Prediction: <span class="string">[1 2]</span></div></pre></td></tr></table></figure>
<p>该模型预测的结果为：第一个样本是杂色鸢尾，第二个样本是维吉尼亚鸢尾。</p>
<h2 id="其他资源">其他资源</h2><ul>
<li>有关tf.contrib.learn的更多参考资料，请参阅<a href="https://www.tensorflow.org/api_guides/python/contrib.learn" target="_blank" rel="external">官方API文档</a>。</li>
<li>要了解有关使用tf.contrib.learn创建线性模型的更多信息，请参阅<a href="https://www.tensorflow.org/tutorials/linear" target="_blank" rel="external">使用TensorFlow的大型线性模型</a>。</li>
<li>要使用tf.contrib.learn API构建自己的评估器，请查看<a href="http://terrytangyuan.github.io/2016/07/08/understand-and-build-tensorflow-estimator/" target="_blank" rel="external">TensorFlow中的Building Machine Learning Estimator</a>。</li>
<li>要在浏览器中尝试神经网络建模和可视化，请查看<a href="http://playground.tensorflow.org/" target="_blank" rel="external">Deep Playground</a>。</li>
<li>有关神经网络的更高级教程，请参阅<a href="https://www.tensorflow.org/tutorials/deep_cnn" target="_blank" rel="external">卷积神经网络</a>和<a href="https://www.tensorflow.org/tutorials/recurrent" target="_blank" rel="external">循环神经网络</a>。</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/03/03/【Tensorflow r1.0 文档翻译】Tensorflow原理导论/" itemprop="url">
                【Tensorflow r1.0 文档翻译】Tensorflow原理导论
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-03-03T21:19:58+08:00" content="2017-03-03">
            2017-03-03
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/03/03/【Tensorflow r1.0 文档翻译】Tensorflow原理导论/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/03/【Tensorflow r1.0 文档翻译】Tensorflow原理导论/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>代码：<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/" target="_blank" rel="external">tensorflow/examples/tutorials/mnist/</a></p>
<p>这篇教程的目的是为了展示如何使用TensorFlow来训练并评估一个简单的<strong>前馈神经网络(feed-forward neural network)</strong>用来识别MNIST手写数字数据集。本教程的目标读者是有兴趣使用TensorFlow的有经验的机器学习用户。</p>
<p>这部分教程不是为了教授普通的机器学习。</p>
<p>请确保您已按照说明<a href="https://www.tensorflow.org/install/index" target="_blank" rel="external">安装了TensorFlow</a>。</p>
<h2 id="教程文件">教程文件</h2><p>本教程引用以下文件：</p>
<table>
<thead>
<tr>
<th style="text-align:left">文件</th>
<th style="text-align:left">目标</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py" target="_blank" rel="external"><code>mnist.py</code></a></td>
<td style="text-align:left">构建一个完全连接的MNIST模型的代码。</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py" target="_blank" rel="external"><code>fully_connected_feed.py</code></a></td>
<td style="text-align:left">利用下载的数据集训练构建好的MNIST模型的主要代码，以数据反馈字典（feed dictionary）的形式作为输入模型。</td>
</tr>
</tbody>
</table>
<p>只需要运行<code>fully_connected_feed.py</code>文件，就可以开启训练：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">python</span> fully_connected_feed.<span class="keyword">py</span></div></pre></td></tr></table></figure>
<h2 id="准备数据">准备数据</h2><p>MNIST是机器学习中的经典问题。这个问题是查看28x28像素的手写数字灰度图像，并确定图像表示的数字，数字范围是0到9。</p>
<p><img src="/img/17_03_03/001.png" alt=""></p>
<p>更多的信息，参加<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">Yann LeCun’s MNIST page</a>或者<a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" target="_blank" rel="external">Chris Olah’s visualizations of MNIST</a>。</p>
<h3 id="下载">下载</h3><p>在<code>run_training()</code>方法的开始部分，<code>input_data.read_data_sets()</code>方法会确保你的本地训练文件夹中，已经下载了正确的数据，然后将这些数据解压并返回一个含有<code>DataSet</code>实例的字典。</p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dat<span class="built_in">a_sets</span> = input_data.read_dat<span class="built_in">a_sets</span>(FLAGS.train_dir, FLAGS.fake_data)</div></pre></td></tr></table></figure>
<p><strong>注意：</strong><code>fake_data</code>标记是用于单元测试的，读者可以不必理会。</p>
<table>
<thead>
<tr>
<th style="text-align:left">数据集</th>
<th style="text-align:left">目标</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>data_sets.train</code></td>
<td style="text-align:left">55000图像和标签，用于初级训练。</td>
</tr>
<tr>
<td style="text-align:left"><code>data_sets.validation</code></td>
<td style="text-align:left">5000图像和标签，用于迭代验证训练准确性。</td>
</tr>
<tr>
<td style="text-align:left"><code>data_sets.test</code></td>
<td style="text-align:left">10000图像和标签，用于最终测试训练的准确性。</td>
</tr>
</tbody>
</table>
<h3 id="输入和占位符">输入和占位符</h3><p><code>placeholder_inputs()</code>方法创建了两个<code>tf.placeholder</code>操作，用于定义输入的形状。形状参数中包含<code>batch_size</code>值，后续还会将实际的训练样本传入图中。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attr">images_placeholder</span> = tf.placeholder(tf.float32, <span class="attr">shape=(batch_size,</span></div><div class="line">                                                       mnist.IMAGE_PIXELS))</div><div class="line"><span class="attr">labels_placeholder</span> = tf.placeholder(tf.int32, <span class="attr">shape=(batch_size))</span></div></pre></td></tr></table></figure>
<p>在训练的循环代码的下方，传入的整个图像和标签数据集会被切片，以符合每一个操作所设置的<code>batch_size</code>值，占位符操作将会填补以符合这个<code>batch_size</code>值。然后使用<code>feed_dict</code>参数，将数据传入<code>sess.run()</code>函数。</p>
<h2 id="构建图">构建图</h2><p>在为数据创建占位符之后，就可以运行<code>mnist.py</code>文件，经过三阶段的模式函数操作：<code>inference()</code>， <code>loss()</code>，和<code>training()</code>。图表就构建完成了。</p>
<ul>
<li>1.<code>inference()</code>-尽可能地构建好图表，满足促使神经网络向前反馈并做出预测的要求。</li>
<li>2.<code>loss()</code>-往inference图表中添加生成损失（loss）所需要的操作（ops）。</li>
<li>3.<code>training()</code>-往损失图表中添加计算并应用梯度（gradients）所需的操作。</li>
</ul>
<p><img src="/img/17_03_03/002.png" alt=""></p>
<h3 id="推理(Inference)">推理(Inference)</h3><p><code>inference()</code>函数会尽可能地构建图表，做到返回包含了预测结果（output prediction）的Tensor。</p>
<p>它采用图像占位符作为输入，并在其上借助<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="external">ReLU</a>)激活函数构建一对完全连接层，以及一个有着十个节点、指明了输出logtis模型的线性层。</p>
<p>每个图层都在唯一的<a href="https://www.tensorflow.org/api_docs/python/tf/name_scope" target="_blank" rel="external"><code>tf.name_scope</code></a>下创建，创建于该作用域之下的所有元素都将带有其前缀。</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'hidden1'</span>):</div></pre></td></tr></table></figure>
<p>在定义的范围内，由这些层中的每一个使用的权重和偏差被生成为<a href="https://www.tensorflow.org/api_docs/python/tf/Variable" target="_blank" rel="external"><code>tf.Variable</code></a>实例，具有它们期望的形状：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">weights = <span class="keyword">tf</span>.Variable(</div><div class="line">    <span class="keyword">tf</span>.truncated_normal([IMAGE_PIXELS, hidden1_units],</div><div class="line">                        stddev=<span class="number">1.0</span> / math.<span class="built_in">sqrt</span>(float(IMAGE_PIXELS))),</div><div class="line">    name=<span class="string">'weights'</span>)</div><div class="line">biases = <span class="keyword">tf</span>.Variable(<span class="keyword">tf</span>.zeros([hidden1_units]),</div><div class="line">                     name=<span class="string">'biases'</span>)</div></pre></td></tr></table></figure>
<p>例如，当在<code>hidden1</code>范围下创建这些时，赋予权重变量的唯一名称将是“<code>hidden1 / weights</code>”。</p>
<p>每个变量在构建时，都会执行初始化操作。</p>
<p>在大多数情况下，通过<a href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal" target="_blank" rel="external"><code>tf.truncated_normal</code></a>函数初始化权重变量，给赋予的shape则是一个二维tensor，其中第一个维度代表该层中权重变量所连接（connect from）的单元数量，第二个维度代表该层中权重变量所连接到的（connect to）单元数量。第一层，名字为<code>hidden1</code>，它的尺寸是<code>[IMAGE_PIXELS, hidden1_units]</code>，因为权重变量将图像输入连接到了<code>hidden1</code>层。<code>tf.truncated_normal</code>初始函数将根据所得到的均值和标准差，生成一个随机分布。</p>
<p>然后，通过<a href="https://www.tensorflow.org/api_docs/python/tf/zeros" target="_blank" rel="external"><code>tf.zeros</code></a>函数初始化偏差变量（biases），确保所有偏差的起始值都是0，而它们的形状则是其在该层中所接到的（connect to）单元数量。</p>
<p>图表的三个主要操作，分别是两个<code>tf.nn.relu</code>操作，它们中嵌入了隐藏层所需的<a href="https://www.tensorflow.org/api_docs/python/tf/matmul" target="_blank" rel="external"><code>tf.matmul</code></a>；以及logits模型所需的另外一个<code>tf.matmul</code>。三者依次生成，各自的<code>tf.Variable</code>实例则与输入占位符或下一层的输出tensor所连接。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden1 = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(images, weights) + biases)</div></pre></td></tr></table></figure>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hidden2 = tf<span class="selector-class">.nn</span><span class="selector-class">.relu</span>(tf.matmul(hidden1, weights) + biases)</div></pre></td></tr></table></figure>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">logits</span> = tf.matmul(hidden2, weights) + biases</div></pre></td></tr></table></figure>
<p>最终，程序会返回包含了输出结果的<code>logits</code>Tensor。</p>
<h3 id="损失">损失</h3><p><code>loss()</code>函数通过添加所需的损失操作，进一步构建图表。</p>
<p>首先，来自<code>labels_placeholder</code>的值将转换为64位整数。然后，添加一个<a href="https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits" target="_blank" rel="external">tf.nn.sparse_softmax_cross_entropy_with_logits</a>操作，以从<code>labels_placeholder</code>自动生成1-hot标签，并且与<code>inference()</code>函数的输出logits与那些1-hot标签进行比较。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attr">labels</span> = tf.to_int64(labels)</div><div class="line"><span class="attr">cross_entropy</span> = tf.nn.sparse_softmax_cross_entropy_with_logits(</div><div class="line">    <span class="attr">labels=labels,</span> <span class="attr">logits=logits,</span> <span class="attr">name='xentropy')</span></div></pre></td></tr></table></figure>
<p>然后使用<a href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" target="_blank" rel="external"><code>tf.reduce_mean</code></a>来求在批量维度（第一维度）上的交叉熵的平均值，作为总损失。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">loss</span> = tf.reduce_mean(cross_entropy, name=<span class="string">'xentropy_mean'</span>)</div></pre></td></tr></table></figure>
<p>然后将包含损失值的张量返回。</p>
<blockquote>
<p><strong>注意：</strong>交叉熵是信息论中的一种理论，它用于描述神经网络的预测结果相对于实际所给定的真实结果的偏差程度。更多的信息，请参阅博文<a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="external">《可视化信息理论》</a>。</p>
</blockquote>
<h3 id="训练">训练</h3><p><code>training()</code>方法通过添加<a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="external">梯度下降</a>的操作来最小化损失。</p>
<p>首先，它通过<code>loss()</code>方法接受损失tensor，然后传递到<a href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar" target="_blank" rel="external"><code>tf.summary.scalar</code></a>，用于在与<code>SummaryWriter</code>（见下文）一起使用时生成事件文件中的摘要值的操作。在这里，它将在每次写出摘要时发出损失的快照值。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf<span class="selector-class">.summary</span><span class="selector-class">.scalar</span>(<span class="string">'loss'</span>, loss)</div></pre></td></tr></table></figure>
<p>接下来，我们实例化一个<a href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer" target="_blank" rel="external"><code>tf.train.GradientDescentOptimizer</code></a>，负责按照所要求的学习效率（learning rate）应用梯度下降法（gradients）。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(learning_rate)</div></pre></td></tr></table></figure>
<p>之后，我们生成一个单个的变量用于统计全局训练的次数，<a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize" target="_blank" rel="external"><code>tf.train.Optimizer.minimize</code></a>操作被同时用作在系统中更新可训练的权值，以及增加全局步长（global step）。按照惯例，这个操作被称为<code>train_op</code>，TensorFlow会话必须运行的，以便引入一个完整的训练步骤（见下文）。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">global_step</span> = tf.Variable(<span class="number">0</span>, name=<span class="string">'global_step'</span>, trainable=<span class="literal">False</span>)</div><div class="line"><span class="attr">train_op</span> = optimizer.minimize(loss, global_step=global_step)</div></pre></td></tr></table></figure>
<h2 id="训练模型">训练模型</h2><p>一旦图被构建，它就可以在由<code>fully_connected_feed.py</code>中的用户代码控制的循环中迭代地训练和求值。</p>
<h3 id="图">图</h3><p>在<code>run_training()</code>方法的一开始的部分，是一个python的<code>with</code>命令，这表示所有构建的操作将与默认全局<a href="https://www.tensorflow.org/api_docs/python/tf/Graph" target="_blank" rel="external"><code>tf.Graph</code></a>实例相关联。</p>
<figure class="highlight coq"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">with</span> tf.<span class="keyword">Graph</span>().as_default():</div></pre></td></tr></table></figure>
<p><code>tf.Graph</code>实例是一系列可以作为整体执行的操作。TensorFlow的大部分场景只需要依赖默认图表一个实例即可。</p>
<p>利用多个图表的更加复杂的使用场景也是可能的，但是超出了本教程的范围。</p>
<h3 id="会话">会话</h3><p>完成全部的构建准备、生成全部所需的操作之后，我们就可以创建一个<a href="https://www.tensorflow.org/api_docs/python/tf/Session" target="_blank" rel="external">tf.Session</a>，用于运行图表。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sess</span> = tf.Session()</div></pre></td></tr></table></figure>
<p>另外，也可以利用<code>with</code>代码块生成<code>Session</code>，限制作用域：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></div></pre></td></tr></table></figure>
<p><code>Session</code>函数中没有传入参数，表明该代码将会依附于（如果还没有创建会话，则会创建新的会话）默认的本地会话。</p>
<p>生成会话之后，所有<code>tf.Variable</code>实例都会立即通过调用各自初始化操作中的<a href="https://www.tensorflow.org/api_docs/python/tf/Session#run" target="_blank" rel="external"><code>tf.Session.run</code></a>函数进行初始化。</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">init</span> = tf.global_variables_initializer()</div><div class="line">sess.run(<span class="keyword">init</span>)</div></pre></td></tr></table></figure>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/Session#run" target="_blank" rel="external"><code>tf.Session.run</code></a>方法将会运行图表中与作为参数传入的操作相对应的完整子集。在初次调用时，<code>init</code>操作只包含了变量初始化程序<a href="https://www.tensorflow.org/api_docs/python/tf/group" target="_blank" rel="external"><code>tf.group</code></a>。图表的其他部分不会在这里，而是在下面的训练循环运行。</p>
<h3 id="训练循环">训练循环</h3><p>在通过会话来初始化变量后，就可以开始训练了。</p>
<p>训练的每一步都是通过用户代码控制，而能实现有效训练的最简单循环就是：</p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="keyword">step</span> <span class="keyword">in</span> xrange(FLAGS.max_steps):</div><div class="line">    sess.<span class="built_in">run</span>(train_op)</div></pre></td></tr></table></figure>
<p>但是，本教程中的例子要更为复杂一点，原因是我们必须把输入的数据根据每一步的情况进行切分，以匹配之前生成的占位符。</p>
<h3 id="向图表提供反馈">向图表提供反馈</h3><p>执行每一步时，我们的代码会生成一个反馈字典（feed dictionary），其中包含对应步骤中训练所要使用的样本，这些样本的key就是其所代表的占位符操作。</p>
<p><code>fill_feed_dict</code>函数会查询给定的<code>DataSet</code>，索要下一批次`batch_size的图像和标签，与占位符相匹配的Tensor则会包含下一批次的图像和标签。</p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">images_feed, labels_feed = data_set.next_batch(<span class="keyword">FLAGS</span>.batch_size,</div><div class="line">                                               <span class="keyword">FLAGS</span>.fake_data)</div></pre></td></tr></table></figure>
<p>然后，以占位符作为键，创建一个Python字典对象，值则是其代表的反馈Tensor。</p>
<figure class="highlight dts"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">feed_dict = &#123;</div><div class="line"><span class="symbol">    images_placeholder:</span> images_feed,</div><div class="line"><span class="symbol">    labels_placeholder:</span> labels_feed,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个字典随后作为<code>feed_dict</code>参数，传入<code>sess.run()</code>函数中，为这一步的训练提供输入样本。</p>
<h3 id="检查状态">检查状态</h3><p>在运行<code>sess.run</code>时，要在代码中明确其需要获取的两个值：<code>[train_op, loss]</code>。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="keyword">step</span> <span class="keyword">in</span> <span class="built_in">xrange</span>(FLAGS.max_steps):</div><div class="line">    feed_dict = fill_feed_dict(data_sets.train,</div><div class="line">                               images_placeholder,</div><div class="line">                               labels_placeholder)</div><div class="line">    <span class="symbol">_</span>, loss_value = sess.run([train_op, loss],</div><div class="line">                             feed_dict=feed_dict)</div></pre></td></tr></table></figure>
<p>因为要获取这两个值，<code>sess.run()</code>会返回一个有两个元素的元组。其中每一个<code>Tensor</code>对象，对应了返回的元组中的numpy数组，而这些数组中包含了当前这步训练中对应Tensor的值。由于<code>train_op</code>并不会产生输出，其在返回的元祖中的对应元素就是<code>None</code>，所以会被抛弃。但是，如果模型在训练中出现偏差，<code>loss</code> Tensor的值可能会变成NaN，所以我们要获取它的值，并记录下来。</p>
<p>假设训练一切正常，没有出现NaN，训练循环会每隔100个训练步骤，就打印一行简单的状态文本，告知用户当前的训练状态。</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">if</span> <span class="built_in">step</span> % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    <span class="built_in">print</span> <span class="string">'Step %d: loss = %.2f (%.3f sec)'</span> % (<span class="built_in">step</span>, loss_value, duration)</div></pre></td></tr></table></figure>
<h3 id="状态可视化">状态可视化</h3><p>为了发出<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">TensorBoard</a>所使用的事件文件（events file），所有的摘要（在这里只有一个）都要在图构建阶段合并至一个Tensor中。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">summary</span> = tf<span class="selector-class">.summary</span><span class="selector-class">.merge_all</span>()</div></pre></td></tr></table></figure>
<p>在创建好会话（session）之后，可以实例化一个<a href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter" target="_blank" rel="external"><code>tf.summary.FileWriter</code></a>，用于写入包含了图表本身和即时数据具体值的事件文件。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">summary_writer = tf<span class="selector-class">.summary</span><span class="selector-class">.FileWriter</span>(FLAGS<span class="selector-class">.train_dir</span>, sess.graph)</div></pre></td></tr></table></figure>
<p>最后，每次评估<code>summary</code>(摘要)并将输出传递给<code>add_summary()</code>函数时，事件文件将被新的摘要值更新。</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">summary_str = sess.<span class="built_in">run</span>(summary, feed_dict=feed_dict)</div><div class="line">summary_writer.add_summary(summary_str, <span class="built_in">step</span>)</div></pre></td></tr></table></figure>
<p>事件文件写入完毕之后，可以就训练文件夹打开一个TensorBoard，查看即时数据的情况。</p>
<p><img src="/img/17_03_03/003.png" alt=""></p>
<blockquote>
<p><strong>注意：</strong>了解更多如何构建并运行TensorBoard的信息，请查看相关教程<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">Tensorboard：训练过程可视化</a>。</p>
</blockquote>
<h3 id="保存检查点">保存检查点</h3><p>为了得到可以用来后续恢复模型以进一步训练或评估的检查点文件（checkpoint file），我们实例化一个<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="external"><code>tf.train.Saver</code></a>。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver = tf<span class="selector-class">.train</span><span class="selector-class">.Saver</span>()</div></pre></td></tr></table></figure>
<p>在训练循环中，将定期调用<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#save" target="_blank" rel="external"><code>tf.train.Saver.save</code></a>方法，使用所有可训练变量的当前值将检查点文件写入训练目录。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver.<span class="built_in">save</span>(sess, FLAGS.train_dir, global_step=<span class="keyword">step</span>)</div></pre></td></tr></table></figure>
<p>在将来的某个时间点，可以通过使用<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#restore" target="_blank" rel="external"><code>tf.train.Saver.restore</code></a>方法重新加载模型参数来恢复训练。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">saver</span><span class="selector-class">.restore</span>(<span class="selector-tag">sess</span>, <span class="selector-tag">FLAGS</span><span class="selector-class">.train_dir</span>)</div></pre></td></tr></table></figure>
<h2 id="评估模型">评估模型</h2><p>每隔一千个训练步骤，我们的代码会尝试使用训练数据集与测试数据集，对模型进行评估。<code>do_eval</code>函数会被调用三次，分别使用训练数据集、验证数据集合测试数据集。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">print</span> <span class="string">'Training Data Eval:'</span></div><div class="line"><span class="keyword">do</span>_<span class="built_in">eval</span>(sess,</div><div class="line">        <span class="built_in">eval</span>_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.train)</div><div class="line"><span class="built_in">print</span> <span class="string">'Validation Data Eval:'</span></div><div class="line"><span class="keyword">do</span>_<span class="built_in">eval</span>(sess,</div><div class="line">        <span class="built_in">eval</span>_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.validation)</div><div class="line"><span class="built_in">print</span> <span class="string">'Test Data Eval:'</span></div><div class="line"><span class="keyword">do</span>_<span class="built_in">eval</span>(sess,</div><div class="line">        <span class="built_in">eval</span>_correct,</div><div class="line">        images_placeholder,</div><div class="line">        labels_placeholder,</div><div class="line">        data_sets.test)</div></pre></td></tr></table></figure>
<blockquote>
<p>注意，更复杂的使用场景通常是，先隔绝<code>data_sets.test</code>测试数据集，只有在大量的超参数优化调整（hyperparameter tuning）之后才进行检查。但是，由于MNIST问题比较简单，我们在这里一次性评估所有的数据。</p>
</blockquote>
<h3 id="构建评估图(Eval_Graph)">构建评估图(Eval Graph)</h3><p>在打开默认图表（Graph）之前，我们应该先调用get_data(train=False)函数，抓取测试数据集。</p>
<p>在进入训练循环之前，评估操作应该通过<code>mnist.py</code>中的<code>evaluate()</code>函数来构建。<code>evaluate()</code>传入的<code>logist</code>和标签参数与<code>loss()</code>函数相同。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">eval_correct</span> = mnist.evaluation(logits, labels_placeholder)</div></pre></td></tr></table></figure>
<p><code>evaluation()</code>函数会生成<a href="https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k" target="_blank" rel="external"><code>tf.nn.in_top_k</code></a>操作，如果在K个最有可能的预测中可以发现真的标签，那么这个操作就会将模型输出标记为正确。在本文中，我们把K的值设置为1，也就是只有在预测是真的标签时，才判定它是正确的。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">eval_correct = tf<span class="selector-class">.nn</span><span class="selector-class">.in_top_k</span>(logits, labels, <span class="number">1</span>)</div></pre></td></tr></table></figure>
<h3 id="评估输出">评估输出</h3><p>之后，我们可以创建一个循环，往其中添加<code>feed_dict</code>，并在调用<code>sess.run()</code>函数时传入<code>eval_correct</code>操作，目的就是用给定的数据集评估模型。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> step <span class="keyword">in</span> xrange(steps_per_epoch):</div><div class="line">    feed_dict = fill_feed_dict(data_<span class="built_in">set</span>,</div><div class="line">                               images_placeholder,</div><div class="line">                               labels_placeholder)</div><div class="line">    <span class="literal">true</span>_count += sess.run(<span class="built_in">eval</span>_correct, feed_dict=feed_dict)</div></pre></td></tr></table></figure>
<p><code>true_count</code>变量会累加所有<code>in_top_k</code>操作判定为正确的预测之和。接下来，只需要将正确测试的总数，除以例子总数，就可以得出准确率了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">precision = <span class="literal">true</span>_count / num_examples</div><div class="line"><span class="built_in">print</span>(<span class="string">'  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f'</span> %</div><div class="line">      (num_examples, <span class="literal">true</span>_count, precision))</div></pre></td></tr></table></figure></span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/26/【Tensorflow r1.0 文档翻译】深入MNIST--专家级/" itemprop="url">
                【Tensorflow r1.0 文档翻译】深入MNIST--专家级
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-26T14:50:58+08:00" content="2017-02-26">
            2017-02-26
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/26/【Tensorflow r1.0 文档翻译】深入MNIST--专家级/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/26/【Tensorflow r1.0 文档翻译】深入MNIST--专家级/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>TensorFlow是一个用于进行大规模数值计算的强大库。其擅长的任务之一是实施和训练深层神经网络。在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。</p>
<p>这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看<a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">新手指南</a>。在开始之前，请确认<a href="https://www.tensorflow.org/install/index" target="_blank" rel="external">安装</a>了TensorFlow。</p>
<h2 id="关于本教程">关于本教程</h2><p>本教程的第一部分解释了<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py" target="_blank" rel="external">mnist_softmax.py</a>代码中发生了什么，这是Tensorflow模型的基本实现。第二部分显示了一些提高精度的方法。</p>
<p>您可以将本教程中的每个代码段复制并粘贴到Python环境中，当然你也可以选择只是读一下这部分代码。</p>
<p>我们将在本教程中完成：</p>
<ul>
<li>创建一个softmax回归函数，这是一个用于识别MNIST数字的模型，其原理是基于查看图像中的每个像素。</li>
<li>使用Tensorflow来训练模型以识别数字，方法是“查看”数千个示例（并运行我们的第一个Tensorflow会话）。</li>
<li>使用我们的测试数据检查模型的精度。</li>
<li>构建，训练和测试多层卷积神经网络以提高结果。</li>
</ul>
<h2 id="准备工作">准备工作</h2><p>在我们创建模型之前，我们首先加载MNIST数据集，并启动TensorFlow会话。</p>
<h3 id="加载MNIST数据">加载MNIST数据</h3><p>如果您要复制粘贴本教程中的代码，请从这两行代码开始，这两行代码将自动下载并读入数据：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from tensorflow<span class="selector-class">.examples</span><span class="selector-class">.tutorials</span><span class="selector-class">.mnist</span> import input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=True)</div></pre></td></tr></table></figure>
<p>这里的<code>mnist</code>是一个轻量级类，将训练集，验证集和测试集存储为NumPy数组。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。</p>
<h3 id="启动TensorFlow_InteractiveSession">启动TensorFlow InteractiveSession</h3><p>Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。</p>
<p>这里，我们使用更加方便的<code>InteractiveSession</code>类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些<a href="https://www.tensorflow.org/get_started/get_started#the_computational_graph" target="_blank" rel="external">计算图</a>，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用<code>InteractiveSession</code>，那么你需要在启动session之前构建整个计算图，然后启<a href="https://www.tensorflow.org/get_started/get_started#the_computational_graph" target="_blank" rel="external">动该计算图</a>。</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="title">sess</span> = tf.<span class="type">InteractiveSession</span>()</div></pre></td></tr></table></figure>
<h3 id="计算图">计算图</h3><p>为了在Python中执行高效的数值计算，我们通常引入类似<strong><a href="http://www.numpy.org/" target="_blank" rel="external">NumPy</a></strong>这种库来执行开销昂贵的操作。例如在Python之外其他高效的语言来执行矩阵乘法这类操作。不幸的是，每次操作之后切换回Python的动作依然是一个巨大的开销。这种开销特别的差，如果你想要以一种分布式的方式运行在GPU上的话，这里传输数据将会是一个巨大的开销。</p>
<p>TensorFlow也会在Python外部执行大量的运算，但它做了进一步的处理来规避了这种开销。取代独立于Python运行单一的代价昂贵的操作的模式，TensorFlow的方式是通过在Python中描述一个可交互的操作图，然后完全在Python之外进行运行。<strong>Theano</strong>或者<strong>Torch</strong>也有与此类似的实现。</p>
<p>在这里Python代码的作用是用来在外部定义一个操作图，然后决定具体哪一部分的运算图要被运行。详细内容，见<a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门/">TensorFlow入门</a>中的<a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门#用于计算的Graph（图）">用于计算的Graph（图）</a>部分。</p>
<h2 id="构建一个Softmax回归模型">构建一个Softmax回归模型</h2><p>这一节，我们通过一个单一的线性层来构建一个softmax回归模型。在下一节中，我们将把这个softmax回归扩展为一个多层卷积网络。</p>
<h3 id="占位符（Placeholders）">占位符（Placeholders）</h3><p>我们通过创建输入的图像创建的节点和输出的类别创建的分类来构建一个计算图。</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, <span class="built_in">shape</span>=[<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">y_ = tf.placeholder(tf.float32, <span class="built_in">shape</span>=[<span class="keyword">None</span>, <span class="number">10</span>])</div></pre></td></tr></table></figure>
<p>这里<code>x</code>和<code>y_</code>不是具体的值。相反，他们都是一个<code>placeholder</code>（占位符）–当我们让TensorFlow开始执行计算时才被输入具体值。</p>
<p>输入图像的<code>x</code>包含一个2维的浮点数张量。这里我们赋予它一个<code>shape</code>（形状）为<code>[None, 784]</code>，其中<code>784</code>是由28乘28像素的图片单行展开后的维度数，<code>None</code>表示第一个维度大小不定，可以是任意尺寸，用以指代batch的大小。目标输出类别<code>y_</code>也包含一个2维的tensor，它每行都是一个10维的one-hot向量，用于表示相应的MNIST图像属于哪个数字类（0到9）。</p>
<p>虽然<code>placeholder</code>的<code>shape</code>参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。</p>
<h3 id="变量（Variables）">变量（Variables）</h3><p>我们现在为我们的模型定义了权值<code>W</code>和偏置量<code>b</code>。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：<code>Variable</code>。一个<code>Variable</code>代表TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用<code>Variable</code>来表示。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W</span> = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</div><div class="line"><span class="attr">b</span> = tf.Variable(tf.zeros([<span class="number">10</span>]))</div></pre></td></tr></table></figure>
<p>我们在调用<code>tf.Variable</code>的时候传入初始值。在这个例子中，我们把<code>W</code>和<code>b</code>初始化全为0的tensor。<code>W</code>是一个$784×10$的矩阵（因为我们有784个输入特征以及10个输出值），<code>b</code>是一个10维向量（因为我们有10种分类）。</p>
<p>在<code>Variable</code>可以在session中被使用之前，他们必须被session初始化。此步骤使用已经指定的初始值（在这里tensor全部以0填充），并将它们分配给每个$Variable$。下面的代码可以一次初始化全部的<code>Variables</code>：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">tf</span><span class="selector-class">.global_variables_initializer</span>())</div></pre></td></tr></table></figure>
<h3 id="类别预测与损失函数">类别预测与损失函数</h3><p>现在我们可以实现我们自己的回归模型了。只需要一行代码！我们把向量化后的图片输入<code>x</code>和权重矩阵<code>W</code>相乘，加上偏置<code>b</code>，然后计算每个分类的softmax概率值。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">y</span> = tf.matmul(x,W) + b</div></pre></td></tr></table></figure>
<p>我们可以很容易地指定一个损失函数。损失表示模型的预测效果在单个示例的糟糕程度；在我们的训练过程中，我们会尽量去最小化这个值。在这里，我们的损失函数就是介于目标值和应用于模型预测的softmax激励函数之间的交叉熵。正如我们在新手教学中用到的稳定的方程一样：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">cross_entropy</span> = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(<span class="attr">labels=y_,</span> <span class="attr">logits=y))</span></div></pre></td></tr></table></figure>
<p>请注意，<code>tf.nn.softmax_cross_entropy_with_logits</code>内部将softmax应用到非规范化的模型预测中，并且将所有的结果求和，通过<code>tf.reduce_mean</code>来取这些和的平均值。</p>
<h2 id="训练模型">训练模型</h2><p>现在我们已经定义好了我们的模型和用于训练的损失函数，那么用TensorFlow进行训练就很简单了。由于TensorFlow知道整个计算图，所以它可以使用自动微分来找出关于每个变量的损失梯度。TensorFlow有多种<a href="https://www.tensorflow.org/api_guides/python/train#optimizers" target="_blank" rel="external">内置的优化算法</a>。对于这个例子，我们将使用最大梯度下降，步长为0.5，来下降交叉熵。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(<span class="number">0.5</span>).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<p>TensorFlow在这一行中实际上是在计算图中添加新的操作。这些操作包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</p>
<p>返回的<code>train_step</code>操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行<code>train_step</code>来完成。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="symbol">_</span> <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</div><div class="line">  <span class="built_in">batch</span> = mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">  train_step.run(feed_dict=&#123;x: <span class="built_in">batch</span>[<span class="number">0</span>], y_: <span class="built_in">batch</span>[<span class="number">1</span>]&#125;)</div></pre></td></tr></table></figure>
<p>每次训练迭代我们都会加入100个训练样本。然后，然后执行一次<code>train_step</code>操作，并通过<code>feed_dict</code>将<code>placeholder</code>tensor<code>x</code>和<code>y_</code>，用训练训练数据替代。请注意，您可以使用<code>feed_dict</code>替换计算图形中的任何tensor。–它不仅仅局限于<code>placeholder</code>。</p>
<h3 id="评估模型">评估模型</h3><p>那么我们的模型表现如何呢？</p>
<p>首先，来让我们找出那些预测正确的标签。<code>tf.argmax</code>是一个很有用的方法，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。例如，<code>tf.argmax(y，1)</code>是我们的模型认为每个输入最可能的标签，而<code>tf.argmax(y_，1)</code>是正确的标签。我们可以用<code>tf.equal</code>来检查我们我预测值与真实值是否相符。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(<span class="keyword">y</span>,<span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_,<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code>会变成<code>[1,0,1,1]</code>，取平均值后得到<code>0.75</code>.</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</div></pre></td></tr></table></figure>
<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>
<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(accuracy.eval(feed_dict=&#123;<span class="attribute">x</span>: mnist<span class="variable">.test</span><span class="variable">.images</span>, y_: mnist<span class="variable">.test</span><span class="variable">.labels</span>&#125;))</div></pre></td></tr></table></figure>
<h2 id="构建多层卷积网络">构建多层卷积网络</h2><p>在MNIST数据集上获得92%的准确率是相当差的。甚至差到令人感到尴尬的地步。在本节中，我们将解决这个问题。我们将从一个非常简单的模型跳转到一个中等复杂的模型：一个小型的卷积神经网络。这将会使我们得到一个大概在99.2%的准确率。–虽然不是最好的结果，但还算是令人满意的一个结果。</p>
<h3 id="权值初始化">权值初始化</h3><p>要创建这个模型，我们需要创建很多权值和偏置量。通常情况下，应该使用少量噪音数据来初始化权值以用于打破对称性，并且防止0梯度产生。由于我们使用的是<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="external">ReLU</a>)神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个用于初始化的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</div><div class="line">  <span class="keyword">return</span> tf.Variable(initial)</div></pre></td></tr></table></figure>
<h3 id="卷积和池化">卷积和池化</h3><p>TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做最大池（max pooling）。为了使代码更简洁，我们把这部分抽象成一个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></div><div class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</div><div class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div></pre></td></tr></table></figure>
<h3 id="第一层卷积">第一层卷积</h3><p>现在，我们可以实现我们的第一层了。它由一个卷积接一个最大池组成。卷积在每个5x5的patch中算出32个特征。卷积的权重tensor形状是<code>[5, 5, 1, 32]</code>。前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</div><div class="line">b_conv1 = bias_variable([<span class="number">32</span>])</div></pre></td></tr></table></figure>
<p>为了用这一层，我们把<code>x</code>变成一个4维tensor，其第<code>2</code>、第<code>3</code>维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</div></pre></td></tr></table></figure>
<p>然后我们将<code>x_image</code>与权值tensor进行卷积，加上偏置量，然后应用ReLU激励函数，最后最大池化。<code>max_pool_2x2</code>方法可将图片大小缩小为14x14。</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div></pre></td></tr></table></figure>
<h3 id="第二层卷积">第二层卷积</h3><p>为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W_conv2</span> = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</div><div class="line"><span class="attr">b_conv2</span> = bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line"><span class="attr">h_conv2</span> = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line"><span class="attr">h_pool2</span> = max_pool_2x2(h_conv2)</div></pre></td></tr></table></figure>
<h3 id="密集连接层">密集连接层</h3><p>现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的tensor reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</div><div class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</div><div class="line"></div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div></pre></td></tr></table></figure>
<h3 id="Dropout">Dropout</h3><p>为了减少过拟合，我们在输出层之前加入<a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="external">dropout</a>。我们用一个<code>placeholder</code>来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的<code>tf.nn.dropout</code>操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。</p>
<blockquote>
<p>对于这个小型卷积网络，性能实际上几乎相同，没有压差。Dropout往往是非常有效的减少过度拟合的方式，但当训练非常大的神经网络时，它是最有用的。</p>
</blockquote>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">keep_prob</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">h_fc1_drop</span> = tf.nn.dropout(h_fc1, keep_prob)</div></pre></td></tr></table></figure>
<h3 id="读出层">读出层</h3><p>最后，我们添加一个softmax层，就像前面的单层softmax 回归一样。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W_fc2</span> = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</div><div class="line"><span class="attr">b_fc2</span> = bias_variable([<span class="number">10</span>])</div><div class="line"></div><div class="line"><span class="attr">y_conv</span> = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</div></pre></td></tr></table></figure>
<h3 id="训练和评估模型">训练和评估模型</h3><p>这个模型的效果如何呢？为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码。</p>
<p>不过有以下几点不同：</p>
<ul>
<li>我们将用更复杂的ADAM优化器来替换最陡的梯度下降优化器。</li>
<li>我们将在<code>feed_dict</code>中包含附加参数<code>keep_prob</code>来控制丢失率。</li>
<li>我们将在训练过程中的每执行100次迭代时，添加一次日志记录。</li>
</ul>
<p>随时可以继续运行此代码，但它会进行20,000次训练迭代，可能需要一段时间（可能长达半小时），具体时间取决于您的处理器。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">cross_entropy = <span class="keyword">tf</span>.reduce_mean(</div><div class="line">    <span class="keyword">tf</span>.<span class="keyword">nn</span>.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</div><div class="line">train_step = <span class="keyword">tf</span>.train.AdamOptimizer(<span class="number">1</span><span class="keyword">e</span>-<span class="number">4</span>).minimize(cross_entropy)</div><div class="line">correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(y_conv,<span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_,<span class="number">1</span>))</div><div class="line">accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</div><div class="line">sess.run(<span class="keyword">tf</span>.global_variables_initializer())</div><div class="line"><span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="number">20000</span>):</div><div class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">    train_accuracy = accuracy.<span class="built_in">eval</span>(feed_dict=&#123;</div><div class="line">        <span class="keyword">x</span>:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_pro<span class="variable">b:</span> <span class="number">1.0</span>&#125;)</div><div class="line">    <span class="keyword">print</span>(<span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy))</div><div class="line">  train_step.run(feed_dict=&#123;<span class="keyword">x</span>: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_pro<span class="variable">b:</span> <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line"><span class="keyword">print</span>(<span class="string">"test accuracy %g"</span>%accuracy.<span class="built_in">eval</span>(feed_dict=&#123;</div><div class="line">    <span class="keyword">x</span>: mnist.test.images, y_: mnist.test.labels, keep_pro<span class="variable">b:</span> <span class="number">1.0</span>&#125;))</div></pre></td></tr></table></figure>
<p>以上代码，在最终测试集上的准确率大概是99.2%。</p>
<p>目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/22/【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别/" itemprop="url">
                【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-22T19:50:58+08:00" content="2017-02-22">
            2017-02-22
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/22/【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/22/【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>本教程面向那些不熟悉<strong>机器学习</strong>和<strong>TensorFlow</strong>的读者。如果你已经知道MNIST是什么，softmax（多项Logistic）回归是什么，你可能更喜欢这个<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">更快节奏的教程</a>。在开始教程之前，请确认<a href="https://www.tensorflow.org/install/index" target="_blank" rel="external">安装TensorFlow</a>。</p>
<p>当一个人开始学习如何编程时，有一个传统，就是编写的第一个程序是能够打印”Hello World.”的程序。正如编程中的”Hello World”一样，机器学习中有MNIST。</p>
<p>MNIST是一个简单的计算机视觉数据集。它由像以下这样的手写数字的图像组成：</p>
<p><img src="/img/17_02_22/001.png" alt=""></p>
<p>它还包括每个图像的标签，用于标识是哪个数字。例如，上述图像的标签是<code>5</code>,<code>0</code>,<code>4</code>和<code>1</code>。</p>
<p>在本教程中，我们将训练一个模型，用来查看图像并预测它们是什么数字。我们的目标不是训练一个真正精准的，拥有高性能的模型，而是浅尝辄止的来体验一下TensorFlow的使用。 - 尽管我们稍后会给出实现这种效果的代码。因此，我们将从一个非常简单的，称为<strong>Softmax回归</strong>的模型开始。</p>
<p>这个教程的实际代码非常短，其中真正有趣的东西只有三行代码。然而，了解背后的想法是非常重要的：TensorFlow如何工作和核心机器学习概念。因此，我们将非常仔细地完成这部分代码。</p>
<h2 id="关于本教程">关于本教程</h2><p>本教程是对<a href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py" target="_blank" rel="external">mnist_softmax.py</a>中的代码进行逐行解释。</p>
<p>您可以通过以下几种不同的方式使用本教程：</p>
<ul>
<li>在阅读每行的解释时，将每个代码段逐行复制并粘贴到Python环境中。</li>
<li>在阅读教程期间，运行整个mnist_softmax.py，并使用本教程来了解您不清楚的代码行。</li>
</ul>
<p>我们将在本教程中完成：</p>
<ul>
<li>了解MNIST数据和softmax回归。</li>
<li>创建一个函数，它是一个用于识别数字的模型，其识别原理是基于查看图像中的每个像素的值来实现的。</li>
<li>使用TensorFlow来训练模型以识别数字，其训练方式是“查看”数千个示例（运行我们的第一个TensorFlow会话来执行此逻辑）。</li>
<li>使用我们的测试数据检查模型的精度。</li>
</ul>
<h2 id="MNIST数据集">MNIST数据集</h2><p>MNIST数据集托管在<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">Yann LeCun的站点</a>。如果您要复制粘贴本教程中的代码，请从这两行代码开始，这两行代码将自动下载并读入数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from tensorflow.examples.tutorials.mnist import input_data</div><div class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</div></pre></td></tr></table></figure>
<p>MNIST数据被分为三部分：55,000个训练数据（<code>mnist.train</code>），10,000个测试数据（<code>mnist.test</code>）和5,000个验证数据（<code>mnist.validation</code>）。这种切分是非常重要的：它能通过一部分我们并没有实际用来训练学习的数据，来确保我们的算法有很好的通用性。</p>
<p>如前所述，每个MNIST数据点有两个部分：手写数字的图像和相应的标签。我们称为图像”x”和标签”y”。训练集和测试集都包含图像及其相应的标签;例如训练图像是<code>mnist.train.images</code>，训练标签是<code>mnist.train.labels</code>。</p>
<p>每张图像的尺寸是28×28像素。我们可以把它解释为一个大的数组：</p>
<p><img src="/img/17_02_22/002.png" alt=""></p>
<p>我们可以将这个数组变成一个长度为28x28 = 784的向量。如何平铺数组其实并不重要，重要的是要保证图像和数组之间的一致性。从这个角度来看，MNIST图像只是784维向量空间中的一堆点，具有<a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" target="_blank" rel="external">非常丰富的结构</a>（警告：计算密集的可视化）。</p>
<p>展平数据丢弃了关于图像的2D结构的信息。这样做是不是并不够好？没错，最好的计算机视觉方法确实可以利用这种2D结构信息，我们将在后面的教程进行介绍。但是我们在这里所使用的一种简单方法：<strong>softmax回归</strong>（下面会给出定义），不会利用到这种信息。</p>
<p><code>mnist.train.images</code>是形状为<code>[55000,784]</code>的张量（n维数组）。第一个维度是在列表中图像的索引，第二个维度是每个图像中的每个像素点的索引。对于特定图像中的特定像素，张量中的每个条目是介于0和1之间的像素强度。</p>
<p><img src="/img/17_02_22/003.png" alt=""></p>
<p>MNIST中的每个图像都有相应的标签，标签用介于0到9之间的数字表示图像中绘制的数字。</p>
<p>为了达到本教程的目的，我们需要要将我们的标签作为“one-hot 向量”。one-hot向量是指在大多数维度上数值为0，仅在其中一个维度上数值为1的向量。在这种情况下，第n个数字将被表示为在第n维中为1的向量。例如，3将表示为$[0,0,0,1,0,0,0,0,0,0]$。因此，<code>mnist.train.labels</code>是一个形状为<code>[55000, 10]</code>的数字矩阵。</p>
<p><img src="/img/17_02_22/004.png" alt=""></p>
<p>现在，我们可以开始构建我们的模型啦！</p>
<h2 id="Softmax回归">Softmax回归</h2><p>我们知道MNIST中的每个图像都是一个在0和9之间的手写数字。因此，对于给定的图像，只有10种可能的结果。我们想要能够看到一个图像，并给出它的每个数字的概率。例如，用我们的模型来查看一个9的图片，80％的可能性确认是9，但有5％的可能是8（因为8和9顶部都有一个圈），剩余的可能性分布在其他数值上。</p>
<p>这是一个<strong>softmax回归</strong>的典型案例。如果你想给一个对象赋予其表示不同数字的概率，可以使用softmax，因为softmax可以得出一组介于0到1之间的值，并且这组值加起来结果为1。即使在以后，当我们训练其他更复杂的模型时，最后一步也是一层softmax。</p>
<p>softmax回归有两个步骤：首先我们将图片中属于某个特定数字的证据（evidence）相加，然后将该证据转换为概率。</p>
<p>为了计算给定图像在特定类中的证据，我们对像素强度进行加权求和。如果像素点有很高的强度表示和对应的标签数字不匹配，那么这一点的权值是负数，相反，权值是正数。</p>
<p>下面的图片显示了一个模型学习到的图片上每个像素对于特定数字类的权值。红色表示负权重，蓝色表示正权重。</p>
<p><img src="/img/17_02_22/005.png" alt=""></p>
<p>我们还需要增加一个偏置量（bias），因为输入往往会带有一些无关的干扰量。因此对于给定的输入图片<strong>x</strong>它代表的是数字<strong>i</strong>的证据可以表示为：</p>
<p>$$<br>\text{evidence}_i = \sum_j W_{i,~ j} x_j + b_i<br>$$</p>
<p>其中，$W_i$表示权值，$b_i$代表$i$类别的偏置量，$j$代表给定图片$x$的像素索引，用于像素求和。然后用softmax函数可以把这些证据转换成概率<strong>y</strong>：</p>
<p>$$<br>y = \text{softmax}(\text{evidence})<br>$$</p>
<p>这里softmax用作“激活”或“链接”函数，将我们的线性函数的输出变形为我们想要的形式 - 在这里，也就是10种数字的概率分布。你可以把它看作是将证据转换为每种分类的概率。它的定义是：</p>
<p>$$<br>\text{softmax}(x) = \text{normalize}(\exp(x))<br>$$</p>
<p>如果你把这个方程展开，你将得到：</p>
<p>$$<br>\text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}<br>$$</p>
<p>但通常我们把softmax定义为第一种形式：对其输入求幂，然后将其归一化处理。这里幂运算表示，更大的证据对应更大的假设模型（hypothesis）里面的乘数权重值。反之，拥有更少的证据意味着在假设模型里面拥有更小的乘数系数。假设模型里的权值不可以是0值或者负值。Softmax然后会正则化这些权重值，使它们的总和等于1，以此构造一个有效的概率分布。（更多的关于Softmax函数的信息，可以参考Michael Nieslen的书里面的这个<a href="http://neuralnetworksanddeeplearning.com/chap3.html#softmax" target="_blank" rel="external">部分</a>，其中有关于softmax的可交互式的可视化解释。）</p>
<p>softmax回归可以表示为下面这张图，不过真实情况下会有更多的$x$值。我们通过计算出$x$的权值之和加上一个偏置量，然后代入到一个softmax中，来计算出每个输出值。</p>
<p><img src="/img/17_02_22/006.png" alt=""></p>
<p>如果我们把它写成方程的形式，我们将得到：</p>
<p><img src="/img/17_02_22/007.png" alt=""></p>
<p>我们可以“向量化”这个过程，把它变成矩阵乘法和向量加法。这有助于提升计算效率。 （这也是一个有用的思考方式。）</p>
<p><img src="/img/17_02_22/008.png" alt=""></p>
<p>更紧凑的表达形式如下：</p>
<p>$$<br>y = \text{softmax}(Wx + b)<br>$$</p>
<p>现在让我们把它变成TensorFlow可以使用的形式。</p>
<h2 id="回归的实现">回归的实现</h2><p>为了在Python中进行高效的数值计算，我们通常使用像<strong><a href="http://www.numpy.org/" target="_blank" rel="external">NumPy</a></strong>这样的库，它们会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果要在GPU上以分布式方式运行计算，那么这种开销尤其糟糕，其中传输数据的成本很高。</p>
<p>TensorFlow也在Python之外做了很大量的计算工作，但它做了进一步的完善以改善前面说的那种切换。TensorFlow不是独立于Python运行一个昂贵的操作，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行。（这样类似的运行方式，可以在不少的机器学习库中看到。）</p>
<p>要使用TensorFlow，首先我们需要导入它。</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<p>我们通过操作符号变量来描述这些交互的操作单元。让我们用下面的方式创建一个：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">x</span> = tf.placeholder(tf.float32, [None, <span class="number">784</span>])</div></pre></td></tr></table></figure>
<p><code>x</code>不是一个特定的值。它是一个占位符(<code>placeholder</code>)，当我们要求TensorFlow运行一个计算时，我们将输入一个值。我们希望能够输入任意数量的MNIST图像，其中每个图像都被展开为784维向量。我们将其表示为float类型的2-D张量，形状为<code>[None, 784]</code>。（这里的<code>None</code>表示维度可以是任何长度。）</p>
<p>我们的模型还需要权重和偏差。当然我们可以把它们当做是另外的输入（使用占位符），但TensorFlow有一个更好的方法来表示它们：<code>Variable</code>。<code>Variable</code>代表一个可修改的张量，它存在于TensorFlow中用于描述交互性操作的图中。在计算过程中，它们可以被拿来使用甚至可以修改。对于机器学习应用，一般都会有模型参数，可以用Variable表示。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W</span> = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</div><div class="line"><span class="attr">b</span> = tf.Variable(tf.zeros([<span class="number">10</span>]))</div></pre></td></tr></table></figure>
<p>我们通过给与<code>tf.Variable</code>初始值来创建<code>Variable</code>:在这种情况下，我们将<code>W</code>和<code>b</code>初始化为全部为0的张量。因为我们要通过学习得到<code>W</code>和<code>b</code>，因此它们的初始值具体是什么并不重要。</p>
<p>注意，<code>W</code>的形状为<code>[784,10]</code>，因为我们想要用784维的图片向量乘以它以得到一个10维的证据值向量，其中每一位对应着不同数字类别。<code>b</code>的形状是<code>[10]</code>，所以我们可以直接把它加到输出上面。</p>
<p>现在，我们可以实现我们的模型啦。只需要一行代码！</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">y = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax</span>(tf.matmul(x, W) + b)</div></pre></td></tr></table></figure>
<p>首先，我们通过表达式<code>tf.matmul(x, W)</code>将<code>x</code>和<code>W</code>相乘。这对应于前面方程中的$Wx$，<code>x</code>是一个拥有多个输入的2D张量。紧接着，我们加上<code>b</code>，最后，代入到<code>tf.nn.softmax</code>中。</p>
<p>就是这样，在几行用来设置变量的代码之后，我们只需要一行代码就可以定义好我们的模型。这不仅仅是因为TensorFlow被设计为使<strong>softmax回归</strong>变得特别简单，它也用这种非常灵活的方式来描述其他各种数值计算，从机器学习模型对物理学模拟仿真模型。一旦被定义好之后，我们的模型就可以在不同的设备上运行：计算机的CPU，GPU，甚至是手机！</p>
<h2 id="训练">训练</h2><p>为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好的。实际上，在机器学习中，我们通常定义指标来表示一个模型是坏的，这个指标称为成本（cost）或损失（loss），然后尽量最小化这个指标。</p>
<p>一个非常常见的，非常好的用来衡量模型损失的函数称为“<strong>交叉熵(cross-entropy)</strong>”。交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：</p>
<p>$$<br>H_{y’}(y) = -\sum_i y’_i \log(y_i)<br>$$</p>
<p><strong>y</strong>是我们预测的概率分布,<strong>y’</strong>是实际的分布（我们输入的one-hot vector)。比较粗糙的理解是，交叉熵是用来衡量相对于真实值我们所给出的预测的低效性。有关交叉熵的更详细的讨论超出了本教程的范畴，但<a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="external">理解它的原理</a>很有必要。</p>
<p>为了计算交叉熵，我们首先需要添加一个新的占位符用于输入正确值：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">y_</span> = tf.placeholder(tf.float32, [None, <span class="number">10</span>])</div></pre></td></tr></table></figure>
<p>然后，我们可以实现交叉熵方法:$-\sum y’\log(y)$</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cross_entropy = <span class="keyword">tf</span>.reduce_mean(-<span class="keyword">tf</span>.reduce_sum(y_ * <span class="keyword">tf</span>.<span class="built_in">log</span>(<span class="keyword">y</span>), reduction_indices=[<span class="number">1</span>]))</div></pre></td></tr></table></figure>
<p>首先，<code>tf.log</code>计算了每个<code>y</code>的对数。接下来，我们将<code>y_</code>与相应的<code>tf.log(y)</code>的元素做乘法运算。然后，由于参数<code>reduction_indices=[1]</code>，<code>tf.reduce_sum</code>将<code>y</code>中的第二维中的元素相加求和。最后，通过<code>tf.reduce_mean</code>计算批次中所有示例的平均值。</p>
<p>注意，在源码中，我们不使用这些信息，因为它在数值上并不稳定。取而代之的是，我们将<code>tf.nn.softmax_cross_entropy_with_logits</code>用于非规范化的逻辑上（例如，我们对<code>tf.matmul(x, W) + b</code>使用<code>softmax_cross_entropy_with_logits</code>），因为这样在数值上更稳定方法，它在内部执行了softmax的计算。在你的代码中考虑使用<code>tf.nn.softmax_cross_entropy_with_logits</code>来代替之前的逻辑。</p>
<p>现在，我们知道了我们想要我们的模型做什么，使用TensorFlow来训练它也非常简单。因为TensorFlow知道用于计算的整个图（graph），它会自动地使用<strong><a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="external">反向传播算法</a></strong>来有效地确定你的变量是如何影响你想要最小化的那个成本值的。然后它可以应用您选择的优化算法修改变量和减少损失。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_step = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(<span class="number">0.5</span>).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<p>在这里，我们通过使用学习率为0.5的<a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="external">梯度下降算法</a>令TensorFlow最小化<code>cross_entropy</code>（交叉熵）。梯度下降是一个简单的程序，它的原理是每次向着减少损失的方向移动一小步，来最小化代价函数。但TensorFlow也提供了<a href="https://www.tensorflow.org/api_guides/python/train#optimizers" target="_blank" rel="external">很多其他的优化算法</a>，只需要简单的调整一行代码就可以随意切换。</p>
<p>TensorFlow在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元，用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。</p>
<p>我们现在可以在<code>InteractiveSession</code>中启动我们的模型：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">sess</span> = tf.InteractiveSession()</div></pre></td></tr></table></figure>
<p>我们首先要创建一个操作来初始化我们创建的变量：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">tf</span><span class="selector-class">.global_variables_initializer</span>()<span class="selector-class">.run</span>()</div></pre></td></tr></table></figure>
<p>让我们开始执行训练 - 我们将运行1000次训练步骤！</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  batch_xs, batch_ys = mnist<span class="selector-class">.train</span><span class="selector-class">.next_batch</span>(<span class="number">100</span>)</div><div class="line">  sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</div></pre></td></tr></table></figure>
<p>每循环一次，我们将从我们的训练集中得到一批100个随机数据点。然后我们用这些数据点作为参数替换之前的占位符来运行<code>train_step</code>。</p>
<p>使用小批随机数据称为<strong>随机训练(stochastic training)</strong> - 在这里更确切的说是随机梯度下降训练。理想情况下，我们希望将所有数据用于训练的每个步骤，因为这能给我们更好的训练结果，但很明显这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。</p>
<h2 id="评估我们的模型">评估我们的模型</h2><p>那么我们的模型表现如何呢？</p>
<p>首先，来让我们找出那些预测正确的标签。<code>tf.argmax</code>是一个很有用的方法，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。例如，<code>tf.argmax(y，1)</code>是我们的模型认为每个输入最可能的标签，而<code>tf.argmax(y_，1)</code>是正确的标签。我们可以用<code>tf.equal</code>来检查我们我预测值与真实值是否相符。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">correct_prediction = <span class="keyword">tf</span>.equal(<span class="keyword">tf</span>.argmax(<span class="keyword">y</span>,<span class="number">1</span>), <span class="keyword">tf</span>.argmax(y_,<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code>会变成<code>[1,0,1,1]</code>，取平均值后得到<code>0.75</code>.</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">accuracy = <span class="keyword">tf</span>.reduce_mean(<span class="keyword">tf</span>.cast(correct_prediction, <span class="keyword">tf</span>.float32))</div></pre></td></tr></table></figure>
<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>
<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(sess.run(accuracy, feed_dict=&#123;<span class="attribute">x</span>: mnist<span class="variable">.test</span><span class="variable">.images</span>, y_: mnist<span class="variable">.test</span><span class="variable">.labels</span>&#125;))</div></pre></td></tr></table></figure>
<p>结果大概维持在92%左右。</p>
<p>这种结果很好吗？其实并不是很好。其实，它相当差。这是因为我们使用的是一个非常简单的模型。我们可以通过做一些简单的修改，可以将正确率提高到97%。事实上，最优秀的模型可以达到超过99.7%的准确率！（想了解更多信息，可以看看这个关于各种模型的<a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" target="_blank" rel="external">性能对比列表</a>。)</p>
<p>比结果更重要的是，我们从这个模型中学习到的设计思想。不过，如果你仍然对这里的结果有点失望，可以查看<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">下一个教程</a>，在那里你可以学习如何用FensorFlow构建更加复杂的模型以获得更好的性能！</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/20/【Tensorflow r1.0 文档翻译】TensorFlow入门/" itemprop="url">
                【Tensorflow r1.0 文档翻译】TensorFlow入门
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-20T21:42:58+08:00" content="2017-02-20">
            2017-02-20
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/20/【Tensorflow r1.0 文档翻译】TensorFlow入门/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/20/【Tensorflow r1.0 文档翻译】TensorFlow入门/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="TensorFlow入门">TensorFlow入门</h2><p>这是一个TensorFlow的入门指南。在你使用这份指南之前，请先<a href="https://www.tensorflow.org/install/" target="_blank" rel="external">安装TensorFlow</a>。在充分的使用本指南之前，您应该了解以下内容：</p>
<ul>
<li>如何使用Python进行编程。</li>
<li>至少对矩阵有一些了解</li>
<li>最好是对<strong>机器学习</strong>有一点了解。但即使你对<strong>机器学习</strong>有一点了解、或者甚至完全不了解，那么你很有必要读一读这一篇指南了。</li>
</ul>
<p>TensorFlow提供了多种API。即使是最低版本的TensorFlow 核心 API，也为您提供了完整的编程控制。如果您是机器学习研究人员，或需要对模型进行精细控制的人，那么我们建议你使用TensorFlow 核心代码，否则我们建议您使用TensorFlow Core API。这些更高级的API通常比TensorFlow 核心代码更容易学习和使用。此外，较高级别的API使重复性任务更容易上手，并且在不同用户之间更一致。高级API（如<strong>tf.contrib.learn</strong>）可帮助您管理数据集、估计量、训练和推断。注意，在一些高级TensorFlow API 中，方法名称包含<code>contrib</code>的API表示仍在开发中。一些<code>contrib</code>方法可能会在随后的TensorFlow版本中发生改变或过时。</p>
<p>本指南从TensorFlow 核心教程开始。稍后，我们将演示如何在<code>tf.contrib.learn</code>中实现相同的模型。了解TensorFlow核心原则将会给你提供一个很棒的心理模型，这个模型是用于说明当您使用更紧凑的更高级别的API时，内部是如何工作的。</p>
<h2 id="Tensors（张量）">Tensors（张量）</h2><p>TensorFlow中的数据的中心单元是<strong>Tensors(张量)</strong>。tensor是由一组原始数据组成，这些原始数据是由一组任意数量维度的数组形成。一个tensor的<strong>rank</strong>是表示它尺寸的一个数值。下面是几个tensor的例子：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="number">3</span> # a rank <span class="number">0</span> tensor; this is a scalar with shape []</div><div class="line">[<span class="number">1.</span> ,<span class="number">2.</span>, <span class="number">3.</span>] # a rank <span class="number">1</span> tensor; this is a <span class="type">vector</span> with shape [<span class="number">3</span>]</div><div class="line">[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]] # a rank <span class="number">2</span> tensor; a matrix with shape [<span class="number">2</span>, <span class="number">3</span>]</div><div class="line">[[[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]], [[<span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>]]] # a rank <span class="number">3</span> tensor with shape [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure>
<h2 id="TensorFlow核心教程">TensorFlow核心教程</h2><h3 id="导入TensorFlow">导入TensorFlow</h3><p>TensorFlow程序的标准导入语句如下：</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<p>这样做可以使得Python能够正常访问TensorFlow的所有类、方法和符号。我们的大多数文档都假设你已经这样做了。</p>
<h3 id="用于计算的Graph（图）">用于计算的Graph（图）</h3><p>也许你会认为TensorFlow Core的程序包含下面两部分组成：</p>
<ul>
<li>1.构建<strong>computational graph（用于计算的图）</strong></li>
<li>2.运行<strong>computational graph（用于计算的图）</strong></li>
</ul>
<p>一个<strong>computational graph（用于计算的图）</strong>是一系列排列在graph的节点上的TensorFlow操作单元。让我们来构建一个简单的<strong>computational graph</strong>。每个节点接受0个或多个tensor作为输入，并且产生一个tensor作为输出。<strong>常量类型</strong>是节点的一种类型。正如所有的TensorFlow常量一样，它是不接收输入的，并且输出一个它内部存储的值。我们可以按照下面的方式来创建两个浮点Tensor节点<code>node1</code>和<code>node2</code>：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">node1 = <span class="keyword">tf</span>.constant(<span class="number">3.0</span>, <span class="keyword">tf</span>.float32)</div><div class="line">node2 = <span class="keyword">tf</span>.constant(<span class="number">4.0</span>) # also <span class="keyword">tf</span>.float32 implicitly</div><div class="line"><span class="keyword">print</span>(node1, node2)</div></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Tensor(<span class="string">"Const:0"</span>, shape=(), dtype=float32) Tensor(<span class="string">"Const_1:0"</span>, shape=(), dtype=float32)</div></pre></td></tr></table></figure>
<p>你会注意到，打印节点并不会如你所想的输出<code>3.0</code>和<code>4.0</code>。相反，这些节点会在计算时分别产生<code>3.0</code>和<code>4.0</code>。为了实际评估这些节点，我们必须以一个 <strong>session（会话）</strong> 来运行 <strong>computational graph</strong>。<strong>session（会话）</strong> 封装了TensorFlow运行时的控件和状态。</p>
<p>下面的代码创建了一个<code>Session</code>对象，并且执行了它的<code>run</code>方法来运行包含了<code>node1</code>和<code>node2</code>的<strong>computational graph</strong>的计算结果。通过在<strong>session</strong>中运行<strong>computational graph</strong>的代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(sess.run([node1, node2])</span></span>)</div></pre></td></tr></table></figure>
<p>我们看到了我们期望看到的<code>3.0</code>和<code>4.0</code>的输出:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">3.0</span>, <span class="number">4.0</span>]</div></pre></td></tr></table></figure>
<p>我们可以通过将<code>Tensor</code>节点与操作节点（操作也是一种节点）组合起来的方式来构建更复杂的计算。例如，我们可以将两个常量节点执行加法操作，并且产生一个新的graph，代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">node3 = tf.add(node1, node2)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"node3: "</span>, node3)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"sess.run(node3): "</span>,sess.run(node3)</span></span>)</div></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node3:  Tensor(<span class="string">"Add_2:0"</span>, shape=(), dtype=float32)</div><div class="line">sess.run(node3):  7.0</div></pre></td></tr></table></figure>
<p>TensorFlow提供了一个叫做<strong>TensorBoard</strong>的很实用的程序，它可以将computational graph可视化的展示出来。下面是通过TensorBoard来可视化一个graph的效果：</p>
<p><img src="/img/17_02_20/001.png" alt=""></p>
<p>由于我们用到的是常量，因此这个图看起来并不是特别有趣，因为它总是产生一个恒定的结果。graph可以被参数化，并且通过<strong>placeholders（占位符）</strong>来接受外部的输入。<strong>placeholders（占位符）</strong>表示对稍后所提供的值的一个承诺。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="attr">a</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">b</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">adder_node</span> = a + b  # + 是tf.add(a, b)的缩写形式</div></pre></td></tr></table></figure>
<p>上面三行的表达形式看起来有点像一个方法，或lambda表达式：其中我们定义两个输入参数（<code>a</code>和<code>b</code>），然后对它们执行一个操作。我们可以通过多个输入来计算这个graph的执行结果，其中我们的输入是通过<code>feed_dict</code>参数来指定对这些<strong>placeholders（占位符）</strong>提供具体值的<code>Tensors</code>的输入的：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">adder_node</span>, &#123;<span class="attribute">a</span>: <span class="number">3</span>, b:<span class="number">4.5</span>&#125;))</div><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">adder_node</span>, &#123;<span class="attribute">a</span>: [<span class="number">1</span>,<span class="number">3</span>], b: [<span class="number">2</span>, <span class="number">4</span>]&#125;))</div></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">7.5</span></div><div class="line">[ <span class="number">3.</span>  <span class="number">7.</span>]</div></pre></td></tr></table></figure>
<p>在TensorBoard中，graph看起来是这个样子：</p>
<p><img src="/img/17_02_20/002.png" alt=""></p>
<p>我们可以通过添加其他操作，来让我们的<strong>computational graph</strong>看起来更复杂。例如：</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">add_and_triple </span>= <span class="keyword">adder_node </span>* <span class="number">3</span>.</div><div class="line"><span class="symbol">print</span>(sess.run(<span class="keyword">add_and_triple, </span>&#123;a: <span class="number">3</span>, <span class="keyword">b:4.5&#125;))</span></div></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">22<span class="selector-class">.5</span></div></pre></td></tr></table></figure>
<p>上面的computational graph在TensorBoard中看起来是这样的：</p>
<p><img src="/img/17_02_20/003.png" alt=""></p>
<p>在机器学习中，我们通常需要一个可以接受任意输入的模型，例如上面的模型。为了使模型可训练，我们需要能够修改<code>graph</code>以获得具有相同输入的新输出。<strong>Variables（变量）</strong>允许我们向graph中添加可训练的参数。它们由一个类型和初始值组成：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">W</span> = tf.Variable([.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="attr">b</span> = tf.Variable([-.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="attr">x</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">linear_model</span> = W * x + b</div></pre></td></tr></table></figure>
<p>当调用<code>tf.constant</code>时，常量被初始化，它们的值永远不会改变。相比之下，变量<code>tf.Variable</code>在调用时不会被初始化。要初始化TensorFlow程序中的所有变量，必须显式调用特殊的初始化操作，如下所示：</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">init</span> = tf.global_variables_initializer()</div><div class="line">sess.run(<span class="keyword">init</span>)</div></pre></td></tr></table></figure>
<p><code>init</code>是TensorFlow的sub-graph的一个重要的操作，它用于初始化所有的全局变量。在这里直到我们调用<code>sess.run</code>之前，变量是未初始化的。</p>
<p>由于<code>x</code>是一个占位符，因此我们可以同时计算<code>linear_model</code>几个值，如下所示：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">linear_model</span>, &#123;<span class="attribute">x</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]&#125;))</div></pre></td></tr></table></figure>
<p>产生如下输出：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[ <span class="number">0.</span>          <span class="number">0.30000001</span>  <span class="number">0.60000002</span>  <span class="number">0.90000004</span>]</div></pre></td></tr></table></figure>
<p>我们创建了一个模型，但目前我们还不知道它是好是坏。为了评估训练数据的模型，我们需要一个<strong>loss function（损失函数）</strong>，我们可以用一个<code>y</code>占位符来提供所需的值。</p>
<p>损失函数会计算出当前训练出的模型和所提供的数据之间的距离。我们将使用用于线性回归的标准损失模型，其原理是将当前模型和提供的数据之间的增量的平方求和。<code>linear_model - y</code>创建一个向量，其中每个元素是相应的样本的误差增量。我们称之为<code>tf.square</code>平方误差。然后，我们使用<code>tf.reduce_sum</code>将所有平方误差求和，以创建一个单一的标量，用于提取出表示所有样本的总误差值：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">y</span> = <span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.float32)</div><div class="line">squared_deltas = <span class="keyword">tf</span>.square(linear_model - <span class="keyword">y</span>)</div><div class="line">loss = <span class="keyword">tf</span>.reduce_sum(squared_deltas)</div><div class="line"><span class="keyword">print</span>(sess.run(loss, &#123;<span class="keyword">x</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], <span class="keyword">y</span>:[<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">3</span>]&#125;))</div></pre></td></tr></table></figure>
<p>输出损失值：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">23<span class="selector-class">.66</span></div></pre></td></tr></table></figure>
<p>我们可以通过手动的将<code>W</code>和<code>b</code>的值重新赋值为<code>-1</code>和<code>1</code>的方式来提高我们的算法的效果。变量可以初始化后将数据提供给<code>tf.Variable</code>对象，也可以使用像<code>tf.assign</code>这样的操作来更改。例如，<code>W=-1</code>和<code>b=1</code>是我们的模型中的最佳参数。因此我们可以更改<code>W</code>和<code>b</code>的值：</p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fixW = tf.assign(W, <span class="string">[-1.]</span>)</div><div class="line">fixb = tf.assign(b, <span class="string">[1.]</span>)</div><div class="line">sess.run(<span class="string">[fixW, fixb]</span>)</div><div class="line">print(sess.run(loss, &#123;x:<span class="string">[1,2,3,4]</span>, y:<span class="string">[0,-1,-2,-3]</span>&#125;))</div></pre></td></tr></table></figure>
<p>最终输出结果的损失是0：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0<span class="selector-class">.0</span></div></pre></td></tr></table></figure>
<p>我们猜到了“最完美”的参数<code>W</code>和<code>b</code>的值，但机器学习的目标是<strong>自动</strong>找到正确的模型参数。我们将在下一节中说明如何完成这一任务。</p>
<h2 id="tf-train_API">tf.train API</h2><p>机器学习的完整讨论超出了本教程的范围。然而，TensorFlow提供了缓慢地改变每个变量以便<strong>最小化损失函数</strong>的<strong>optimizers（优化器）</strong>。其中最简单的优化器是<strong>gradient descent（梯度下降）</strong>。其原理是根据相对于该变量的损失导数的大小修改每个变量的值。一般来说，人工计算导数是繁琐的并且容易出错。因此，TensorFlow可以使用<code>tf.gradients</code>函数，来自动的产生当前所给模型描述的导数。为了简化操作，优化器通常会自动地为您执行此操作。例如：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attr">optimizer</span> = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line"><span class="attr">train</span> = optimizer.minimize(loss)</div></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">init</span>) # 将值重置为不正确的默认值</div><div class="line"><span class="selector-tag">for</span> <span class="selector-tag">i</span> <span class="selector-tag">in</span> <span class="selector-tag">range</span>(1000):</div><div class="line">  <span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-tag">train</span>, &#123;<span class="attribute">x</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">3</span>]&#125;)</div><div class="line"></div><div class="line"><span class="selector-tag">print</span>(<span class="selector-tag">sess</span><span class="selector-class">.run</span>(<span class="selector-attr">[W, b]</span>))</div></pre></td></tr></table></figure>
<p>最终训练得出的模型参数：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[array([<span class="number">-0.9999969</span>], dtype=float32), array([ <span class="number">0.99999082</span>],</div><div class="line"> dtype=float32)]</div></pre></td></tr></table></figure>
<p>现在我们已经完成了实际的机器学习！完成这个简单的线性回归不需要太多的TensorFlow核心代码，但是更复杂的学习模型和方法通常需要更多的代码。因此，TensorFlow为通用模式、结构和功能提供了一套更高级别的抽象实现。我们将在下一节中学习如何使用这些抽象实现。</p>
<h3 id="完整的程序">完整的程序</h3><p>完成的可训练线性回归模型程序如下所示：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">import</span> numpy as np</div><div class="line"><span class="built_in">import</span> tensorflow as tf</div><div class="line"></div><div class="line"><span class="comment"># Model parameters</span></div><div class="line"><span class="attr">W</span> = tf.Variable([.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="attr">b</span> = tf.Variable([-.<span class="number">3</span>], tf.float32)</div><div class="line"><span class="comment"># Model input and output</span></div><div class="line"><span class="attr">x</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="attr">linear_model</span> = W * x + b</div><div class="line"><span class="attr">y</span> = tf.placeholder(tf.float32)</div><div class="line"><span class="comment"># loss</span></div><div class="line"><span class="attr">loss</span> = tf.reduce_sum(tf.square(linear_model - y)) <span class="comment"># sum of the squares</span></div><div class="line"><span class="comment"># optimizer</span></div><div class="line"><span class="attr">optimizer</span> = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line"><span class="attr">train</span> = optimizer.minimize(loss)</div><div class="line"><span class="comment"># training data</span></div><div class="line"><span class="attr">x_train</span> = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div><div class="line"><span class="attr">y_train</span> = [<span class="number">0</span>,-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">3</span>]</div><div class="line"><span class="comment"># training loop</span></div><div class="line"><span class="attr">init</span> = tf.global_variables_initializer()</div><div class="line"><span class="attr">sess</span> = tf.Session()</div><div class="line">sess.run(init) <span class="comment"># reset values to wrong</span></div><div class="line">for i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">  sess.run(train, &#123;x:x_train, y:y_train&#125;)</div><div class="line"></div><div class="line"><span class="comment"># evaluate training accuracy</span></div><div class="line">curr_W, curr_b, <span class="attr">curr_loss</span>  = sess.run([W, b, loss], &#123;x:x_train, y:y_train&#125;)</div><div class="line">print(<span class="string">"W: %s b: %s loss: %s"</span>%(curr_W, curr_b, curr_loss))</div></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">W</span>: <span class="selector-attr">[-0.9999969]</span> <span class="selector-tag">b</span>: <span class="selector-attr">[ 0.99999082]</span> <span class="selector-tag">loss</span>: 5<span class="selector-class">.69997e-11</span></div></pre></td></tr></table></figure>
<p>这个更复杂的程序也可以在TensorBoard中可视化：</p>
<p><img src="/img/17_02_20/004.png" alt=""></p>
<h2 id="tf-contrib-learn">tf.contrib.learn</h2><p><code>tf.contrib.learn</code>是一个高级别的TensorFlow库，它简化了机器学习的机制，包括：</p>
<ul>
<li>运行训练循环</li>
<li>运行评估循环</li>
<li>管理数据集</li>
<li>管理数据导入</li>
</ul>
<p><code>tf.contrib.learn</code>定义了许多常见的模型。</p>
<h3 id="基本用法">基本用法</h3><p>请注意，线性回归在使用<code>tf.contrib.learn</code>的情况下变得更简单了：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">import</span> tensorflow as tf</div><div class="line"><span class="comment"># NumPy is often used to load, manipulate and preprocess data.</span></div><div class="line"><span class="built_in">import</span> numpy as np</div><div class="line"></div><div class="line"><span class="comment"># Declare list of features. We only have one real-valued feature. There are many</span></div><div class="line"><span class="comment"># other types of columns that are more complicated and useful.</span></div><div class="line"><span class="attr">features</span> = [tf.contrib.layers.real_valued_column(<span class="string">"x"</span>, <span class="attr">dimension=1)]</span></div><div class="line"></div><div class="line"><span class="comment"># An estimator is the front end to invoke training (fitting) and evaluation</span></div><div class="line"><span class="comment"># (inference). There are many predefined types like linear regression,</span></div><div class="line"><span class="comment"># logistic regression, linear classification, logistic classification, and</span></div><div class="line"><span class="comment"># many neural network classifiers and regressors. The following code</span></div><div class="line"><span class="comment"># provides an estimator that does linear regression.</span></div><div class="line"><span class="attr">estimator</span> = tf.contrib.learn.LinearRegressor(<span class="attr">feature_columns=features)</span></div><div class="line"></div><div class="line"><span class="comment"># TensorFlow provides many helper methods to read and set up data sets.</span></div><div class="line"><span class="comment"># Here we use `numpy_input_fn`. We have to tell the function how many batches</span></div><div class="line"><span class="comment"># of data (num_epochs) we want and how big each batch should be.</span></div><div class="line"><span class="attr">x</span> = np.array([<span class="number">1</span>., <span class="number">2</span>., <span class="number">3</span>., <span class="number">4</span>.])</div><div class="line"><span class="attr">y</span> = np.array([<span class="number">0</span>., -<span class="number">1</span>., -<span class="number">2</span>., -<span class="number">3</span>.])</div><div class="line"><span class="attr">input_fn</span> = tf.contrib.learn.io.numpy_input_fn(&#123;<span class="string">"x"</span>:x&#125;, y, <span class="attr">batch_size=4,</span></div><div class="line">                                              <span class="attr">num_epochs=1000)</span></div><div class="line"></div><div class="line"><span class="comment"># We can invoke 1000 training steps by invoking the `fit` method and passing the</span></div><div class="line"><span class="comment"># training data set.</span></div><div class="line">estimator.fit(<span class="attr">input_fn=input_fn,</span> <span class="attr">steps=1000)</span></div><div class="line"></div><div class="line"><span class="comment"># Here we evaluate how well our model did. In a real example, we would want</span></div><div class="line"><span class="comment"># to use a separate validation and testing data set to avoid overfitting.</span></div><div class="line">estimator.evaluate(<span class="attr">input_fn=input_fn)</span></div></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'global_step'</span>: <span class="number">1000</span>, <span class="string">'loss'</span>: <span class="number">1.9650059</span>e-<span class="number">11</span>&#125;</div></pre></td></tr></table></figure>
<h3 id="定制模型">定制模型</h3><p><code>tf.contrib.learn</code>不会将你锁定在预定义模型中。假设我们想创建一个未内置到TensorFlow中的自定义模型。我们仍然可以保留<code>tf.contrib.learn</code>的数据集、馈送、训练等的高级抽象。为了说明，我们将演示如何使用我们的较低级别TensorFlow API的知识来实现​​我们自己的等效模型到<code>LinearRegressor</code>。</p>
<p>要定义与<code>tf.contrib.learn</code>一起使用的自定义模型，我们需要使用<code>tf.contrib.learn.Estimator</code>。 <code>tf.contrib.learn.LinearRegressor</code>实际上是<code>tf.contrib.learn.Estimator</code>的子类。替代子类<code>Estimator</code>，我们只是提供<code>Estimator</code>一个<code>model_fn</code>函数，用于告诉<code>tf.contrib.learn</code>如何评估预测、训练步骤和损失。代码如下：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">import</span> numpy as np</div><div class="line"><span class="built_in">import</span> tensorflow as tf</div><div class="line"><span class="comment"># Declare list of features, we only have one real-valued feature</span></div><div class="line">def model(features, labels, mode):</div><div class="line">  <span class="comment"># Build a linear model and predict values</span></div><div class="line">  <span class="attr">W</span> = tf.get_variable(<span class="string">"W"</span>, [<span class="number">1</span>], <span class="attr">dtype=tf.float64)</span></div><div class="line">  <span class="attr">b</span> = tf.get_variable(<span class="string">"b"</span>, [<span class="number">1</span>], <span class="attr">dtype=tf.float64)</span></div><div class="line">  <span class="attr">y</span> = W*features['x'] + b</div><div class="line">  <span class="comment"># Loss sub-graph</span></div><div class="line">  <span class="attr">loss</span> = tf.reduce_sum(tf.square(y - labels))</div><div class="line">  <span class="comment"># Training sub-graph</span></div><div class="line">  <span class="attr">global_step</span> = tf.train.get_global_step()</div><div class="line">  <span class="attr">optimizer</span> = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</div><div class="line">  <span class="attr">train</span> = tf.group(optimizer.minimize(loss),</div><div class="line">                   tf.assign_add(global_step, <span class="number">1</span>))</div><div class="line">  <span class="comment"># ModelFnOps connects subgraphs we built to the</span></div><div class="line">  <span class="comment"># appropriate functionality.</span></div><div class="line">  return tf.contrib.learn.ModelFnOps(</div><div class="line">      <span class="attr">mode=mode,</span> <span class="attr">predictions=y,</span></div><div class="line">      <span class="attr">loss=</span> loss,</div><div class="line">      <span class="attr">train_op=train)</span></div><div class="line"></div><div class="line"><span class="attr">estimator</span> = tf.contrib.learn.Estimator(<span class="attr">model_fn=model)</span></div><div class="line"><span class="comment"># define our data set</span></div><div class="line"><span class="attr">x=np.array([1.,</span> <span class="number">2</span>., <span class="number">3</span>., <span class="number">4</span>.])</div><div class="line"><span class="attr">y=np.array([0.,</span> -<span class="number">1</span>., -<span class="number">2</span>., -<span class="number">3</span>.])</div><div class="line"><span class="attr">input_fn</span> = tf.contrib.learn.io.numpy_input_fn(&#123;<span class="string">"x"</span>: x&#125;, y, <span class="number">4</span>, <span class="attr">num_epochs=1000)</span></div><div class="line"></div><div class="line"><span class="comment"># train</span></div><div class="line">estimator.fit(<span class="attr">input_fn=input_fn,</span> <span class="attr">steps=1000)</span></div><div class="line"><span class="comment"># evaluate our model</span></div><div class="line">print(estimator.evaluate(<span class="attr">input_fn=input_fn,</span> <span class="attr">steps=10))</span></div></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'loss'</span>: <span class="number">5.9819476</span>e-<span class="number">11</span>, <span class="string">'global_step'</span>: <span class="number">1000</span>&#125;</div></pre></td></tr></table></figure>
<p>注意，自定义<code>model()</code>函数的内容与低版本API的手册中的模型训练循环非常相似。</p>
<h2 id="下一步">下一步</h2><p>现在你了解到了TensorFlow的基本运作的知识。我们还有几个教程，您可以查看以了解更多。如果你是机器学习的初学者，请参阅<a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">【深度学习的HelloWorld – MNIST手写数字识别】</a>，否则请参阅<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">【深入MNIST – 专家级】</a>。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/20/【Tensorflow r1.0 文档翻译】入门教程/" itemprop="url">
                【Tensorflow r1.0 文档翻译】入门教程
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-20T21:19:58+08:00" content="2017-02-20">
            2017-02-20
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              
                ， 
              

            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/Tensorflow/" itemprop="url" rel="index">
                  <span itemprop="name">Tensorflow</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/20/【Tensorflow r1.0 文档翻译】入门教程/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/20/【Tensorflow r1.0 文档翻译】入门教程/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><ul>
<li><a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门/">【TensorFlow入门】</a></li>
<li><a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">【机器学习的HelloWorld – MNIST手写数字识别】</a></li>
<li><a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">【深入MNIST – 专家级】</a></li>
<li><a href="/2017/03/03/【Tensorflow%20r1.0%20文档翻译】Tensorflow原理导论/">【TensorFlow原理导论】</a></li>
<li><a href="/2017/03/05/【Tensorflow%20r1.0%20文档翻译】【tf.contrib.learn快速入门】/">【tf.contrib.learn快速入门】</a></li>
<li><a href="/2017/03/06/【Tensorflow%20r1.0%20文档翻译】通过tf.contrib.learn来构建输入函数/">【通过tf.contrib.learn来构建输入函数】</a></li>
<li><a href="/2017/03/07/【Tensorflow%20r1.0%20文档翻译】TensorBoard-可视化学习/">【TensorBoard:可视化学习】</a></li>
<li><a href="/2017/03/07/【Tensorflow%20r1.0%20文档翻译】TensorBoard-嵌入可视化/">【TensorBoard:嵌入可视化】</a></li>
<li><a href="/2017/03/07/【Tensorflow%20r1.0%20文档翻译】TensorBoard-图的可视化/">【TensorBoard:图的可视化】</a></li>
<li><a href="/2017/03/09/【Tensorflow%20r1.0%20文档翻译】使用tf.contrib.learn记录和监视的基本知识/">【使用tf.contrib.learn记录和监视的基本知识】</a></li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/09/斯坦福机器学习课程 第七周 (2)核函数/" itemprop="url">
                斯坦福机器学习课程 第七周 (2)核函数
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-09T21:07:58+08:00" content="2017-02-09">
            2017-02-09
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/09/斯坦福机器学习课程 第七周 (2)核函数/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/09/斯坦福机器学习课程 第七周 (2)核函数/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="核函数_I">核函数 I</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/YOMHn/kernels-i" target="_blank" rel="external">视频地址</a></p>
<p>在本节，我将对支持向量机算法做一些改变，以构造复杂的非线性分类器。我们用<strong>“kernels(核函数)”</strong>来达到此目的。</p>
<p>我们来看看<strong>核函数</strong>是什么，以及如何使用。</p>
<p>如果你有一个像这个样的训练集：</p>
<p><img src="/img/17_02_09/001.png" alt=""></p>
<p>然后你希望拟合一个非线性的判别边界来区别正负样本，那么你的判别边界可能是这样的：</p>
<p><img src="/img/17_02_09/002.png" alt=""></p>
<p>当我们这么做的时候，其实这个决策边界是由类似于下面这种多项式构成的：</p>
<ul>
<li><p>如果$<br>\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1x_2+\theta_4x_1^2 + \theta_5x_2^2 + … \ge 0<br>$，则预测$h_{\theta}(x)=1$；</p>
</li>
<li><p>如果$<br>\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1x_2+\theta_4x_1^2 + \theta_5x_2^2 + … \lt 0<br>$，则预测$h_{\theta}(x)=0$；</p>
</li>
</ul>
<p>如果我们把假设函数改写成下面这种形式：</p>
<p>$$<br>\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\theta_4f_4+\theta_5f_5+…<br>$$</p>
<p>因此有：</p>
<p>$$<br>f_1=x_1<br>$$</p>
<p>$$<br>f_2=x_2<br>$$</p>
<p>$$<br>f_3=x_1x_2<br>$$</p>
<p>$$<br>f_4=x_1^2<br>$$</p>
<p>$$<br>f_5=x_2^2<br>$$</p>
<p>$$<br>…<br>$$</p>
<p>以此类推，可以依次加入这些高阶项，但我们其实并不知道这些高阶项是不是我们真正需要的。我们之前谈到计算机视觉的时候，提到过在这里的输入是一个有很多像素的图像，我们看到如果用高阶项作为特征变量，运算量将是非常大的，因为有太多的高阶项需要被计算。</p>
<p>因此，我们是否有不同的选择，或者是更好的选择来构造特征变量，以用来嵌入到假设函数中呢？</p>
<h3 id="用核函数构造新特征">用核函数构造新特征</h3><p>事实上，这里有一个可以构造新特征$f_1$、$f_2$、$f_3$的方法。</p>
<p>首先我们定义三个特征变量(但是对于实际问题而言，我们可以定义非常多的特征变量）：</p>
<p><img src="/img/17_02_09/003.png" width="300" height="200" align="center"></p>
<p>将这三个点标记为$l^{(1)}$、$l^{(2)}$、$l^{(3)}$，接下来我要做的是定义新的特征变量：</p>
<p>$$<br>f_1=similarity(x,l^{(1)})<br>$$</p>
<p>这里$similarite(x,l^{(1)})$是一种相似度的度量，度量样本$x$与第一个标记$l^{(1)}$的相似度。</p>
<p>这个度量相似度的公司是这样的：</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=similarity(x,l^{(1)})<br>\\<br>&amp;=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<blockquote>
<p>$exp$是自然常数$e$为底的指数函数。</p>
</blockquote>
<p>不知道你之前是否看了上一个选修课程的视频，$||w||$是表示向量$w$的长度。因此这里的$||x-l^{(1)}||$的意思就是就是向量的欧式距离。</p>
<p>因此，我们可以依次写出$f_1$、$f_2$、$f_3$:</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=similarity(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_2&amp;=similarity(x,l^{(2)})<br>=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_3&amp;=similarity(x,l^{(3)})<br>=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>这里的$similarite(x,l)$函数，就被称为<strong>核函数(Kernels)</strong>。在这里，我们的例子中所说的<strong>核函数</strong>，实际上是<strong>高斯核函数</strong>，在后面我们还会见到不同的<strong>核函数</strong>。</p>
<p>核函数我们通常不写作$similarity(x,l^{(i)})$，而是写作：</p>
<p>$$<br>k(x,l^{(i)})<br>$$</p>
<h3 id="核函数可以做什么？">核函数可以做什么？</h3><p>我们来看看核函数到底可以做什么？</p>
<p>首先让我们来看看第一个标记：</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=similarity(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>=exp(-\frac{\sum_{j=1}^{n}(x_j-l_j^{(1)})^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$l^{(1)}$是我之前在图中选取的几个点之中的一个，上面是$x$和$l^{(1)}$之间的核函数。</p>
<p>其中$||x-l^{(1)}||^2$这一项可以表示成各个$x$向量到$l$向量的距离求和的形式：$\sum_{j=1}^{n}(x_j-l_j^{(1)})^2$。（这里我们依然忽略了截距的影响，即令$x_0=1$）。</p>
<p>假设，如果$x\approx l^{(1)}$，即$x$与其中一个标记点非常接近，那么这个欧氏距离$||x-l^{(1)}||$就会接近0，因此：</p>
<p>$$<br>f_1<br>\approx<br>exp(-\frac{0^2}{2σ^2})<br>\approx1<br>$$</p>
<p>相反的，如果$x$离$l^{(1)}$很远，那么会有：</p>
<p>$$<br>f_1<br>\approx<br>exp(-\frac{(large\ number)^2}{2σ^2})<br>\approx0<br>$$</p>
<p><strong>这些特征变量的作用是度量$x$到标记$l^{(1)}$的相似度的，并且如果$x$离$l$非常接近，那么特征变量$f$就接近1；如果$x$离标记$l^{(1)}$非常远，那么特征变量$f$就接近于0。</strong></p>
<p>之前我绘制的三个标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$每一个标记点会定义一个新的特征变量：</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=k(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_2&amp;=k(x,l^{(2)})<br>=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>$$<br>\begin{align*}<br>f_3&amp;=k(x,l^{(3)})<br>=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p><strong>也就是说，给出一个训练样本$x$，我们就能基于我们之前给出的标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$来计算出三个新的特征变量$f_1$、$f_2$、$f_3$。</strong></p>
<h3 id="深入理解核函数">深入理解核函数</h3><p>接下来让我们通过画一些图来更好地理解<strong>核函数</strong>是什么样的。</p>
<h4 id="$x$对$f$的值的影响">$x$对$f$的值的影响</h4><p>看下面这个例子，假设我们有两个特征$x_1$和$x_2$，假设我们第一个标记点是$l^{(1)}$：</p>
<p>$$<br>l^{(1)}=<br>\begin{bmatrix}<br>   3 \\<br>    5<br> \end{bmatrix}<br>$$</p>
<p>假设:</p>
<p>$$<br>σ^2=1<br>$$</p>
<p>如果我画出:</p>
<p>$$<br>\begin{align*}<br>f_1&amp;=k(x,l^{(1)})<br>=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})<br>\end{align*}<br>$$</p>
<p>结果就是这样的：</p>
<table>
<thead>
<tr>
<th style="text-align:center">3D曲面图</th>
<th style="text-align:center">等高线图</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/004.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/005.png" width="300" height="200" align="center"></td>
</tr>
</tbody>
</table>
<p>其中左侧的图纵轴是$f_1$，水平方向的两个轴分别是$x_1$和$x_2$。右侧的图是左侧图的等高线图。</p>
<p>你会发现，当$x=(3,5)$的时候，$f_1=1$，因为它在最大值的位置上。所以如果$x$往旁边移动，离这个点越远，那么从图中可以看到$f_1$的值就越接近0。</p>
<h4 id="$σ^2$对$f$的值的影响">$σ^2$对$f$的值的影响</h4><p>在这里，要提到的另一点就是$σ^2$对结果的影响。$σ^2$是<strong>高斯核函数</strong>的参数，改变它会得到略微不同的结果。</p>
<p>可以对比$σ^2=1$、$σ^2=0.5$、$σ^2=3$的情况：</p>
<table>
<thead>
<tr>
<th style="text-align:center">$σ^2=1$</th>
<th style="text-align:center">$σ^2=0.5$</th>
<th style="text-align:center">$σ^2=3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/004.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/006.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/008.png" width="300" height="200" align="center"></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/005.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/007.png" width="300" height="200" align="center"></td>
<td style="text-align:center"><img src="/img/17_02_09/009.png" width="300" height="200" align="center"></td>
</tr>
</tbody>
</table>
<p>你会发现，函数的形状还是相似的，只是$σ^2=0.5$相较于$σ^2=1$凸起的宽度变窄了，等值线图也收缩了一些；$σ^2=3$相较于$σ^2=1$凸起的宽度变宽了，等值线也扩张了一些。</p>
<p>所以，如果我们将$σ^2$设为0.5时，特征变量$f_1$下降到0的速度也会相应变快；如果我们将$σ^2$设为3时，特征变量$f_1$下降到0的速度也会相应变慢。</p>
<h3 id="获取预测函数">获取预测函数</h3><p>讲完了特征变量的定义，我们来看看我们能得到什么样的预测函数。</p>
<p>给定一个训练样本$x$，我们要计算出三个特征变量$f_1$ $f_2$ $f_3$</p>
<p><img src="/img/17_02_09/010.png" alt=""></p>
<p>并且如果$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3 \ge 0$，则预测函数的预测值为1，即$y=1$。</p>
<p>对于这个特定的例子而言，假设我们已经找到了一个学习算法，并且假设我已经得到了这些参数的值，比如如果：</p>
<p>$$<br>\theta_0=-0.5 \\<br>\theta_1=1 \\<br>\theta_2=1 \\<br>\theta_3=0<br>$$</p>
<p>如果我们现在有一个训练样本$x$：</p>
<p><img src="/img/17_02_09/011.png" alt=""></p>
<p>我想知道预测函数会给出什么结果。</p>
<p>看看这个公式：</p>
<p>$$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3 \ge 0$$</p>
<p>因为我的训练样本$x$接近于$l^{(1)}$，那么$f_1$就接近于1：</p>
<p>$$<br>f_1\approx1<br>$$</p>
<p>又因为训练样本$x$离$l^{(2)}$ $l^{(3)}$都很远，所以$f_2$就接近于0，$f_3$也接近于0：</p>
<p>$$<br>f_2\approx0 \\<br>f_3\approx0<br>$$</p>
<p>所以带入上面的公式可以得到：</p>
<p>$$<br>\begin{align*}<br>\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3<br>&amp;=\theta_0 + \theta_1·1 + \theta_2·0 +  \theta_3·0<br>\\<br>&amp;=-0.5+1<br>\\<br>&amp;=0.5\\<br>\ge0<br>\end{align*}<br>$$</p>
<p>因此对于这一点，我们预测的结果是$y=1$。</p>
<hr>
<p>现在我们选择另一个不同的点，假设我选择了另一个点：</p>
<p><img src="/img/17_02_09/012.png" alt=""></p>
<p>如果将这个训练样本$x$带入之前相同的计算，你发现$f_1$ $f_2$ $f_3$都接近于0。</p>
<p>因此，我们得到$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3=-0.5$，因为$θ_0=-0.5$，并且$f_1$ $f_2$ $f_3$都为0，因此最后结果是-0.5，小于0。因此这个点，我们预测的y值是0。</p>
<hr>
<p>类似的，如果你自己来对大量的点进行这样相应的处理，你应该可以确定如果你有一个非常接近于$l^{(2)}$的训练样本，那么通过这个点预测的y值也是1。</p>
<p>实际上，你最后得到的结果是：<strong>对于接近$l^{(1)}$和$l^{(2)}$的点，我们的预测值是1，对于远离$l^{(1)}$和$l^{(2)}$的点，我们最后预测的结果是等于0的</strong>。</p>
<p>我们最后会得到这个预测函数的判别边界看起来是类似这样的结果：</p>
<p><img src="/img/17_02_09/013.png" width="300" height="200" align="center"></p>
<p>在这个红色的判别边界里面，预测的y值等于1；在这外面预测的y值等于0。</p>
<p>因此这就是一个我们如何通过标记点，以及核函数，来训练出非常复杂的非线性判别边界的方法。</p>
<p>这就是核函数这部分的概念，以及我们如何在支持向量机中使用它们。我们通过标记点和相似性函数来定义新的特征变量从而训练复杂的非线性分类器。</p>
<p>目前还有一些问题我们并没有做出回答，其中一个是<strong>我们如何得到这些标记点</strong>；另一个是<strong>其他的相似度方程（核函数）是什么样的</strong>，如果有其他的话，我们能够用其他的相似度方程来代替我们所讲的这个高斯核函数吗？在下一个视频中我们会回答这些问题，然后把所有东西都整合到一起来看看支持向量机如何通过核函数的定义 有效地学习复杂非线性函数。</p>
<h2 id="核函数_II">核函数 II</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/hxdcH/kernels-ii" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>在上一节视频里，我们讨论了<strong>核函数</strong>这个想法以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。例如，<strong>怎么处理支持向量机中的偏差方差折中</strong>。</p>
</blockquote>
<h3 id="如何选取标记点(landmark)">如何选取标记点(landmark)</h3><p>在上一节课中，我谈到过选择标记点，例如$l^{(1)}$ $l^{(2)}$ $l^{(3)}$ 这些点使我们能够定义<strong>相似度函数</strong>，也称之为<strong>核函数</strong>。在这个例子里，我们的相似度函数为<strong>高斯核函数</strong>。</p>
<p><img src="/img/17_02_09/014.png" alt=""></p>
<p>但是，我们从哪里得到这些标记点呢？我们从哪里得到$l^{(1)}$ $l^{(2)}$ $l^{(3)}$？ 而且在一些复杂的学习问题中，也许我们需要更多的标记点，而不是我们手选的这三个。</p>
<p>因此，在实际应用时，怎么选取标记点，是机器学习中必须解决的问题。</p>
<p>这是我们的数据集：</p>
<p><img src="/img/17_02_09/015.png" alt=""></p>
<p>有一些正样本和一些负样本。我们的想法是：我们直接将训练样本作为标记点。</p>
<p><img src="/img/17_02_09/016.png" alt=""></p>
<p>如果我们有一个训练样本$x^{(1)}$，那么我将把第一个标记点就放在和第一个训练样本点完全重合的地方：</p>
<p><img src="/img/17_02_09/017.png" alt=""></p>
<p>同样，第二个标记点也是使用同样的方式来标注：</p>
<p><img src="/img/17_02_09/018.png" alt=""></p>
<p>在右边的这幅图上，我们使用红点和蓝点来阐述这幅图以及这些点的颜色，可能并不显眼，但是利用这个方法最终能得到m个标记点：</p>
<p>$$<br>l^{(1)} ，l^{(2)}，…，l^{(m)}<br>$$</p>
<p>即每一个标记点的位置，都与每一个样本点的位置精确对应。</p>
<p>这说明，特征函数基本上是在描述<strong>每一个样本距离样本集中其他样本的距离</strong>。</p>
<p>我们具体的列出这个过程的大纲：</p>
<p>给定m个训练样本：</p>
<p>$$<br>(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)}),<br>$$</p>
<p>我将选取与m个训练样本精确一致的位置作为我的标记点：</p>
<p>$$<br>l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…,l^{(m)}=x^{(m)}.<br>$$</p>
<p>当输入样本$x$（样本$x$可以属于训练集，也可以属于交叉验证集，也可以属于测试集），我们可以计算这些特征，即：</p>
<p>$$<br>f_1=similarity(x,l^{(1)}) \\<br>f_2=similarity(x,l^{(2)}) \\<br>…<br>$$</p>
<blockquote>
<p>这里的$l^{(1)}=x^{(1)}$，$l^{(2)}=x^{(2)}$…。</p>
</blockquote>
<p>最终我们能得到一个特征向量，我们将特征向量记为$f$：</p>
<p>$$<br>f=<br>\left[<br>\begin{matrix}<br> f_1  \\<br> f_2  \\<br> …    \\<br> f_m<br>\end{matrix}<br>\right]<br>$$</p>
<p>此外，按照惯例，如果我们需要的话，可以添加额外的特征$f_0$，$f_0$的值始终为1：</p>
<p>$$<br>f=<br>\left[<br>\begin{matrix}<br> f_0  \\<br> f_1  \\<br> f_2  \\<br> …    \\<br> f_m<br>\end{matrix}<br>\right]<br>$$</p>
<p>它与我们之前讨论过的截距$x^0$的作用相似。</p>
<hr>
<p>举个例子，假设我们有训练样本$(x^{(i)},y^{(i)})$，这个样本对应的特征向量可以这样计算：</p>
<p>给定$x^{(i)}$，我们可以通过相似度函数<br>$$<br>f_1^{(i)}=similarity(x^{(i)},l^{(1)}) \\<br>f_2^{(i)}=similarity(x^{(i)},l^{(2)}) \\<br>… \\<br>f_m^{(i)}=similarity(x^{(i)},l^{(m)}) \\<br>$$</p>
<p>在这一列中的某个位置，即第$i$个元素，有一个特征：</p>
<p>$$<br>f_i^{(i)}=similarity(x^{(i)},l^{(i)})<br>$$</p>
<blockquote>
<p>这里的$l^{(i)}$就等于$x^{(i)}$。</p>
</blockquote>
<p>所以$f_i^{(i)}$衡量的是$x^{(i)}$与其自身的相似度，如果你使用高斯核函数的话，这一项为：</p>
<p>$$<br>f_i^{(i)}=similarity(x^{(i)},l^{(i)})=exp(-\frac{0}{2σ^2})=1<br>$$</p>
<p>所以，对于这个样本来说，其中的某一个特征等于1。接下来，类似于我们之前的过程，我将这m个特征合并为一个特征向量。于是，相比之前用$x^{(i)}$来描述样本，$x^{(i)}$为n维或者n+1维空间。我们现在可以使用这个特征向量$f^{(i)}$来描述我的特征向量：</p>
<p>$$<br>f^{(i)}=<br>\left[<br>\begin{matrix}<br> f_0^{(i)}  \\<br> f_1^{(i)}  \\<br> f_2^{(i)}  \\<br> …    \\<br> f_m^{(i)}<br>\end{matrix}<br>\right]<br>$$</p>
<p>其中 $f_0^{(i)}=1$。</p>
<p>那么这个向量就是我们用于描述训练样本的特征向量。</p>
<p>当给定核函数和相似度函数后，我们按照这个方法来使用<strong>支持向量机</strong>。</p>
<hr>
<p>如果你已经得到参数$\theta$并且想对样本x做出预测，我们先要计算特征向量$f$，$f$是$m+1$维的特征向量（这里有m是因为我们有m个训练样本，因此就有m个标记点）。</p>
<p>我们在$\theta^Tf\ge0$时，预测$y=1$</p>
<blockquote>
<p>这里$\theta^Tf=\theta_0f_0 + \theta_1f_1 + … + \theta_mf_m$</p>
</blockquote>
<p>以上就是<strong>当已知参数$\theta$时，怎么做出预测的过程</strong>。</p>
<p>那么，怎样得到参数$\theta$呢？</p>
<p>你在使用SVM学习算法的时候，具体来说就是要求解这个最小化问题：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}f^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}f^{(i)})<br>]+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>你需要求出能使这个式子取最小值的参数$\theta$。注意，这里我们把之前的$x^{(i)}$换成了$f^{(i)}$。</p>
<p>通过解决这个最小化问题，我们就能得到支持向量机的参数。</p>
<p>最后一个对于这个优化问题的细节是：我们有$n=m$个特征。有效的特征数量应该等于$f$的维数，所以$n$其实就等于$m$。</p>
<p><img src="/img/17_02_09/019.png" alt=""></p>
<p>以上就是支持向量机的学习算法。</p>
<p>我在这里还要讲到一个数学细节，在支持向量机实现的过程中，最后一项与上面式子中的最后一项$\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^2$有细微差别。其实在实现支持向量机时，你并不需要知道这个细节，事实上这个式子已经给你提供了全部需要的原理。但是在支持向量机实现的过程中，这一项$\sum_{j=1}^{n}\theta_{j}^2$可以被重写为$\theta^T\theta$(如果我们忽略$\theta_0$的话)，即：</p>
<p>$$<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>=<br>\theta^T\theta<br>$$</p>
<p>其中：</p>
<p>$$<br>\theta=<br>\left[<br>\begin{matrix}<br> \theta_1  \\<br> \theta_2  \\<br> …    \\<br> \theta_m  \\<br>\end{matrix}<br>\right] \ \ (ignore \ \theta_0)<br>$$</p>
<p>大多数支持向量机在实现的时候其实是替换掉$\theta^T\theta$的，用$\theta^TM\theta$来代替，其中$M$是某矩阵，具体是什么矩阵取决于你采用的核函数。这其实是另一种略有区别的距离度量方法。我们用这种略有变化的度量距离的形式来取代对$||\theta||^2$（即$\theta^T\theta$，或者$\sum_{j=1}^{n}\theta_{j}^2$）进行最小化的形式，这是参数向量$\theta$的<strong>变尺度形式</strong>，这种变化和核函数相关。这个数学细节使得支持向量机能够更有效率的运行。</p>
<p>支持向量机做这种修改的理由是：这么做可以适应超大的训练集。</p>
<p>例如：当你的训练集有10000个样本时</p>
<p>$$<br>M=10000<br>$$</p>
<p>根据我们之前定义标记点的方法，我们最终有10000个标记点，$\theta$也随之是10000维的向量。或许这时这么做还可行，但是，当$m$变得非常非常大时，那么求解这么多参数时，此时利用支持向量机软件包来解决这里的最小化问题时，求解这些参数的成本会非常高。</p>
<p>这些都是数学细节，事实上你没有必要了解这些，这里$\theta^TM\theta$实际上细微的修改了最后一项，使得最终的优化目标与直接最小化$||\theta||^2$略有区别。</p>
<p>如果你愿意的话，你可以直接认为这个具体的实现细节尽管略微的改变了优化目标，但是它主要是为了计算效率，所以你不必要对此有太多担心。</p>
<p>顺便说一下，你可能会想，为什么我们不将核函数这个想法应用到其他算法，比如逻辑回归上。事实证明，如果你愿意的话，确实可以将核函数这个想法用于定义特征向量，将标记点之类的技术用于逻辑回归算法。但是用于支持向量机的计算技巧不能较好的推广到其他算法，诸如逻辑回归上。所以，将核函数用于逻辑回归上时，会变得非常的慢。相比之下，这些计算技巧，比如这一步：$\theta^TM\theta$，这些细节的修改，以及支持向量机软件的实现细节，使得支持向量机可以和核函数相得益彰，而逻辑回归和核函数则会运行的十分缓慢，更何况它们还不能使用那些高级优化技巧，因为这些技巧是人们专门为使用核函数的支持向量机开发的。但是这些问题只有在你亲自实现最小化函数时才会遇到。</p>
<p>我将在下一节视频中进一步讨论这些问题，但是你并不需要知道怎么去写一个软件，来最小化代价函数。你能找到很好的成熟的软件来做这些，就像我一直不建议自己写矩阵求逆函数，或者平方根函数的道理一样。这些软件包已经包含了那些数值优化技巧，所以你不必担心这些东西。</p>
<hr>
<h3 id="SVM参数：">SVM参数：</h3><h4 id="$C(=\frac{1}{\lambda})$">$C(=\frac{1}{\lambda})$</h4><p>但是另外一个值得说明的问题是，在你使用支持向量机时，怎么选择支持向量机中的参数？在本节视频的末尾，我想稍微说明一下，在使用支持向量机时的“偏差-方差折中”其中一个要选择的事情是，目标函数中的参数$C$。回忆一下，$C$的作用与$\frac{1}{\lambda}$相似。</p>
<p><img src="/img/17_02_09/020.png" alt=""></p>
<p>$\lambda$是逻辑回归算法中的正则化参数，所以$C$对应着我们之前在逻辑回归问题中的$\lambda$，这意味着:</p>
<ul>
<li>较小的$\lambda$对应较大的$C$，这就意味着有可能得到一个低偏差但高方差的模型。</li>
<li>较大的$\lambda$对应较小的$C$，这就意味着有可能得到一个高偏差但低方差的模型。</li>
</ul>
<p>所以使用较大的$C$值模型，为高方差，更倾向于过拟合；而使用较小的$C$值的模型，为高偏差，更倾向于欠拟合。</p>
<h4 id="$σ^2$">$σ^2$</h4><p>另一个要处理的参数是<strong>高斯核函数</strong>中的$σ^2$。</p>
<p>当高斯核函数中的$σ^2$偏大时，那么对应的相似度函数为：</p>
<p>$$<br>exp(-\frac{||x-l^{(i)}||^2}{2σ^2})<br>$$</p>
<p>在下面这个例子中，如果我们只有一个特征$x_1$，有一个标记点$l$：</p>
<p><img src="/img/17_02_09/021.png" alt=""></p>
<p><strong>如果$σ^2$越大</strong>，那么高斯核函数倾向于变得<strong>越平滑</strong>，由于函数平滑且变化的比较平缓，这会给你的模型带来<strong>较高的偏差和较低的方差</strong>，由于高斯核函数变得平缓，就更倾向于得到一个随着输入$x$变化得缓慢的模型：</p>
<p><img src="/img/17_02_09/022.png" alt=""></p>
<p>反之<strong>如果$σ^2$越小</strong>，那么高斯核函数会变化的<strong>很剧烈</strong>，在这种情况下，最终得到的模型会是<strong>低偏差和高方差</strong>：</p>
<p><img src="/img/17_02_09/023.png" alt=""></p>
<hr>
<p>这就是利用核函数的支持向量机算法。</p>
<p>希望这些关于方差和偏差的讨论能给你一些对于算法结果预期的直观印象。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2017/02/06/斯坦福机器学习课程 第七周 (1)大间距分类/" itemprop="url">
                斯坦福机器学习课程 第七周 (1)大间距分类
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2017-02-06T00:42:58+08:00" content="2017-02-06">
            2017-02-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/机器学习/" itemprop="url" rel="index">
                  <span itemprop="name">机器学习</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2017/02/06/斯坦福机器学习课程 第七周 (1)大间距分类/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2017/02/06/斯坦福机器学习课程 第七周 (1)大间距分类/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="优化目标">优化目标</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/sHfVT/optimization-objective" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>到目前为止，你已经见过一系列不同的学习算法。在监督学习中许多学习算法的性能都非常类似，因此重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是应用这些算法时所创建的大量数据。</p>
<p>在应用这些算法时，表现情况通常依赖于你的水平。比如你为学习算法所设计的 特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法，广泛的应用于工业界和学术界，它被称为<strong>支持向量机(Support Vector Machine)</strong>。</p>
<p>与逻辑回归和神经网络相比，<strong>支持向量机</strong>或者简称<strong>SVM</strong>在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p>
<p>因此，在接下来的视频中我会探讨这一算法，在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于<strong>支持向量机</strong>，鉴于该算法的强大和受欢迎度，在本课中我会花许多时间来讲解它，它也是我们所介绍的最后一个监督学习算法。</p>
</blockquote>
<h3 id="支持向量机引入">支持向量机引入</h3><p>为了描述<strong>支持向量机</strong>，我将会从逻辑回归开始，展示我们如何一点一点修改，来得到本质上的支持向量机。</p>
<p>在逻辑回归中，我们已经熟悉了它的假设函数形式：</p>
<p>$$<br>h_{\theta}(x)=\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}<br>$$</p>
<p>和S型激励函数：</p>
<p><img src="/img/17_02_06/001.png" width="300" height="200" align="center"></p>
<p>现在让我们一起来考虑下，我们想要逻辑回归做什么？</p>
<ul>
<li><p>如果有一个样本为$y=1$，那么我们希望假设函数$h(x)≈1$，即$\theta^{T}&gt;&gt;0$。你不难发现，此时逻辑回归的输出将趋近于1。</p>
</li>
<li><p>如果有另一个样本为$y=0$，那么我们希望假设函数$h(x)≈0$，即$\theta^{T}&lt;&lt;0$。此时逻辑回归的输出将趋近于0。</p>
</li>
</ul>
<hr>
<p>如果你进一步观察逻辑回归的代价函数，你会发现每个样本(x, y)都会为总代价函数增加这样的一项：</p>
<p>$$<br>-(ylogh_{\theta}(x) + (1-y)log(1-h_{\theta}(x)))<br>= -ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。</p>
<p>现在一起来考虑<strong>y=1</strong>和<strong>y=0</strong>的两种情况：</p>
<ul>
<li><strong>y=1的情况下（即$\theta^{T}x&gt;&gt;0$）</strong>：</li>
</ul>
<p>对于</p>
<p>$$<br>-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>由于$(1-y)=0$，所以我们只需考虑前半部分：</p>
<p>$$<br>-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}<br>$$</p>
<p>如果画出代价函数关于$z$的图，你会看到下图：</p>
<p><img src="/img/17_02_06/002.png" width="300" height="200" align="center"></p>
<p>我们可以看到，当$z$增大时(即$\theta^{T}x$增大时)，$z$对应的值会变得非常小，对整个代价函数而言，影响也非常小。</p>
<p>现在开始建立<strong>支持向量机</strong>，我们会从这个代价函数$-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}$开始，一点点的修改：</p>
<p>我们画出一个非常接近于逻辑回归函数的折线，这个折线经由$z=1$的一点的两条线段组成：</p>
<p><img src="/img/17_02_06/003.png" width="300" height="200" align="center"></p>
<p>到这里已经非常接近逻辑回归中使用的代价函数了，只是这里是由两条线段组成。先不要考虑线段的斜率，这并不重要，重要的是我们将在$y=1$的前提下使用新的代价函数。</p>
<ul>
<li><strong>y=0的情况下（即$\theta^{T}x&lt;&lt;0$）</strong>：</li>
</ul>
<p>对于</p>
<p>$$<br>-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>由于$y=0$，所以我们只需考虑后半部分：</p>
<p>$$<br>(1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})<br>$$</p>
<p>如果画出代价函数关于$z$的图，你会看到下图：</p>
<p><img src="/img/17_02_06/004.png" width="300" height="200" align="center"></p>
<p>用相似的方法，我们开始建立<strong>支持向量机</strong>：</p>
<p><img src="/img/17_02_06/005.png" width="300" height="200" align="center"></p>
<p>我们将在$y=0$的前提下使用新的代价函数。</p>
<hr>
<p>那么现在我们来给这两个方程命名：</p>
<p>对于这个函数：</p>
<p><img src="/img/17_02_06/003.png" width="300" height="200" align="center"></p>
<p>我们命名为<strong>$cost_{1}(z)$</strong>。</p>
<p>对于第二个函数：</p>
<p><img src="/img/17_02_06/005.png" width="300" height="200" align="center"></p>
<p>我们命名为<strong>$cost_{0}(z)$</strong>。</p>
<p>这里的下标指的是在函数中对应的$y=1$和$y=0$的情况。</p>
<hr>
<h3 id="构建支持向量机">构建支持向量机</h3><p>拥有了这些定义之后，现在我们就开始构建<strong>支持向量机</strong>。</p>
<h4 id="1-替换逻辑回归函数">1.替换逻辑回归函数</h4><p>这就是我们在逻辑回归中使用的代价函数$J(\theta)$：</p>
<p><img src="/img/17_02_06/006.png" alt=""></p>
<p>对于支持向量机而言，实际上，我们要将</p>
<ul>
<li><p>上面式子中的这一项：$(-logh_{\theta}(x^{(i)}))$替换为：$cost_{1}(z)$，即:$cost_{1}(\theta^{T}x^{(i)})$</p>
</li>
<li><p>同样，这一项：$((-log(1-h_{\theta}(x^{(i)}))))$替换为：$cost_{0}(z)$，即:$cost_{0}(\theta^{T}x^{(i)})$</p>
</li>
</ul>
<p>这里替换之后的$cost_{1}(z)$和$cost_{0}(z)$就是上面提到的那两条靠近逻辑回归函数的折线。</p>
<p>所以对于<strong>支持向量机</strong>的最小化代价函数问题，代价函数的形式如下：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{m}[<br>\sum_{i=1}^{m}<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{\lambda}{2m}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<h4 id="2-去除多余的常数项_$\frac{1}{m}$">2.去除多余的常数项 $\frac{1}{m}$</h4><p>现在按照<strong>支持向量机</strong>的惯例，我们去除$\frac{1}{m}$这一项，因为这一项是个常数项，即使去掉我们也可以得出相同的$\theta$最优值：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\sum_{i=1}^{m}<br>[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{\lambda}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<h4 id="3-正则化项系数的处理">3.正则化项系数的处理</h4><p>在逻辑回归的目标函数中，我们有两项表达式：</p>
<ul>
<li>来自于训练样本的代价函数:</li>
</ul>
<p>$$<br>\frac{1}{m}[<br>\sum_{i=1}^{m}<br>y^{(i)}<br>(-logh_{\theta}(x^{(i)}))+<br>(1-y^{(i)})<br>((-log(1-h_{\theta}(x^{(i)}))))<br>]<br>$$</p>
<ul>
<li>正则化项：</li>
</ul>
<p>$$<br>\frac{\lambda}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>我们不得不使用正则化项来平衡我们的代价函数。这就相当于：</p>
<p>$$<br>A + \lambda B<br>$$</p>
<p>其中A相当于上面的第一项，B相当于第二项。</p>
<p>我们通过修改不同的正则化参数$\lambda$来达到优化目的，这样我们就能够使得训练样本拟合的更好。</p>
<p>但对于<strong>支持向量机</strong>，按照惯例我们将使用一个不同的参数来替换这里使用的$\lambda$来实现权衡这两项的目的。这个参数我们称为<strong>C</strong>。同时将优化目标改为:</p>
<p>$$<br>CA + B<br>$$</p>
<p>因此，在逻辑回归中，如果给$\lambda$一个很大的值，那么就意味着给与$B$了一个很大的权重，而在<strong>支持向量机</strong>中，就相当于对$C$设定了一个非常小的值，这样一来就相当于对$B$给了比$A$更大的权重。</p>
<p>因此，这只是一种来控制这种权衡关系的不同的方式。当然你也可以把这里的$C$当做$farc{1}{\lambda}$来使用。</p>
<p>因此，这样就得到了在<strong>支持向量机</strong>中的我们的整个优化目标函数：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<hr>
<p>最后有别于<strong>逻辑回归</strong>的一点，对于<strong>支持向量机</strong>假设函数的形式如下：</p>
<p>$$<br>h_{\theta}(x) = 1 \ \ \ if \ \theta^Tx \ge 0<br>$$</p>
<p>$$<br>h_{\theta}(x) = 0 \ \ \ if \ \theta^Tx \lt 0<br>$$</p>
<p>而不是<strong>逻辑回归</strong>中的S型曲线：</p>
<p>$$<br>h_{\theta}(x)=\frac{1}{1+e^{-x}}<br>$$</p>
<h2 id="大间距的直觉">大间距的直觉</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/wrjaS/large-margin-intuition" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>人们有时将<strong>支持向量机</strong>看做是<strong>大间距分类器</strong>。在这一部分，我将介绍其中的含义，这有助于我们直观地理解SVM模型的假设是什么样的。</p>
</blockquote>
<p>这是我的支持向量机模型的代价函数：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<ul>
<li>如果你有一个正样本，即$y=1$时，那么代价函数$cost_{1}(z)$的图像如下：</li>
</ul>
<p><img src="/img/17_02_06/007.png" width="300" height="200" align="center"></p>
<p>可以看出，<strong>只有在$z\ge1$(即$\theta^{T}x\ge1$)时(不仅仅是$\ge0$)，代价函数$cost_{1}(z)$的值才等于0</strong>。</p>
<ul>
<li>反之，如果你有一个负样本，即$y=0$时，那么代价函数$cost_{0}(z)$的图像如下：</li>
</ul>
<p><img src="/img/17_02_06/008.png" width="300" height="200" align="center"></p>
<p>可以看出，<strong>只有在$z\le-1$(即$\theta^{T}x\le-1$)时(不仅仅是$\lt0$)，代价函数$cost_{0}(z)$的值才等于0</strong>。</p>
<p>这是<strong>支持向量机</strong>的一个有趣的性质。</p>
<h3 id="安全距离因子">安全距离因子</h3><p>事实上，在逻辑回归中：</p>
<ul>
<li><p>如果你有一个正样本，即$y=1$的情况下，我们仅仅需要$\theta^{T}x\ge0$；</p>
</li>
<li><p>如果你有一个负样本，即$y=0$的情况下，我们仅仅需要$\theta^{T}x\lt0$；</p>
</li>
</ul>
<p>就能将该样本恰当的分类了。</p>
<p>但是<strong>支持向量机</strong>的要求更高，不仅仅要求$\theta^{T}x\ge0$或$\theta^{T}x\lt0$，而且要求$\theta^{T}x$比0大很多，或小很多。比如这里要求$\theta^{T}x\ge1$以及$\theta^{T}x\le-1$。</p>
<p>这就相当于在<strong>支持向量机</strong>中嵌入了一个额外的安全因子（或者说是安全距离因子）。接下来让我们来看看这个因子会导致什么结果：</p>
<p>具体而言，我接下来会将代价函数中的常数项$C$设置成一个非常大的值，比如100000或者其他非常大的数，然后再来观察支持向量机会给出什么结果。</p>
<p>当代价函数中</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>$C$的值非常大时，则最小化代价函数的时候，我们会很希望找到一个使第一项：</p>
<p>$$<br>\sum_{i=1}^{m}[<br>y^{(i)}<br>cost_{1}(\theta^{T}x^{(i)})+<br>(1-y^{(i)})<br>cost_{0}(\theta^{T}x^{(i)})<br>]<br>$$</p>
<p>为0的最优解。</p>
<p>可以看到当输入一个正样本$y^{(i)}=1$时，我们想令上面这一项为0，从图中可以得出</p>
<p><img src="/img/17_02_06/007.png" width="300" height="200" align="center"></p>
<p>对于代价函数$cost_{1}(z)$我们需要使得$\theta^{T}x^{(i)}\ge1$。</p>
<p>类似地，对于一个负训练样本$y^{(i)}=0$时，我们想令上面这一项为0，从图中可以得出</p>
<p><img src="/img/17_02_06/008.png" width="300" height="200" align="center"></p>
<p>对于代价函数$cost_{0}(z)$我们需要使得$\theta^{T}x^{(i)}\le-1$。</p>
<hr>
<p>这样一来会产生下面这种优化问题：</p>
<p>因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>C0+<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>我们知道是$C0$的结果是0，因此可以删掉，所以最终得到的结果是：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>其中：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>这样我们就得到了一个非常有趣的决策边界。</p>
<h3 id="SVM决策边界：线性分割案例">SVM决策边界：线性分割案例</h3><p>具体而言，如果你仔细观察下面这个既有正样本又有负样本的数据集</p>
<p><img src="/img/17_02_06/009.png" width="300" height="200" align="center"></p>
<p>不难看出，这个数据集是线性可分的（即存在一条直线把正负样本分开）。可以看出有很多直线都可以把正负样本区分开，比如下面这两条看起来不太自然的直线：</p>
<p><img src="/img/17_02_06/010.png" width="300" height="200" align="center"></p>
<p>支持向量机会选择黑色的这一条直线：</p>
<p><img src="/img/17_02_06/011.png" width="300" height="200" align="center"></p>
<p>这条直线看起来好很多，因为它看起来更加稳健。在数学上来讲就是这条直线拥有相对于训练数据更大的最短距离，这个所谓的距离就是指<strong>间距(margin)</strong>：</p>
<p><img src="/img/17_02_06/012.png" width="300" height="200" align="center"></p>
<p>而之前两条粉线和蓝线距离训练样本非常近，在分离样本时就会表现的比黑线差。</p>
<p>这就是<strong>支持向量机</strong>拥有<a href="http://baike.baidu.com/link?url=My7Y1mL_9uj-XdR2DC2kyGLop-AaPdzSgdNmgRmaJVYV77puxNs-_A7ERwLv3uWih02JCu6esljRn90mc3EkMwKXBckm_6wSqU42EX06vC4ouxQhinIZco7crxr7HetC" target="_blank" rel="external">鲁棒性</a>的原因。因为它一直努力用一个最大间距来分离样本。因此支持向量机分类器有时又被称为<strong>大间距分类器</strong>。</p>
<p>也许你想知道支持向量机是如何做到产生这个大间距分类器的，目前我还没解释这一点，在下一节中我会直观的来解释这一点。目前这个例子只是用于理解<strong>支持向量机模型</strong>的做法，即努力将正负样本用最大的间距区分开。</p>
<h3 id="大间距分类器中的异常值">大间距分类器中的异常值</h3><p>最后要讲的一点是对于支持向量机中的异常数据的处理。在下面这组训练集中：</p>
<p><img src="/img/17_02_06/013.png" width="300" height="200" align="center"></p>
<p>我们通过使用支持向量机来进行分类，会得到这条黑色的决策边界，从而最大间距的区分这两种数据：</p>
<p><img src="/img/17_02_06/014.png" width="300" height="200" align="center"></p>
<p>当有一个异常值产生时：</p>
<p><img src="/img/17_02_06/015.png" width="300" height="200" align="center"></p>
<p>我们的算法会受到异常值的影响。这时我们将支持向量机中的正则化因子$C$设置的非常大，那么我们会得到类似这样一条粉色的决策边界：</p>
<p><img src="/img/17_02_06/016.png" width="300" height="200" align="center"></p>
<p>那么我们仅仅通过一个异常值，就将我们的决策边界旋转了这么大的角度，实在是不明智的。</p>
<p><img src="/img/17_02_06/017.png" width="300" height="200" align="center"></p>
<p>当我们的正则化因子$C$的值非常大时，支持向量机确实会如此处理，但如果我们适当的减小$C$的值，你最终还是会得到那条黑色的决策边界的。</p>
<p>如果数据是线性不可分的话，像这样：</p>
<p><img src="/img/17_02_06/018.png" width="300" height="200" align="center"></p>
<p>支持向量机也可以恰当的将它们分开。</p>
<p>值得提醒的是$C$的作用其实等同于$\frac{1}{\lambda}$，$\lambda$就是我们之前用到的正则化参数。在支持向量机中，$C$不是很大的时候，可以对包含异常数据、以及线性不可分的数据有比较好的处理效果。</p>
<p>稍后我们还会介绍支持向量机的偏差和方差，希望到那时候关于如何处理参数的这种平衡会变得更加清晰。</p>
<h2 id="大间距分类器背后的数学原理(选学)">大间距分类器背后的数学原理(选学)</h2><p><a href="https://www.coursera.org/learn/machine-learning/lecture/3eNnh/mathematics-behind-large-margin-classification" target="_blank" rel="external">视频地址</a></p>
<blockquote>
<p>这一节将介绍大间距分类背后的数学原理。</p>
<p>本节作为选学内容，你完全可以跳过，但是听听这节课可能让你对支持向量机中的优化问题，以及如何得到大间距分类器产生更好的直观理解。</p>
</blockquote>
<h3 id="向量内积">向量内积</h3><p>首先，带大家复习一下<strong>向量内积</strong>的知识。</p>
<p>假设我们有两个二维向量：</p>
<p>$$<br>u=<br> \begin{bmatrix}<br>   u_1 \\<br>    u_2<br> \end{bmatrix}<br> \<br> v=<br> \begin{bmatrix}<br>   v_1 \\<br>   v_2<br> \end{bmatrix}<br>$$</p>
<p>我们把</p>
<p>$$<br>u^{T}v<br>$$</p>
<p>的计算结果称作向量$u$和$v$之间的<strong>内积</strong>。</p>
<p>由于这里我们用的是二维向量，因此我们可以把这两个向量绘制在同一坐标系内，向量$u$和$v$如下：</p>
<p><img src="/img/17_02_06/019.png" width="300" height="200" align="center"></p>
<p>其中我们用$||u||$来表示$u$的<strong>范数</strong>（即$u$的长度），因此$||u||$的计算公式如下：</p>
<p>$$<br>||u||=\sqrt{u_1^2+u_2^2}<br>$$</p>
<p>下面我们来看看<strong>向量内积</strong>具体是如何计算的：</p>
<p>将向量$v$投影到向量$u$上，如下图我们对向量$v$做一个相对于向量$u$的直角投影：</p>
<p><img src="/img/17_02_06/020.png" width="300" height="200" align="center"></p>
<p>投影之后的长度就是图中红线$p$的长度：</p>
<p>$$<br>p = 向量v投影到向量u上的长度<br>$$</p>
<p>同时也有另外一种计算内积的方式：</p>
<p>$$<br>u^Tv=p·||u||<br>$$</p>
<p>通过这种方式计算出来的内积，答案和之前也是一样的。</p>
<p>事实上，如果你想要使用将$u$投影到$v$上来用这种方式来计算内积，得到的答案也是相同的。</p>
<p>要注意的一点是，这里的$p$是有符号的，如果两个向量的夹角大于90°，像下图中这种情形，如果将$v$投影到$u$上会得到这样一种投影：</p>
<p><img src="/img/17_02_06/021.png" width="300" height="200" align="center"></p>
<p>此时的$p$就是一个负数。</p>
<blockquote>
<p>在向量的内积问题中，如果两个向量的夹角小于90°，那么$p$的符号就是为正；如果两个向量的夹角大于90°，那么$p$的符号就为负。</p>
</blockquote>
<p>这就是<strong>向量内积</strong>的知识，接下来我们尝试使用它来理解支持向量机中的目标函数。</p>
<h3 id="SVM决策边界">SVM决策边界</h3><p>下面是我们在支持向量机中的目标函数:</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>$$</p>
<p>其中</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>接下来为了让目标函数更容易被分析，我们来忽略掉截距的影响，令$\theta_0=0$，这样更容易绘制示意图。并且我们将特征数$n$设置为2，因此我们仅有两个特征$x_1$和$x_2$：</p>
<p>$$<br>\begin{align*}<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>&amp;=<br>\frac{1}{2}<br>(\theta_1^2+\theta_2^2)<br>\\&amp;=<br>\frac{1}{2}<br>(\sqrt{\theta_1^2+\theta_2^2})^2<br>\end{align*}<br>$$</p>
<p>其中</p>
<p>$$<br>\sqrt{\theta_1^2+\theta_2^2} = ||\theta||<br>$$</p>
<p>因此可以得出：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>=<br>\frac{1}{2}||\theta||^2<br>$$</p>
<p>可见，<strong>支持向量机所做的事情，其实就是在极小化参数向量$\theta$范数的平方（或者说是长度的平方）</strong>。</p>
<hr>
<p>现在让我们来看看这两行的含义：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>想一想$\theta^{T}x^{(i)}$这一项等于什么呢？</p>
<p>在前面我们画出了$u^Tv$的示意图，这里$\theta^T$就相当于$u^T$、$x^{(i)}$就相当于$v$。让我们来看一下示意图：</p>
<p>我们考虑一个单一的样本$x^{(i)}$，其坐标为$(x^{(i)}_1,x^{(i)}_2)$</p>
<p><img src="/img/17_02_06/022.png" width="300" height="200" align="center"></p>
<p>这个训练样本点其实可以表示为一个训练样本向量：</p>
<p><img src="/img/17_02_06/023.png" width="300" height="200" align="center"></p>
<p>现在，我们有一个参数向量：</p>
<p><img src="/img/17_02_06/024.png" width="300" height="200" align="center"></p>
<p>那么我们向量内积的计算方式，通过使用之前的方法可以得出。训练样本向量投影到参数向量上的长度$p^{(i)}$，表示第i个训练样本在参数向量$\theta$上的投影：</p>
<p><img src="/img/17_02_06/025.png" width="300" height="200" align="center"></p>
<p>根据之前我们所学到的，我们可以知道：</p>
<p>$$<br>\begin{align*}<br>\theta^Tx^{(i)}<br>&amp;=p^{(i)}·||\theta||<br>\\<br>&amp;=\theta_1x_1^{(i)}+\theta_2x_2^{(i)}<br>\end{align*}<br>$$</p>
<p>那么，这告诉我们了什么呢？这说明：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>这里的约束项是可以用$p^{(i)}·||\theta||$来替代的：</p>
<ul>
<li>若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$</li>
<li>若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$</li>
</ul>
<p>因此，将其写入我们的优化目标后，<strong>完整的目标函数</strong>为：</p>
<p>$$<br>\mathop{min}\limits_{θ}<br>\frac{1}{2}<br>\sum_{j=1}^{n}<br>\theta_{j}^2<br>=<br>\frac{1}{2}||\theta||^2<br>$$</p>
<p>其中</p>
<ul>
<li>若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$</li>
<li>若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$</li>
</ul>
<hr>
<h4 id="实例">实例</h4><p>现在让我们考虑下面这里的训练样本：</p>
<p><img src="/img/17_02_06/026.png" width="300" height="200" align="center"></p>
<p>其中假设截距依然为0，即$\theta_0=0$，我们来看一下支持向量机会选择什么样的决策边界。</p>
<p>假设有这样一条决策边界：</p>
<p><img src="/img/17_02_06/027.png" width="300" height="200" align="center"></p>
<p>很明显，这不是一个好的决策边界，因为这个决策边界离训练样本很近，我们来看一下为什么支持向量机不会选择它。</p>
<p>由于<strong>决策边界和参数向量是正交的(斜率相乘结果为-1)</strong>(<a href="https://zhidao.baidu.com/question/1992397864989257747.html" target="_blank" rel="external">为什么决策边界和参数向量是正交的</a>)，我们可以绘制出对应的参数向量$\theta$：</p>
<p><img src="/img/17_02_06/028.png" width="300" height="200" align="center"></p>
<blockquote>
<p>这里由于我们指定了$\theta_0=0$，也就意味着决策边界是过原点的。</p>
</blockquote>
<p>假设我们以这一点为第一个训练样本：</p>
<p><img src="/img/17_02_06/029.png" width="300" height="200" align="center"></p>
<p>我们可以画出这个样本向量到$\theta$的投影$p^{(1)}$：</p>
<p><img src="/img/17_02_06/030.png" width="300" height="200" align="center"></p>
<p>类似的，我们也可以画出第二个样本向量到$\theta$的投影$p^{(2)}$：</p>
<p><img src="/img/17_02_06/031.png" width="300" height="200" align="center"></p>
<p>我们会发现，这些$p^{(i)}$将会是一些非常小的数。因此当我们考察优化目标函数的时候：</p>
<ul>
<li><p>对于<strong>正样本($y^{(i)}=1$，即图中的”x”样本)</strong>而言，我们需要$p^{(i)}·||\theta||\ge1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。</p>
</li>
<li><p>对于<strong>负样本($y^{(i)}=-1$，即图中的”o”样本)</strong>而言，我们需要$p^{(i)}·||\theta||\le-1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。</p>
</li>
</ul>
<p>但我们的实际目标是希望找到一个参数$\theta$，使得它的范数$||\theta||$是尽可能小的，因此这并不是一个好的决策边界，因为我们的$||\theta||$比较大。</p>
<hr>
<p>对于下面这个决策边界来说，情况就会有很大的不同：</p>
<p><img src="/img/17_02_06/032.png" width="300" height="200" align="center"></p>
<p>这里，我们以纵坐标作为决策边界，那么我们的参数向量的方向就是垂直于它的方向：</p>
<p><img src="/img/17_02_06/033.png" width="300" height="200" align="center"></p>
<p>如果我们现在再来绘制出样本向量在参数向量上的投影$p^{(1)}$和$p^{(2)}$的话，你会发现这些投影的长度比之前长多了：</p>
<p><img src="/img/17_02_06/034.png" width="300" height="200" align="center"></p>
<p>因为投影$p$的长度变大了，随之$\theta$的范数$||\theta||$也相应的变小了。这就意味着通过选择第二种远离样本的决策边界，支持向量机可以使参数$\theta$的范数$||\theta||$变小很多。</p>
<p>这就是<strong>为什么支持向量机可以产生大间距分类的原因</strong>。</p>
<hr>
<p>最后一点，我们的推导自始至终都使用了<strong>截距为0（即$\theta_0=0$）</strong>这个简化假设。这样做的作用就是可以使得决策边界始终是通过原点的，如果你的决策边界不过原点，那么$\theta_0\ne0$，但支持向量机会产生大间距分类器的结论依然成立（具体推导过程不再叙述，和这里很类似）。但是可以说明的是，即便$\theta_0\ne0$，支持向量机仍然会找到正样本和负样本之间的大间距分隔。总之 我们解释了为什么支持向量机是一个大间距分类器。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/">&laquo;</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/3/">&raquo;</a>
  </nav>

 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="http://dannylee1991.github.io/images/avatar.jpg" alt="DannyLee佳楠" itemprop="image"/>
          <p class="site-author-name" itemprop="name">DannyLee佳楠</p>
        </div>
        <p class="site-description motion-element" itemprop="description">一只在迈向机器学习道路上狂奔的程序猿.</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">108</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">16</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">19</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DannyLee1991" target="_blank">GitHub</a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="http://creativecommons.org/licenses/by-nc-sa/4.0" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee佳楠</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dannylee1991"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  

  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
